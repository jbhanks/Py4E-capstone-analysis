{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "# import re\n",
    "import dill\n",
    "from itertools import tee\n",
    "import src.models\n",
    "import src.helpers\n",
    "import src.pdfutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_top(words, tolerance=0.3):\n",
    "#     \"\"\"Adjust 'top' values so that small variations within tolerance are treated as equal. This is necessary when parsing PDFs where words on a line may have slightly different top positions. Made with the help of ChatGPT\"\"\"\n",
    "#     sorted_by_top = sorted(words, key=lambda w: w[\"top\"])\n",
    "#     clusters = []\n",
    "\n",
    "#     for word in sorted_by_top:\n",
    "#         if not clusters or abs(word[\"top\"] - clusters[-1][0]) > tolerance:\n",
    "#             clusters.append((word[\"top\"], []))  # Create new cluster\n",
    "#         clusters[-1][1].append(word)\n",
    "\n",
    "#     # Assign the lowest top value in each cluster\n",
    "#     top_mapping = {}\n",
    "#     for cluster_top, cluster_words in clusters:\n",
    "#         for word in cluster_words:\n",
    "#             top_mapping[word[\"top\"]] = cluster_top\n",
    "\n",
    "#     sorted_words = sorted(words, key=lambda w: (top_mapping[w[\"top\"]], w[\"x0\"]))\n",
    "#     return sorted_words\n",
    "\n",
    "# def map_pdf(pdf_path, same_line_tolerance=0.3):\n",
    "#     fulltext = {}\n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         char_index = 0\n",
    "#         all_lines = []  # Store all detected lines first\n",
    "#         for page in pdf.pages:\n",
    "#             last_top = None\n",
    "#             words = page.extract_words()  \n",
    "#             sorted_words = normalize_top(words, same_line_tolerance)\n",
    "#             line = []\n",
    "\n",
    "#             for word in sorted_words:\n",
    "#                 word_length = len(word[\"text\"])\n",
    "#                 word['range'] = (char_index, char_index + word_length)\n",
    "#                 char_index += word_length + 1\n",
    "\n",
    "#                 if last_top is None or abs(word[\"top\"] - last_top) > same_line_tolerance:  # New line\n",
    "#                     line_start = word['range'][0]\n",
    "#                     line_end = word['range'][1]\n",
    "#                     line_range = (line_start, line_end)\n",
    "#                     if last_top is not None:\n",
    "#                         all_lines.append((line_range, line))  # Store completed line\n",
    "#                     last_top = word[\"top\"]\n",
    "#                     line = [word]\n",
    "#                 else:\n",
    "#                     line.append(word)\n",
    "\n",
    "#             if line:  # Ensure the last line is added\n",
    "#                 line_end = word['range'][1]\n",
    "#                 line_range = (line_start, line_end)\n",
    "#                 all_lines.append((line_range, line))\n",
    "\n",
    "#             # Store lines, skipping first and last. The first line is the title, and thus identical for each page, the last line is just the page number.\n",
    "#             line_no = 0\n",
    "#             for line in all_lines[1:-1]:  # Slice to remove first and last lines\n",
    "#                 char_range = (next(iter(line[1]))['range'][0],  next(reversed(line[1]))['range'][1])\n",
    "#                 x_range = (next(iter(line[1]))['x0'],  next(reversed(line[1]))['x1'])\n",
    "#                 line_info = {'range' : char_range, 'x_dims' : x_range }\n",
    "#                 line_content = ' '.join([w['text'] for w in line[1]])\n",
    "#                 print(line_content)\n",
    "#                 fulltext[(line_no, line_content)] = (line_info, line)\n",
    "#                 line_no += 1\n",
    "#     return fulltext\n",
    "\n",
    "fulltext = src.pdfutils.map_pdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in fulltext.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_starts_x(line):\n",
    "   starts = [word['x0'] for word in line]\n",
    "   return starts\n",
    "\n",
    "\n",
    "tables = {}\n",
    "in_description = False\n",
    "in_table = False\n",
    "\n",
    "\n",
    "for key,value in fulltext.items():\n",
    "    line = key[1]\n",
    "    line_start = value[0]['range'][0]\n",
    "    line_start = value[0]['range'][1]\n",
    "    # Make sure description is set to False at the beginning of each definition, as 'Field Name' is the first part of each definition\n",
    "    if line.startswith('Field Name:'):\n",
    "        in_description = False\n",
    "        in_table = False\n",
    "        if len(value[1][1]) > 2: # Exclude the explanation of \"Field Name\" itself on page 3\n",
    "            field_name = value[1][1][-1]['text'][1:-1] # Get the field name minus the enclosing parentheses\n",
    "        table = []\n",
    "        continue\n",
    "    # Detect the beginning of a description section, which might contain a table (anything outside it does not contain a table I am interested in)\n",
    "    if line.startswith('Description:'):\n",
    "        in_description = True\n",
    "        prev_line_start = line_start\n",
    "    if in_description is True:\n",
    "        # print('in description:', line)\n",
    "        # If inside a description section, check for lines that start with value or `VALUE`, as this is what the first line of a table alway starts with.\n",
    "        # As there are lines that start with `VALUE` but are not tables, also check if the line starts at a larger x0 than the previous line.\n",
    "        if (line.startswith('Value') or line.startswith('VALUE')) and len(value[1][1]) <= 3: # Maximum number of words in a column heading\n",
    "            print(\"Start of table\")\n",
    "            print('Table heading', line)\n",
    "            col_starts = get_word_starts_x(value[1][1])\n",
    "            in_table = True\n",
    "            print(col_starts)\n",
    "            prev_line_start = line_start\n",
    "            table = [(line, value)]\n",
    "        elif in_table is True and (abs(col_starts[0] - get_word_starts_x(value[1][1])[0]) < .5) :\n",
    "            print('Row: ', line)\n",
    "            table.append((line, value))\n",
    "        elif in_table is True:\n",
    "            print(\"End of table\")\n",
    "            tables[field_name] = table\n",
    "            in_table = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = {}\n",
    "\n",
    "for field,table in tables.items():\n",
    "    rows = []\n",
    "    k1,k2 = table[0][0].split(' ', 1)\n",
    "    for i in table[1:]:\n",
    "        row = i[0].split(' ', 1)\n",
    "        rows.append({k1: row[0], k2: row[1] })\n",
    "    table_dicts[field] = rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,d in table_dicts.items():\n",
    "    print(k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_zoning(pdf_path):\n",
    "    all_tables = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            # Extract raw text as lines\n",
    "            lines = page.extract_text().splitlines()\n",
    "            # Extract tables\n",
    "            tables = page.extract_tables()\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Find the position of the table in the raw text\n",
    "                table_start_line = find_table_start(lines, table)\n",
    "                # Extract the line before the table, if available\n",
    "                label_line = (\n",
    "                    lines[table_start_line - 2] if table_start_line > 0 else None\n",
    "                )\n",
    "                table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                if label_line is not None:\n",
    "                    if \"APPENDIX\" in label_line:\n",
    "                        label_line = re.sub(\"APPENDIX.*: \", \"\", label_line)\n",
    "                        label_line = re.sub(\" +\", \"_\", label_line.lower())\n",
    "                        prev_label_line = label_line\n",
    "                    elif \"PLUTO DATA DICTIONARY\" in label_line:\n",
    "                        label_line = None\n",
    "                    elif \"APPENDIX\" not in label_line:\n",
    "                        print(\"what's this?: \", print('label_line is', label_line))\n",
    "                        table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                    if label_line != None:\n",
    "                        all_tables[label_line] = table\n",
    "                    else:\n",
    "                        all_tables[prev_label_line] = all_tables[prev_label_line] + table\n",
    "                else:\n",
    "                    print('table_index is', table_index)\n",
    "                    print('missed:', lines[table_start_line])\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "def find_table_start(lines, table):\n",
    "    \"\"\"\n",
    "    Identify the start of the table in the text by matching table rows\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        # Convert the table's first row into a string and search for it in the text\n",
    "        table_row = \" \".join(str(cell) for cell in table[1] if cell)  # Skip empty cells\n",
    "        if line in table_row:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add tables from appendixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = parse_zoning(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pdfplumber.open(filename) as pdf:\n",
    "#     page = pdf.pages[-1]\n",
    "#     # Extract raw text as lines\n",
    "#     lines = page.extract_text(layout=True).splitlines()\n",
    "#     words = page.extract_words(use_text_flow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "\n",
    "# def group_words_by_row(words, y_thresh=5):\n",
    "#     \"\"\"Groups words into rows based on proximity of their 'top' values.\"\"\"\n",
    "#     words = sorted(words, key=lambda w: w['top'])  # Sort by vertical position\n",
    "#     rows = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         added = False\n",
    "#         for row in rows:\n",
    "#             if abs(word['top'] - row[0]['top']) <= y_thresh:  # Same row if close in top values\n",
    "#                 row.append(word)\n",
    "#                 added = True\n",
    "#                 break\n",
    "#         if not added:\n",
    "#             rows.append([word])  # Start a new row\n",
    "    \n",
    "#     return rows\n",
    "\n",
    "# def adjust_bounding_boxes(rows):\n",
    "#     \"\"\"Expands row bounding boxes so all words in a row share the same top/bottom.\"\"\"\n",
    "#     adjusted_rows = []\n",
    "    \n",
    "#     for row in rows:\n",
    "#         min_top = min(word['top'] for word in row)\n",
    "#         max_bottom = max(word['bottom'] for word in row)\n",
    "        \n",
    "#         adjusted_row = []\n",
    "#         for word in row:\n",
    "#             adjusted_word = word.copy()\n",
    "#             adjusted_word['top'] = min_top\n",
    "#             adjusted_word['bottom'] = max_bottom\n",
    "#             adjusted_row.append(adjusted_word)\n",
    "        \n",
    "#         adjusted_rows.append(adjusted_row)\n",
    "    \n",
    "#     return adjusted_rows\n",
    "\n",
    "# def merge_words_into_rows(words, x_thresh=10, y_thresh=5):\n",
    "#     \"\"\"\n",
    "#     Groups words into rows and merges horizontally close words.\n",
    "    \n",
    "#     Returns:\n",
    "#     - List of lists, where each inner list represents a row with merged text blocks.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Group words into rows\n",
    "#     rows = group_words_by_row(words, y_thresh)\n",
    "    \n",
    "#     # Step 2: Adjust bounding boxes for row uniformity\n",
    "#     adjusted_rows = adjust_bounding_boxes(rows)\n",
    "    \n",
    "#     # Step 3: Merge words within each row into text segments\n",
    "#     row_blocks = []\n",
    "    \n",
    "#     for row in adjusted_rows:\n",
    "#         print('ROW IS:', row)\n",
    "#         row.sort(key=lambda w: w['x0'])  # Sort words left-to-right\n",
    "#         line_blocks = []\n",
    "#         line = []\n",
    "        \n",
    "#         for word in row:\n",
    "#             if line and (word['x0'] - line[-1]['x1']) <= x_thresh:  # Merge if close horizontally\n",
    "#                 line.append(word)\n",
    "#             else:\n",
    "#                 if line:\n",
    "#                     line_blocks.append(line)\n",
    "#                 line = [word]\n",
    "#         if line:\n",
    "#             line_blocks.append(line)\n",
    "        \n",
    "#         # Convert words to formatted output\n",
    "#         row_blocks.append([\n",
    "#             (\" \".join(word['text'] for word in line),  # Merged text\n",
    "#              (min(line, key=lambda w: w['x0'])['x0'],  # Bounding box (x0, top, x1, bottom)\n",
    "#               min(line, key=lambda w: w['top'])['top'],\n",
    "#               max(line, key=lambda w: w['x1'])['x1'],\n",
    "#               max(line, key=lambda w: w['bottom'])['bottom']),\n",
    "#              len(line))  # Word count\n",
    "#             for line in line_blocks\n",
    "#         ])\n",
    "    \n",
    "#     return row_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pdfplumber.open(filename) as pdf:\n",
    "#     page = pdf.pages[-1]\n",
    "#     words = page.extract_words(use_text_flow=True)\n",
    "#     text_blocks = merge_words_into_rows(words, x_thresh=10, y_thresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "\n",
    "# def group_words_by_row(words, y_thresh=5):\n",
    "#     \"\"\"Groups words into rows based on proximity of their 'top' values.\"\"\"\n",
    "#     words = sorted(words, key=lambda w: w['top'])  # Sort by vertical position\n",
    "#     rows = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         added = False\n",
    "#         for row in rows:\n",
    "#             if abs(word['top'] - row[0]['top']) <= y_thresh:  # Same row if close in top values\n",
    "#                 row.append(word)\n",
    "#                 added = True\n",
    "#                 break\n",
    "#         if not added:\n",
    "#             rows.append([word])  # Start a new row\n",
    "    \n",
    "#     return rows\n",
    "\n",
    "# def detect_header_by_uppercase(rows):\n",
    "#     \"\"\"Identifies the header row by checking if all words are uppercase.\"\"\"\n",
    "#     header_row = []\n",
    "#     body_rows = []\n",
    "    \n",
    "#     for row in rows:\n",
    "#         if all(word['text'].isupper() for word in row):  # All words must be uppercase\n",
    "#             header_row = row\n",
    "#             print('header_row:', [w['text'] for w in header_row])\n",
    "#         else:\n",
    "#             body_rows.append(row)\n",
    "    \n",
    "#     return header_row, body_rows\n",
    "\n",
    "# def merge_words_into_rows(words, x_thresh=10, y_thresh=5):\n",
    "#     \"\"\"\n",
    "#     Groups words into rows and merges horizontally close words.\n",
    "    \n",
    "#     - Detects headers based on uppercase text in all words.\n",
    "#     - Merges words and handles row alignment.\n",
    "    \n",
    "#     Returns:\n",
    "#     - List of lists, where each inner list represents a row with merged text blocks.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Group words into rows\n",
    "#     rows = group_words_by_row(words, y_thresh)\n",
    "    \n",
    "#     # Step 2: Detect the header row based on all uppercase words\n",
    "#     header_row, body_rows = detect_header_by_uppercase(rows)\n",
    "\n",
    "#     # Step 3: Merge words within each row into text segments\n",
    "#     all_rows = [header_row] + body_rows  # Ensure headers come first\n",
    "#     row_blocks = []\n",
    "    \n",
    "#     for row in all_rows:\n",
    "#         row.sort(key=lambda w: w['x0'])  # Sort words left-to-right\n",
    "#         line_blocks = []\n",
    "#         line = []\n",
    "        \n",
    "#         for word in row:\n",
    "#             if line and (word['x0'] - line[-1]['x1']) <= x_thresh:  # Merge if close horizontally\n",
    "#                 line.append(word)\n",
    "#             else:\n",
    "#                 if line:\n",
    "#                     line_blocks.append(line)\n",
    "#                 line = [word]\n",
    "#         if line:\n",
    "#             line_blocks.append(line)\n",
    "        \n",
    "#         # Convert words to formatted output\n",
    "#         row_blocks.append([\n",
    "#             (\" \".join(word['text'] for word in line),  # Merged text\n",
    "#              (min(line, key=lambda w: w['x0'])['x0'],  # Bounding box (x0, top, x1, bottom)\n",
    "#               min(line, key=lambda w: w['top'])['top'],\n",
    "#               max(line, key=lambda w: w['x1'])['x1'],\n",
    "#               max(line, key=lambda w: w['bottom'])['bottom']),\n",
    "#              len(line))  # Word count\n",
    "#             for line in line_blocks\n",
    "#         ])\n",
    "    \n",
    "#     return row_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Usage\n",
    "# with pdfplumber.open(filename) as pdf:\n",
    "#     page = pdf.pages[-1]\n",
    "#     words = page.extract_words(use_text_flow=True)\n",
    "#     table = merge_words_into_rows(words, x_thresh=10, y_thresh=20)\n",
    "\n",
    "# for row in table:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_blocks[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, block in enumerate(text_blocks):\n",
    "#     print(idx, block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "\n",
    "# def group_words_by_row(words, y_thresh=5):\n",
    "#     \"\"\"Groups words into rows based on proximity of their 'top' values.\"\"\"\n",
    "#     words = sorted(words, key=lambda w: w['top'])  # Sort by vertical position\n",
    "#     rows = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         print(word)\n",
    "#         added = False\n",
    "#         for row in rows:\n",
    "#             if abs(word['top'] - row[-1]['top']) <= y_thresh:  # Compare with last word in row\n",
    "#                 row.append(word)\n",
    "#                 added = True\n",
    "#                 break\n",
    "#         if not added:\n",
    "#             rows.append([word])  # Start a new row\n",
    "    \n",
    "#     return rows\n",
    "\n",
    "# def detect_header_by_uppercase(rows, y_thresh=10):\n",
    "#     \"\"\"Identifies the header row by checking if all words are uppercase, with y-threshold filtering.\"\"\"\n",
    "#     header_row = []\n",
    "#     body_rows = []\n",
    "    \n",
    "#     for row in rows:\n",
    "#         if all(word['text'].isupper() for word in row):  # All words must be uppercase for header\n",
    "#             if row[0]['top'] < y_thresh:  # Ensure header is within the correct vertical space\n",
    "#                 header_row.extend(row)  # Combine all words of the header row\n",
    "#             else:\n",
    "#                 body_rows.append(row)\n",
    "#         else:\n",
    "#             body_rows.append(row)\n",
    "    \n",
    "#     return header_row, body_rows\n",
    "\n",
    "# def merge_words_in_row(row, x_thresh=10, y_thresh=5):\n",
    "#     \"\"\"\n",
    "#     Merges words in a single row, considering the provided x_thresh and y_thresh.\n",
    "    \n",
    "#     Args:\n",
    "#     - row: List of word dicts in the row.\n",
    "#     - x_thresh: Horizontal threshold for merging words.\n",
    "#     - y_thresh: Vertical threshold for grouping words into rows.\n",
    "    \n",
    "#     Returns:\n",
    "#     - A list of merged text blocks, each with the merged text, bounding box, and word count.\n",
    "#     \"\"\"\n",
    "#     row.sort(key=lambda w: w['x0'])  # Sort words left-to-right\n",
    "#     line_blocks = []\n",
    "#     line = []\n",
    "    \n",
    "#     for word in row:\n",
    "#         if line and (word['x0'] - line[-1]['x1']) <= x_thresh:  # Merge if close horizontally\n",
    "#             line.append(word)\n",
    "#         else:\n",
    "#             if line:\n",
    "#                 line_blocks.append(line)\n",
    "#             line = [word]\n",
    "#     if line:\n",
    "#         line_blocks.append(line)\n",
    "    \n",
    "#     # Convert words to formatted output\n",
    "#     return [\n",
    "#         (\" \".join(word['text'] for word in line),  # Merged text\n",
    "#          (min(line, key=lambda w: w['x0'])['x0'],  # Bounding box (x0, top, x1, bottom)\n",
    "#           min(line, key=lambda w: w['top'])['top'],\n",
    "#           max(line, key=lambda w: w['x1'])['x1'],\n",
    "#           max(line, key=lambda w: w['bottom'])['bottom']),\n",
    "#          len(line))  # Word count\n",
    "#         for line in line_blocks\n",
    "#     ]\n",
    "\n",
    "# def merge_words_into_rows(words, header_x_thresh=10, header_y_thresh=10, body_x_thresh=15, body_y_thresh=5):\n",
    "#     \"\"\"\n",
    "#     Groups words into rows and merges horizontally close words for header and body rows with different thresholds.\n",
    "    \n",
    "#     - Detects headers based on uppercase text in all words.\n",
    "#     - Merges words and handles row alignment with separate thresholds for header and body rows.\n",
    "    \n",
    "#     Returns:\n",
    "#     - List of lists, where each inner list represents a row with merged text blocks.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Group words into rows\n",
    "#     rows = group_words_by_row(words, body_y_thresh)  # Use body row y_thresh for initial grouping\n",
    "    \n",
    "#     # Step 2: Detect the header row based on all uppercase words and vertical threshold\n",
    "#     header_row, body_rows = detect_header_by_uppercase(rows, y_thresh=header_y_thresh)\n",
    "\n",
    "#     # Step 3: Merge words in header and body rows with separate thresholds\n",
    "#     all_rows = [header_row] + body_rows  # Ensure headers come first\n",
    "#     row_blocks = []\n",
    "    \n",
    "#     # Merge header row with header-specific thresholds\n",
    "#     if header_row:\n",
    "#         row_blocks.append(merge_words_in_row(header_row, x_thresh=header_x_thresh, y_thresh=header_y_thresh))\n",
    "    \n",
    "#     # Merge body rows with body-specific thresholds\n",
    "#     for row in body_rows:\n",
    "#         row_blocks.append(merge_words_in_row(row, x_thresh=body_x_thresh, y_thresh=body_y_thresh))\n",
    "    \n",
    "#     # Sort rows by top value to maintain correct vertical order\n",
    "#     # Note: Sorting by both 'top' and 'x0' (for tie-breaking in case 'top' is the same) ensures correct ordering\n",
    "#     final_row_blocks = []\n",
    "#     for row_block in row_blocks:\n",
    "#         final_row_blocks.append(sorted(row_block, key=lambda w: (w[1][1], w[1][0])))  # Sort by 'top' and 'x0'\n",
    "    \n",
    "#     return final_row_blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def group_words_by_row(words, y_thresh=5):\n",
    "    \"\"\"Groups words into rows based on vertical proximity, allowing small deviations in top values.\"\"\"\n",
    "    words = sorted(words, key=lambda w: w['top'])  # Sort words top-to-bottom\n",
    "    rows = []\n",
    "\n",
    "    for word in words:\n",
    "        added = False\n",
    "        for row in rows:\n",
    "            # Compare with first word in the row for stability\n",
    "            if abs(word['top'] - row[0]['top']) <= y_thresh:\n",
    "                row.append(word)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            rows.append([word])\n",
    "\n",
    "    return rows\n",
    "\n",
    "def merge_words_in_row(row, x_thresh=10):\n",
    "    \"\"\"\n",
    "    Merges words in a single row, considering the provided x_thresh for horizontal grouping.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of merged text blocks, each with the merged text and bounding box.\n",
    "    \"\"\"\n",
    "    row.sort(key=lambda w: w['x0'])  # Sort words left-to-right\n",
    "    merged_blocks = []\n",
    "    current_block = []\n",
    "\n",
    "    for word in row:\n",
    "        if current_block and (word['x0'] - current_block[-1]['x1']) <= x_thresh:\n",
    "            current_block.append(word)\n",
    "        else:\n",
    "            if current_block:\n",
    "                merged_blocks.append(current_block)\n",
    "            current_block = [word]\n",
    "\n",
    "    if current_block:\n",
    "        merged_blocks.append(current_block)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": \" \".join(w[\"text\"] for w in block),\n",
    "            \"x0\": min(w[\"x0\"] for w in block),\n",
    "            \"x1\": max(w[\"x1\"] for w in block),\n",
    "            \"top\": min(w[\"top\"] for w in block),\n",
    "            \"bottom\": max(w[\"bottom\"] for w in block),\n",
    "        }\n",
    "        for block in merged_blocks\n",
    "    ]\n",
    "\n",
    "def merge_words_into_rows(words, x_thresh=10, y_thresh=5):\n",
    "    \"\"\"\n",
    "    Groups words into rows and merges horizontally close words.\n",
    "    \"\"\"\n",
    "    rows = group_words_by_row(words, y_thresh)\n",
    "    merged_rows = [merge_words_in_row(row, x_thresh) for row in rows]\n",
    "    return merged_rows\n",
    "\n",
    "def assign_columns_to_blocks(merged_rows, column_gap_thresh=20):\n",
    "    \"\"\"\n",
    "    Assigns a column index to each merged text block by detecting significant gaps in x0 values.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_rows: List of lists of merged word blocks.\n",
    "    - column_gap_thresh: Minimum gap to consider as a column boundary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list where each element is a tuple (column_index, word_block_dict).\n",
    "    \"\"\"\n",
    "    all_x_values = sorted(set(block[\"x0\"] for row in merged_rows for block in row))\n",
    "\n",
    "    # Detect gaps to determine column boundaries\n",
    "    column_boundaries = [all_x_values[0]]\n",
    "    for i in range(1, len(all_x_values)):\n",
    "        if all_x_values[i] - all_x_values[i - 1] > column_gap_thresh:\n",
    "            column_boundaries.append(all_x_values[i])\n",
    "\n",
    "    def get_column_index(x0):\n",
    "        \"\"\"Finds the appropriate column index for a given x0 value.\"\"\"\n",
    "        for i, boundary in enumerate(column_boundaries):\n",
    "            if x0 < boundary:\n",
    "                return max(i - 1, 0)\n",
    "        return len(column_boundaries) - 1\n",
    "\n",
    "    structured_output = []\n",
    "    for row in merged_rows:\n",
    "        row_output = [(get_column_index(block[\"x0\"]), block) for block in row]\n",
    "        structured_output.append(row_output)\n",
    "\n",
    "    return structured_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 1\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh)\n",
    "    structured_output = assign_columns_to_blocks(merged_rows, column_gap_thresh)\n",
    "\n",
    "for row in structured_output:\n",
    "    print(row)  # Prints the structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_x_thresh = 10\n",
    "header_y_thresh = 10\n",
    "body_x_thresh = 10\n",
    "body_y_thresh = 20\n",
    "\n",
    "# merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)\n",
    "merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in merged_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_table(filename, page, ncol, top_boundary_text, bottom_boundary_text, header_x_thresh=10, header_y_thresh=10, body_x_thresh=10, body_y_thresh=20, column_gap_thresh=20):\n",
    "    \"\"\"This for extracting table-like arrangements of text in a PDF (which I call \"pseudo-tables\"), for cases where the \"table\" is not explicitly defined as such and thus missed by `pdfplumber.extract_tables()`\n",
    "    To help correctly define the table columns, this function allows the user to provide some hints to assist in identifying boundaries. For the moment, this is only for this particular \"table\" (pseudo-table), but I hope it turns out to be reusable.\n",
    "\n",
    "    Args:\n",
    "        filename (_type_): Path to the pdf file\n",
    "        page (_type_): Page on which the table is located\n",
    "        ncol (_type_): Number of columns in the table\n",
    "        top_boundary_text (_type_): Unique string of text in the last line before the start of the table header\n",
    "        bottom_boundary_text (_type_): Unique string of text in the first line after the end of the table\n",
    "        header_x_thresh (_type_): Horizontal distance within which words will be grouped into the same block, in the header\n",
    "        header_y_thresh (_type_): Vertical distance within which words will be grouped into the same block, to account for table rows that have multiple lines of text inside them, in the header.\n",
    "        body_x_thresh (_type_): Horizontal distance within which words will be grouped into the same block, in the table body\n",
    "        body_y_thresh (_type_): Vertical distance within which words will be grouped into the same block, to account for table rows that have multiple lines of text inside them, in the table body.\n",
    "        column_gap_thresh (_type_): Adjust based on observed spacing\n",
    "    \"\"\"\n",
    "    column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "\n",
    "    with pdfplumber.open(filename) as pdf:\n",
    "        words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "        merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh)\n",
    "        structured_output = assign_columns_to_blocks(merged_rows, column_gap_thresh)\n",
    "    \n",
    "    return structured_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 3\n",
    "cols = {i:[] for i in range(ncol)}\n",
    "for row in merged_rows:\n",
    "    if len(row) <= ncol:\n",
    "        print(row)\n",
    "    for n in range(len(row)):\n",
    "        coldata = [row[n] for row in merged_rows]\n",
    "        cols[n] = coldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"wb\") as f:\n",
    "    dill.dump({'table_dicts' : table_dicts}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
