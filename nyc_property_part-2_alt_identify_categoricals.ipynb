{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from itertools import tee\n",
    "import src.models\n",
    "import src.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the delimiter string\n",
    "delimiter = \"Field Name\"\n",
    "\n",
    "\n",
    "# Extract text from the entire PDF\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    full_text = \"\\n\".join(\n",
    "        page.extract_text() for page in pdf.pages if page.extract_text()\n",
    "    )\n",
    "\n",
    "# Split the text based on the delimiter\n",
    "split_texts = full_text.split(f'\\n{delimiter}:')[1:]\n",
    "restored_texts = [f'\\n{delimiter}: {section}' for section in split_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_on_strings(text, delimiters):\n",
    "    pattern = \"|\".join(map(re.escape, delimiters))  # Regex pattern for delimiters\n",
    "    matches = re.finditer(pattern, text)  # Find delimiter positions\n",
    "    matches, matches_copy = tee(matches)  # Create two iterators (one for bounds, one for extraction)\n",
    "    start = 0\n",
    "    for match, next_match in zip(matches_copy, list(matches)[1:] + [None]):  \n",
    "        end = next_match.start() if next_match else len(text)  \n",
    "        yield (match[0], text[match.end():end])  # Include delimiter at the start, stop before the next\n",
    "\n",
    "# Example Usage\n",
    "delimiters = [\"Field Name:\", \"Format:\", \"Data Source:\", \"Description:\"]\n",
    "\n",
    "entries = []\n",
    "for text in restored_texts:\n",
    "    definitions = {}\n",
    "    for delimiter, segment in split_on_strings(text, delimiters):\n",
    "        definitions[delimiter[0:-1]] = segment.strip()\n",
    "    entries.append(definitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_field_name(name_string):\n",
    "    long = name_string.split('(')[0].strip()\n",
    "    short = re.sub(r'.*?\\((.*?)\\).*?', r'\\1', name_string)\n",
    "    return long.lower().replace(' ', '_'),  short.lower()\n",
    "\n",
    "def parse_definitions_table(description_string, table_start_regex):\n",
    "    # table_string = re.sub(r'.*Value Description(.*)', r'\\1', description_string, flags=re.DOTALL)\n",
    "    print(description_string)\n",
    "    table_string = re.sub(f'{table_start_regex}(.*)', r'\\1', description_string, flags=re.DOTALL)\n",
    "    lines = table_string.splitlines()\n",
    "    d = {}\n",
    "    for line in lines:\n",
    "        try:\n",
    "            key, value = line.split(' ', 1)\n",
    "            d[key.strip()] = value.strip()\n",
    "        except:\n",
    "            print(line)\n",
    "    return d\n",
    "\n",
    "definitions = []\n",
    "\n",
    "category_markers = ['TRACT', 'DISTRICT', 'PRECINCT', 'HEALTH AREA']\n",
    "# Alphanumeric columns that are not good candidates for lookup tables\n",
    "alphanumeric_exceptions = ['EDesigNum', 'APPBBL']\n",
    "# Numeric columns that are category codes\n",
    "numeric_exceptions = ['CD', 'PLUTOMapID']\n",
    "for entry in entries:\n",
    "    d = {}\n",
    "    d['category'] = src.helpers.isCategory(entry, category_markers, alphanumeric_exceptions, numeric_exceptions)\n",
    "    d['long_name'], d['short_name'] = parse_field_name(entry['Field Name'])\n",
    "    # The regex below is used to identify the start of the table in the description, it seems to work ok here.\n",
    "    table_start_regex = r'\\n\\s?Value\\s[a-zA-Z]+\\s?\\n'\n",
    "    if re.search(table_start_regex, entry['Description']):\n",
    "        d['table'] = parse_definitions_table(entry['Description'], table_start_regex)\n",
    "    definitions.append(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = filename\n",
    "\n",
    "xvals = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        words = page.extract_words()  # Each word has an x0 (left) position\n",
    "        for word in words:\n",
    "            # print(f\"Text: {word['text']}, X: {word['x0']}, Y: {word['top']}\")\n",
    "            xvals.append(int(word['x0']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        words = page.extract_words()  \n",
    "        \n",
    "        # Sort words by vertical position (Y-axis) first, then by left position (X-axis)\n",
    "        words.sort(key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
    "        \n",
    "        first_words = []\n",
    "        last_top = None\n",
    "        for word in words:\n",
    "            if last_top is None or abs(word[\"top\"] - last_top) > 2:  # New line detected\n",
    "                first_words.append(word)\n",
    "                last_top = word[\"top\"]\n",
    "\n",
    "        # Print or analyze first words of each line\n",
    "        for word in first_words:\n",
    "            print(f\"First word: {word['text']} at X: {word['x0']}, Y: {word['top']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        words = page.extract_words()\n",
    "\n",
    "        words.sort(key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
    "        first_words = []\n",
    "        last_top = None\n",
    "        last_x = None\n",
    "        table_start = None\n",
    "        table_end = None\n",
    "        char_index = 0  # Track estimated character index in the text\n",
    "        prev_line_startX = None\n",
    "        for word in words:\n",
    "            if last_top is None or abs(word[\"top\"] - last_top) > 2: # New line detected\n",
    "                text = word[\"text\"]\n",
    "                if prev_line_startX is not None:\n",
    "                    if word[\"x0\"] > prev_line_startX + 1:\n",
    "                        word_length = len(text)\n",
    "                        first_words.append(word)\n",
    "                        last_top = word[\"top\"]\n",
    "                        table_start = char_index + 1\n",
    "                        if word_length > 0:\n",
    "                            first_char_index = char_index\n",
    "                            last_char_index = char_index + word_length - 1\n",
    "                            print(f\"Word: {text}, Position: {word['x0']}, First Char Index: {first_char_index}, Last Char Index: {last_char_index}\")\n",
    "                        # Advance the character index (assuming spaces count as 1 character)\n",
    "                    elif word[\"x0\"] < last_x + 1:\n",
    "                        table_end = word_length + 1\n",
    "                last_x = word[\"x0\"]\n",
    "                prev_line_startX = word[\"x0\"]\n",
    "            char_index += word_length + 1  # +1 for space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = {}\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    char_index = 0\n",
    "    for page in pdf.pages:\n",
    "        last_top = None\n",
    "        words = page.extract_words()  \n",
    "        # Sort words by vertical position (Y-axis) first, then by left position (X-axis)\n",
    "        words.sort(key=lambda w: (w[\"top\"], w[\"x0\"]))\n",
    "        line = []\n",
    "        for word in words:\n",
    "            word_length = len(word[\"text\"])\n",
    "            word['range'] = (char_index, char_index + word_length)\n",
    "            char_index += word_length + 1\n",
    "            if last_top is None or abs(word[\"top\"] - last_top) > 2: # New line detected\n",
    "                if last_top is not None:\n",
    "                    print(line)\n",
    "                    fulltext[' '.join([w['text'] for w in line])] = line\n",
    "                last_top = word[\"top\"]\n",
    "                # fulltext.append(line)\n",
    "                line = [word]\n",
    "            else:\n",
    "                line.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
