{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the database based on the data\n",
    "* #### This notebook parses metadata associated with some of the datasets, most especially the PLUTO dataset, which contains columns that are also in many other datasets I looked at on NYCOpenData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import dill\n",
    "from itertools import tee\n",
    "from src.models import ColCustomization\n",
    "import src.helpers\n",
    "import src.pdfutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_word_starts_x(line):\n",
    "#     starts = [word['x0'] for word in line]\n",
    "#     return starts\n",
    "\n",
    "# def normalize_top(objects, tolerance=0.3):\n",
    "#     \"\"\"Adjust 'top' values so that small variations within tolerance are treated as equal. This is necessary when parsing PDFs where words on a line may have slightly different top positions. Made with the help of ChatGPT\"\"\"\n",
    "#     sorted_by_top = sorted(objects, key=lambda w: w[\"top\"])\n",
    "#     clusters = []\n",
    "\n",
    "#     for object in sorted_by_top:\n",
    "#         if not clusters or abs(object[\"top\"] - clusters[-1][0]) > tolerance:\n",
    "#             clusters.append((object[\"top\"], []))  # Create new cluster\n",
    "#         clusters[-1][1].append(object)\n",
    "\n",
    "#     # Assign the lowest top value in each cluster\n",
    "#     top_mapping = {}\n",
    "#     for cluster_top, cluster_objects in clusters:\n",
    "#         for object in cluster_objects:\n",
    "#             top_mapping[object[\"top\"]] = cluster_top\n",
    "\n",
    "#     sorted_objects = sorted(objects, key=lambda w: (top_mapping[w[\"top\"]], w[\"x0\"]))\n",
    "#     return sorted_objects\n",
    "\n",
    "\n",
    "# def extract_and_normalize_elements(page, same_line_tolerance):\n",
    "#     \"\"\"Extract words and section markers, normalize their positions, and return sorted elements.\"\"\"\n",
    "#     words = page.extract_words()\n",
    "#     normalized_words = normalize_top(words, same_line_tolerance)\n",
    "\n",
    "#     rects = [\n",
    "#         # {\"text\": \"---section---\", \"top\": r[\"top\"], 'x0': r['x0'], 'x1': r['x1']}  # Dummy marker for sorting\n",
    "#         {**r, \"text\": \"---section---\"}\n",
    "#         for r in page.objects[\"rect\"]\n",
    "#         if r[\"width\"] > page.width * 0.5 and r[\"height\"] < 2 and r['non_stroking_color'] is not None \n",
    "#         and r['non_stroking_color'][0] > 0.5  # Adjust thresholds as needed\n",
    "#     ]\n",
    "#     normalized_rects = normalize_top(rects, same_line_tolerance)\n",
    "#     elements = normalized_words + normalized_rects\n",
    "#     return sorted(elements, key=lambda e: e[\"top\"])\n",
    "\n",
    "# def group_elements_into_lines(elements_sorted, same_line_tolerance):\n",
    "#     \"\"\"Group sorted elements into lines based on top coordinate similarity.\"\"\"\n",
    "#     char_index = 0\n",
    "#     page_lines = []\n",
    "#     last_top = None\n",
    "#     line = []\n",
    "\n",
    "#     for word in elements_sorted:\n",
    "#         if word['text'] == \"---section---\":\n",
    "#             if line:  \n",
    "#                 page_lines.append(line)\n",
    "#             page_lines.append([word])  # Append section break\n",
    "#             line = []\n",
    "#             last_top = word[\"top\"]\n",
    "#             continue\n",
    "\n",
    "#         word_length = len(word[\"text\"])\n",
    "#         char_index += word_length + 1\n",
    "\n",
    "#         if last_top is None or abs(word[\"top\"] - last_top) > same_line_tolerance:\n",
    "#             if last_top is not None and line:\n",
    "#                 sorted_line = sorted(line, key=lambda w: w[\"x0\"]) # Do one last horizontal sort to make sure the words in each line are in the correct order\n",
    "#                 page_lines.append(sorted_line)\n",
    "#             last_top = word[\"top\"]\n",
    "#             line = [word]\n",
    "#         else:\n",
    "#             line.append(word)\n",
    "\n",
    "#     if line:\n",
    "#         sorted_line = sorted(line, key=lambda w: w[\"x0\"])\n",
    "#         page_lines.append(sorted_line)\n",
    "\n",
    "#     return page_lines\n",
    "\n",
    "# def process_page_lines(page_lines):\n",
    "#     \"\"\"Store lines while removing the title and page number, and format them.\"\"\"\n",
    "#     all_lines = []\n",
    "#     line_no = 0\n",
    "\n",
    "#     for line in page_lines[1:-1]:  # Skip title and page number\n",
    "#         if line[0]['text'] == \"---section---\":\n",
    "#             all_lines.append(line)\n",
    "#             continue\n",
    "#         all_lines.append(line)\n",
    "#         line_no += 1\n",
    "\n",
    "#     return all_lines\n",
    "\n",
    "# def segment_lines_into_sections(all_lines):\n",
    "#     \"\"\"Segment the processed lines into structured sections.\"\"\"\n",
    "#     sections = []\n",
    "#     section = []\n",
    "\n",
    "#     for line in all_lines:\n",
    "#         if line[0]['text'] == \"---section---\":\n",
    "#             sections.append(section)\n",
    "#             section = []\n",
    "#         else:\n",
    "#             section.append(line)\n",
    "\n",
    "#     return sections\n",
    "\n",
    "# def map_pdf(pdf_path, same_line_tolerance=0.3, start_page=None, stop_page=None):\n",
    "#     \"\"\"Main function to extract structured sections from a PDF.\"\"\"\n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         all_lines = []\n",
    "#         for page in pdf.pages[start_page:stop_page]:\n",
    "#             elements_sorted = extract_and_normalize_elements(page, same_line_tolerance)\n",
    "#             page_lines = group_elements_into_lines(elements_sorted, same_line_tolerance)\n",
    "#             all_lines.extend(process_page_lines(page_lines))\n",
    "\n",
    "#         sections = segment_lines_into_sections(all_lines)\n",
    "#     return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section = map_pdf(filename, same_line_tolerance=0.3, start_page=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (re.compile(r'[ ,â€“]+'), '_'),\n",
    "    (re.compile(r'#'), 'num'),\n",
    "    (re.compile(r'/'), 'or'),\n",
    "    (re.compile(r'&'), 'and')\n",
    "]\n",
    "\n",
    "fk_markers = ['code', 'category', 'class', 'district', 'precinct', 'company', 'name', 'health_area', 'type', 'borough', 'name', 'health_area', 'health_center_district']\n",
    "\n",
    "# def clean_name(full_name, patterns):\n",
    "#     new_name = full_name.lower()\n",
    "#     for pattern, replacement in patterns:\n",
    "#         new_name = pattern.sub(replacement, new_name)\n",
    "#     return new_name\n",
    "\n",
    "# def get_word_starts_x(line):\n",
    "#    starts = [word['x0'] for word in line]\n",
    "#    return starts\n",
    "\n",
    "# def parse_table(table):\n",
    "#     rows = []\n",
    "#     col2_start = table[0][1][1]['x0']\n",
    "#     rows = []\n",
    "#     full_row_text = ''\n",
    "#     for i in table[1:]:\n",
    "#         txt = i[0]\n",
    "#         word_metadata = i[1]\n",
    "#         print(txt)\n",
    "#         abs(word_metadata[0]['x0'] - col2_start)\n",
    "#         # This logic helps deal with table rows that have multiple lines of text in a cell\n",
    "#         if abs(word_metadata[0]['x0'] - col2_start) < 1:\n",
    "#             print(\"secondary line\")\n",
    "#             full_row_text = f'{full_row_text} {txt}'\n",
    "#         elif full_row_text:\n",
    "#             rows.append(full_row_text.split(' ', 1))\n",
    "#             print(\"First line of later row\")\n",
    "#             full_row_text = txt\n",
    "#         else:\n",
    "#             print(\"First line of first row\")\n",
    "#             full_row_text = txt  \n",
    "#     return rows\n",
    "\n",
    "column_customizations=[]\n",
    "\n",
    "for section in pdf_by_section:\n",
    "    in_table = False\n",
    "    in_description = False\n",
    "    table = None\n",
    "    col_mods = None  # Initialize col_mods here\n",
    "    for value in section:\n",
    "        line = ' '.join([word['text'] for word in value])\n",
    "        if line.startswith('Field Name:') and len(value) > 2: # Exclude the explanation of \"Field Name\" itself on page 3\n",
    "            col_mods = ColCustomization(short_name=value[-1]['text'][1:-1]) # Get the field name minus the enclosing parentheses\n",
    "            full_name = ' '.join(word['text'] for word in value[2:-1])\n",
    "            new_name = clean_name(full_name.lower(), patterns=patterns)\n",
    "            is_fk = any([word in new_name for word in fk_markers])\n",
    "            col_mods.new_name = new_name\n",
    "            if any([w in new_name for w in ['year', 'number', 'precinct']]):\n",
    "                col_mods.dtype = \"Integer\"\n",
    "            if 'date' in new_name:\n",
    "                col_mods.dtype = \"Date\"\n",
    "        elif line.startswith('Format:') and not col_mods.dtype:\n",
    "            if \"Alphanumeric\" in line:\n",
    "                col_mods.dtype = \"String\"\n",
    "            if \"Numeric\" in line and not col_mods.dtype:\n",
    "                col_mods.dtype = \"Float\"\n",
    "        elif line.startswith('Description:'):\n",
    "            in_description = True\n",
    "        if in_description is True:\n",
    "            if (line.startswith('Value') or line.startswith('VALUE')) and len(value) <= 3: # Maximum number of words in a column heading\n",
    "                col_starts = get_word_starts_x(value)\n",
    "                in_table = True\n",
    "                table = [(line, value)]\n",
    "            elif in_table is True and (abs(col_starts[0] - get_word_starts_x(value)[0]) < .5 or abs(col_starts[1] - get_word_starts_x(value)[0]) < .5):\n",
    "                table.append((line, value))\n",
    "            elif in_table is True:\n",
    "                col_mods.definitions = parse_table(table)\n",
    "                in_table = False\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    if col_mods is not None:\n",
    "        if not col_mods.definitions and table:\n",
    "            col_mods.definitions = parse_table(table)\n",
    "        if col_mods.definitions:\n",
    "            col_mods.is_fk = True\n",
    "        column_customizations.append(col_mods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col.definitions for col in column_customizations if len(col.definitions) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_zoning(pdf_path):\n",
    "    all_tables = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            # Extract raw text as lines\n",
    "            lines = page.extract_text().splitlines()\n",
    "            # Extract tables\n",
    "            tables = page.extract_tables()\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Find the position of the table in the raw text\n",
    "                table_start_line = find_table_start(lines, table)\n",
    "                # Extract the line before the table, if available\n",
    "                label_line = (\n",
    "                    lines[table_start_line - 2] if table_start_line > 0 else None\n",
    "                )\n",
    "                table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                if label_line is not None:\n",
    "                    if \"APPENDIX\" in label_line:\n",
    "                        label_line = re.sub(\"APPENDIX.*: \", \"\", label_line)\n",
    "                        label_line = re.sub(\" +\", \"_\", label_line.lower())\n",
    "                        prev_label_line = label_line\n",
    "                    elif \"PLUTO DATA DICTIONARY\" in label_line:\n",
    "                        label_line = None\n",
    "                    elif \"APPENDIX\" not in label_line:\n",
    "                        print(\"what's this?: \", print('label_line is', label_line))\n",
    "                        table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                    if label_line != None:\n",
    "                        all_tables[label_line] = table\n",
    "                    else:\n",
    "                        all_tables[prev_label_line] = all_tables[prev_label_line] + table\n",
    "                else:\n",
    "                    print('table_index is', table_index)\n",
    "                    print('missed:', lines[table_start_line])\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "def find_table_start(lines, table):\n",
    "    \"\"\"\n",
    "    Identify the start of the table in the text by matching table rows\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        # Convert the table's first row into a string and search for it in the text\n",
    "        table_row = \" \".join(str(cell) for cell in table[1] if cell)  # Skip empty cells\n",
    "        if line in table_row:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add tables from appendixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = parse_zoning(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse Appendix D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the last table, which isn't actually a table, just text arranged in a table-like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def trim_lines_outside_table(lines, table_top_boundary_text=None, table_bottom_boundary_text=None):\n",
    "    \"\"\"Returns the index of the first line to contain the specified table_top_boundary_text\n",
    "\n",
    "    Args:\n",
    "        lines (_type_): _description_\n",
    "        table_top_boundary_text (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for idx, line in enumerate(lines):\n",
    "        if table_top_boundary_text is not None and table_top_boundary_text in \" \".join(\n",
    "            [word[\"text\"] for word in line]\n",
    "        ):\n",
    "            top_trim_line = idx\n",
    "            continue\n",
    "        elif (\n",
    "            table_bottom_boundary_text is not None\n",
    "            and table_bottom_boundary_text in \" \".join([word[\"text\"] for word in line])\n",
    "        ):\n",
    "            bottom_trim_line = idx\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    trimmed_lines = [\n",
    "        line\n",
    "        for idx, line in enumerate(lines)\n",
    "        if idx > top_trim_line and idx < bottom_trim_line\n",
    "    ]\n",
    "    return trimmed_lines\n",
    "\n",
    "def group_words_by_row(words, y_thresh=5):\n",
    "    \"\"\"Groups words into rows based on vertical proximity, allowing small deviations in top values.\"\"\"\n",
    "    words = sorted(words, key=lambda w: w['top'])  # Sort words top-to-bottom\n",
    "    rows = []\n",
    "\n",
    "    for word in words:\n",
    "        added = False\n",
    "        for row in rows:\n",
    "            # Compare with first word in the row for stability\n",
    "            if abs(word['top'] - row[0]['top']) <= y_thresh:\n",
    "                row.append(word)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            rows.append([word])\n",
    "\n",
    "    return rows\n",
    "\n",
    "def merge_words_in_row(row, x_thresh=10):\n",
    "    \"\"\"\n",
    "    Merges words in a single row, considering the provided x_thresh for horizontal grouping.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of merged text blocks, each with the merged text and bounding box.\n",
    "    \"\"\"\n",
    "    row.sort(key=lambda w: w['x0'])  # Sort words left-to-right\n",
    "    merged_blocks = []\n",
    "    current_block = []\n",
    "\n",
    "    for word in row:\n",
    "        if current_block and (word['x0'] - current_block[-1]['x1']) <= x_thresh:\n",
    "            current_block.append(word)\n",
    "        else:\n",
    "            if current_block:\n",
    "                current_block.sort(key=lambda w: w['top']) # Sort block by top coordinate to get text in each table cell correctly ordered.\n",
    "                merged_blocks.append(current_block)\n",
    "            current_block = [word]\n",
    "\n",
    "    if current_block:\n",
    "        merged_blocks.append(current_block)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": \" \".join(w[\"text\"] for w in block),\n",
    "            \"x0\": min(w[\"x0\"] for w in block),\n",
    "            \"x1\": max(w[\"x1\"] for w in block),\n",
    "            \"top\": min(w[\"top\"] for w in block),\n",
    "            \"bottom\": max(w[\"bottom\"] for w in block),\n",
    "        }\n",
    "        for block in merged_blocks\n",
    "    ]\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_lines_in_row(lines, y_thresh):\n",
    "    merged_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if not merged_lines:\n",
    "            merged_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        prev_line = merged_lines[-1]\n",
    "        \n",
    "        # Compute merging condition\n",
    "        min_top_current = min(word[\"top\"] for word in line)\n",
    "        max_bottom_prev = max(word[\"bottom\"] for word in prev_line)\n",
    "        \n",
    "        if min_top_current - max_bottom_prev < y_thresh:\n",
    "            # Merge into the previous line\n",
    "            merged_lines[-1].extend(line)\n",
    "        else:\n",
    "            # Start a new line\n",
    "            merged_lines.append(line)\n",
    "\n",
    "    # Now merge words by `x0` within each line\n",
    "    result = []\n",
    "    \n",
    "    for line in merged_lines:\n",
    "        grouped = defaultdict(list)\n",
    "        \n",
    "        for (_, word) in enumerate(line):\n",
    "            grouped[word[\"x0\"]].append(word)\n",
    "        \n",
    "        merged_words = []\n",
    "        \n",
    "        for x0 in sorted(grouped.keys()):  # Preserve order\n",
    "            words = grouped[x0]\n",
    "            merged_text = \" \".join(w[\"text\"] for w in words)\n",
    "            x1 = max(w[\"x1\"] for w in words)\n",
    "            top = min(w[\"top\"] for w in words)\n",
    "            bottom = max(w[\"bottom\"] for w in words)\n",
    "            \n",
    "            merged_words.append({\"text\": merged_text, \"x0\": x0, \"x1\": x1, \"top\": top, \"bottom\": bottom})\n",
    "        \n",
    "        result.append(merged_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def detect_header_by_uppercase(rows):\n",
    "    \"\"\"Identifies the header row by checking if all words are uppercase.\"\"\"\n",
    "    header_row = []\n",
    "    body_rows = []\n",
    "\n",
    "    for row in rows:\n",
    "        if all(word[\"text\"].isupper() for word in row):  # All words must be uppercase\n",
    "            header_row = header_row + row\n",
    "        else:\n",
    "            body_rows.append(row)\n",
    "\n",
    "    return header_row, body_rows\n",
    "\n",
    "\n",
    "def merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh):\n",
    "    \"\"\"\n",
    "    Groups words into rows and merges horizontally close words.\n",
    "    \"\"\"\n",
    "    rows = group_words_by_row(words, header_y_thresh)\n",
    "    trimmed_rows = trim_lines_outside_table(rows, table_top_boundary_text=\"APPENDIX D: LAND USE CATEGORIES\", table_bottom_boundary_text=\"NOTES:\")\n",
    "    header_row, body_rows = detect_header_by_uppercase(trimmed_rows)\n",
    "    merged_header = merge_words_in_row(header_row, header_x_thresh)\n",
    "    merged_rows = [merge_words_in_row(row, body_x_thresh) for row in body_rows]\n",
    "    merged_rows = merge_lines_in_row(merged_rows, body_y_thresh)\n",
    "    all_rows = [merged_header] + merged_rows\n",
    "    return all_rows\n",
    "\n",
    "def assign_columns_to_blocks(merged_rows, column_gap_thresh=20, ncol=3):\n",
    "    \"\"\"\n",
    "    Assigns a column index to each merged text block by detecting significant gaps in x0 values.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_rows: List of lists of merged word blocks.\n",
    "    - column_gap_thresh: Minimum gap to consider as a column boundary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list where each element is a tuple (column_index, word_block_dict).\n",
    "    \"\"\"\n",
    "    all_x_values = sorted(set(block[\"x0\"] for row in merged_rows for block in row))\n",
    "\n",
    "    # Detect gaps to determine column boundaries\n",
    "    column_boundaries = [all_x_values[0]]\n",
    "    for i in range(1, len(all_x_values)):\n",
    "        if all_x_values[i] - all_x_values[i - 1] > column_gap_thresh:\n",
    "            column_boundaries.append(all_x_values[i])\n",
    "\n",
    "    def get_column_index(x0):\n",
    "        \"\"\"Finds the appropriate column index for a given x0 value.\"\"\"\n",
    "        for i, boundary in enumerate(column_boundaries):\n",
    "            if x0 < boundary:\n",
    "                return max(i - 1, 0)\n",
    "        return len(column_boundaries) - 1\n",
    "\n",
    "    structured_output = []\n",
    "    for idx,row in enumerate(merged_rows):\n",
    "        row_output = [((idx, (get_column_index(block[\"x0\"]))), block) for block in row] # Store row and column indices for each block with that block\n",
    "        # row_output = [row for row in row_output if row[0] <= ncol]\n",
    "        structured_output.append(row_output)\n",
    "\n",
    "    return structured_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 20\n",
    "body_x_thresh = 5\n",
    "body_y_thresh = 10\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "ncol = 3\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)\n",
    "    last_table = assign_columns_to_blocks(merged_rows, column_gap_thresh)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"wb\") as f:\n",
    "    dill.dump({'column_customizations': column_customizations, 'table_dicts': table_dicts, 'last_table': last_table}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
