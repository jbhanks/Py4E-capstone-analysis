{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the database based on the data\n",
    "* #### This notebook parses metadata associated with some of the datasets, most especially the PLUTO dataset, which contains columns that are also in many other datasets I looked at on NYCOpenData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import dill\n",
    "from bisect import bisect_left\n",
    "from itertools import tee\n",
    "from src.models import ColCustomization\n",
    "import src.helpers\n",
    "import src.pdfutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section = src.pdfutils.map_pdf(filename, same_line_tolerance=0.3, start_page=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (re.compile(r'[ ,â€“]+'), '_'),\n",
    "    (re.compile(r'#'), 'num'),\n",
    "    (re.compile(r'/'), 'or'),\n",
    "    (re.compile(r'&'), 'and')\n",
    "]\n",
    "\n",
    "fk_markers = ['code', 'category', 'class', 'district', 'precinct', 'company', 'name', 'health_area', 'type', 'borough', 'name', 'health_area', 'health_center_district']\n",
    "\n",
    "column_customizations=[]\n",
    "\n",
    "for section in pdf_by_section:\n",
    "    in_table = False\n",
    "    in_description = False\n",
    "    table = None\n",
    "    col_mods = None  # Initialize col_mods here\n",
    "    for value in section:\n",
    "        line = ' '.join([word['text'] for word in value])\n",
    "        if line.startswith('Field Name:') and len(value) > 2: # Exclude the explanation of \"Field Name\" itself on page 3\n",
    "            col_mods = ColCustomization(short_name=value[-1]['text'][1:-1]) # Get the field name minus the enclosing parentheses\n",
    "            full_name = ' '.join(word['text'] for word in value[2:-1])\n",
    "            new_name = src.pdfutils.clean_name(full_name.lower(), patterns=patterns)\n",
    "            is_fk = any([word in new_name for word in fk_markers])\n",
    "            col_mods.new_name = new_name\n",
    "            if any([w in new_name for w in ['year', 'number', 'precinct']]):\n",
    "                col_mods.dtype = \"Integer\"\n",
    "            if 'date' in new_name:\n",
    "                col_mods.dtype = \"Date\"\n",
    "        elif line.startswith('Format:') and not col_mods.dtype:\n",
    "            if \"Alphanumeric\" in line:\n",
    "                col_mods.dtype = \"String\"\n",
    "            if \"Numeric\" in line and not col_mods.dtype:\n",
    "                col_mods.dtype = \"Float\"\n",
    "        elif line.startswith('Description:'):\n",
    "            in_description = True\n",
    "        if in_description is True:\n",
    "            if (line.startswith('Value') or line.startswith('VALUE')) and len(value) <= 3: # Maximum number of words in a column heading\n",
    "                col_starts = src.pdfutils.get_word_starts_x(value)\n",
    "                in_table = True\n",
    "                table = [(line, value)]\n",
    "            elif in_table is True and (abs(col_starts[0] - src.pdfutils.get_word_starts_x(value)[0]) < .5 or abs(col_starts[1] - src.pdfutils.get_word_starts_x(value)[0]) < .5):\n",
    "                table.append((line, value))\n",
    "            elif in_table is True:\n",
    "                col_mods.definitions = src.pdfutils.parse_table(table)\n",
    "                in_table = False\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    if col_mods is not None:\n",
    "        if not col_mods.definitions and table:\n",
    "            col_mods.definitions = src.pdfutils.parse_table(table)\n",
    "        if col_mods.definitions:\n",
    "            col_mods.is_fk = True\n",
    "        column_customizations.append(col_mods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_zoning(pdf_path):\n",
    "    all_tables = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            # Extract raw text as lines\n",
    "            lines = page.extract_text().splitlines()\n",
    "            # Extract tables\n",
    "            tables = page.extract_tables()\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Find the position of the table in the raw text\n",
    "                table_start_line = find_table_start(lines, table)\n",
    "                # Extract the line before the table, if available\n",
    "                label_line = (\n",
    "                    lines[table_start_line - 2] if table_start_line > 0 else None\n",
    "                )\n",
    "                table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                if label_line is not None:\n",
    "                    if \"APPENDIX\" in label_line:\n",
    "                        label_line = re.sub(\"APPENDIX.*: \", \"\", label_line)\n",
    "                        label_line = re.sub(\" +\", \"_\", label_line.lower())\n",
    "                        prev_label_line = label_line\n",
    "                    elif \"PLUTO DATA DICTIONARY\" in label_line:\n",
    "                        label_line = None\n",
    "                    elif \"APPENDIX\" not in label_line:\n",
    "                        print(\"what's this?: \", print('label_line is', label_line))\n",
    "                        table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                    if label_line != None:\n",
    "                        all_tables[label_line] = table\n",
    "                    else:\n",
    "                        all_tables[prev_label_line] = all_tables[prev_label_line] + table\n",
    "                else:\n",
    "                    print('table_index is', table_index)\n",
    "                    print('missed:', lines[table_start_line])\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "def find_table_start(lines, table):\n",
    "    \"\"\"\n",
    "    Identify the start of the table in the text by matching table rows\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        # Convert the table's first row into a string and search for it in the text\n",
    "        table_row = \" \".join(str(cell) for cell in table[1] if cell)  # Skip empty cells\n",
    "        if line in table_row:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add tables from appendixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = parse_zoning(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess dictionary keys by truncating last letter (for singular/plural matching)\n",
    "truncated_keys = {key[:-1]: value for key, value in table_dicts.items()}\n",
    "\n",
    "# Create a sorted list of `new_name` for efficient prefix search\n",
    "sorted_new_names = sorted(item.new_name for item in column_customizations)\n",
    "item_dict = {item.new_name: item for item in column_customizations}\n",
    "\n",
    "# Function to find matching prefixes using bisect\n",
    "def find_matching_keys(prefix):\n",
    "    i = bisect_left(sorted_new_names, prefix)\n",
    "    matches = []\n",
    "    while i < len(sorted_new_names) and sorted_new_names[i].startswith(prefix):\n",
    "        matches.append(sorted_new_names[i])\n",
    "        i += 1\n",
    "    return matches\n",
    "\n",
    "# Apply updates\n",
    "for key, value in truncated_keys.items():\n",
    "    matches = find_matching_keys(key)\n",
    "    for match in matches:\n",
    "        item_dict[match].definitions = value  # Update definitions\n",
    "        item_dict[match].is_fk = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse Appendix D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the last table, which isn't actually a table, just text arranged in a table-like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_data(data, x_threshold=10):\n",
    "    result = []\n",
    "    for group in data:\n",
    "        subgroups = []\n",
    "        subgroup = [group[0]]\n",
    "        for item in group[1:]:\n",
    "            if item[\"x0\"] - subgroup[-1][\"x1\"] <= x_threshold:\n",
    "                subgroup.append(item)\n",
    "            else:\n",
    "                subgroups.append(subgroup)\n",
    "                subgroup = [item]\n",
    "        subgroups.append(subgroup)\n",
    "        result.append(subgroups)\n",
    "    return result\n",
    "\n",
    "def merge_sublists(data):\n",
    "    # Extract the first sublist\n",
    "    first_sublist = data[0]\n",
    "\n",
    "    # Iterate over the remaining sublists\n",
    "    for sublist in data[1:]:\n",
    "        for subsublist in sublist:\n",
    "            # Determine the x-range of the sub-sub-list\n",
    "            start = min(item[\"x0\"] for item in subsublist)\n",
    "            stop = max(item[\"x1\"] for item in subsublist)\n",
    "\n",
    "            # Find the appropriate sub-sub-list in the first sublist to append to\n",
    "            for target_subsublist in first_sublist:\n",
    "                target_start = min(item[\"x0\"] for item in target_subsublist)\n",
    "                target_stop = max(item[\"x1\"] for item in target_subsublist)\n",
    "\n",
    "                if target_start <= start <= target_stop:\n",
    "                    target_subsublist.extend(subsublist)\n",
    "                    break\n",
    "\n",
    "    return [first_sublist]\n",
    "\n",
    "\n",
    "\n",
    "def fix_row(row):\n",
    "    first_sort = sorted(row, key=lambda x: (x[\"top\"], x[\"x0\"]))\n",
    "    # Partition based on top:\n",
    "    from itertools import groupby\n",
    "    grouped_items = groupby(first_sort, lambda x: x[\"top\"])\n",
    "    result = {}\n",
    "    for key, item in grouped_items:\n",
    "        result[key] = list(item)\n",
    "    grouped_by_top = [result[k] for k in result.keys()]\n",
    "    restructured_data = restructure_data(grouped_by_top)\n",
    "    merged_data = merge_sublists(restructured_data)\n",
    "    print(\"merged_data is\", merged_data)\n",
    "    return merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def trim_lines_outside_table(lines, table_top_boundary_text=None, table_bottom_boundary_text=None):\n",
    "    \"\"\"Returns the index of the first line to contain the specified table_top_boundary_text\n",
    "\n",
    "    Args:\n",
    "        lines (_type_): _description_\n",
    "        table_top_boundary_text (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for idx, line in enumerate(lines):\n",
    "        if table_top_boundary_text is not None and table_top_boundary_text in \" \".join(\n",
    "            [word[\"text\"] for word in line]\n",
    "        ):\n",
    "            top_trim_line = idx\n",
    "            continue\n",
    "        elif (\n",
    "            table_bottom_boundary_text is not None\n",
    "            and table_bottom_boundary_text in \" \".join([word[\"text\"] for word in line])\n",
    "        ):\n",
    "            bottom_trim_line = idx\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    trimmed_lines = [\n",
    "        line\n",
    "        for idx, line in enumerate(lines)\n",
    "        if idx > top_trim_line and idx < bottom_trim_line\n",
    "    ]\n",
    "    return trimmed_lines\n",
    "\n",
    "\n",
    "def group_words_by_row(words, y_thresh=5):\n",
    "    \"\"\"Groups words into rows based on vertical proximity, allowing small deviations in top values.\"\"\"\n",
    "    words = sorted(words, key=lambda w: w['top'])  # Sort words top-to-bottom\n",
    "    rows = []\n",
    "    for word in words:\n",
    "        added = False\n",
    "        for row in rows:\n",
    "            # Compare with first word in the row for stability\n",
    "            if abs(word['top'] - max([w['top'] for w in row])) <= y_thresh:\n",
    "                row.append(word)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            rows.append([word])\n",
    "\n",
    "    return rows\n",
    "\n",
    "# def group_words_by_row(words, y_thresh=5):\n",
    "#     \"\"\"Groups words into rows based on vertical proximity, allowing small deviations in top values.\"\"\"\n",
    "#     words = sorted(words, key=lambda w: w['top'])  # Sort words top-to-bottom\n",
    "#     rows = []\n",
    "\n",
    "#     for word in words:\n",
    "#         added = False\n",
    "#         for row in rows:\n",
    "#             # Compare with first word in the row for stability\n",
    "#             if abs(word['top'] - row[0]['top']) <= y_thresh:\n",
    "#                 row.append(word)\n",
    "#                 added = True\n",
    "#                 break\n",
    "#         if not added:\n",
    "#             rows.append([word])\n",
    "\n",
    "#     return rows\n",
    "\n",
    "def merge_words_in_row(row, x_thresh=10):\n",
    "    \"\"\"\n",
    "    Merges words in a single row, considering the provided x_thresh for horizontal grouping.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of merged text blocks, each with the merged text and bounding box.\n",
    "    \"\"\"\n",
    "    row.sort(key=lambda w: (w['x0'], w['top']))  # Sort words left-to-right\n",
    "    merged_blocks = []\n",
    "    current_block = []\n",
    "    print(\"row is:\", row)\n",
    "    for word in row:\n",
    "        print('word is:', word)\n",
    "        if current_block and (word['x0'] - current_block[-1]['x1']) <= x_thresh:\n",
    "            current_block.append(word)\n",
    "            print(f\"Appended {word}\")\n",
    "        else:\n",
    "            if current_block:\n",
    "                current_block.sort(key=lambda w: w['top']) # Sort block by top coordinate to get text in each table cell correctly ordered.\n",
    "                merged_blocks.append(current_block)\n",
    "            current_block = [word]\n",
    "\n",
    "    if current_block:\n",
    "        print(\"current_block is\", current_block)\n",
    "        merged_blocks.append(current_block)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": \" \".join(w[\"text\"] for w in block),\n",
    "            \"x0\": min(w[\"x0\"] for w in block),\n",
    "            \"x1\": max(w[\"x1\"] for w in block),\n",
    "            \"top\": min(w[\"top\"] for w in block),\n",
    "            \"bottom\": max(w[\"bottom\"] for w in block),\n",
    "        }\n",
    "        for block in merged_blocks\n",
    "    ]\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_lines_in_row(lines, y_thresh):\n",
    "    merged_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if not merged_lines:\n",
    "            merged_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        prev_line = merged_lines[-1]\n",
    "        \n",
    "        # Compute merging condition\n",
    "        min_top_current = min(word[\"top\"] for word in line)\n",
    "        max_bottom_prev = max(word[\"bottom\"] for word in prev_line)\n",
    "        \n",
    "        if min_top_current - max_bottom_prev < y_thresh:\n",
    "            # Merge into the previous line\n",
    "            merged_lines[-1].extend(line)\n",
    "        else:\n",
    "            # Start a new line\n",
    "            merged_lines.append(line)\n",
    "\n",
    "    # Now merge words by `x0` within each line\n",
    "    result = []\n",
    "    \n",
    "    for line in merged_lines:\n",
    "        grouped = defaultdict(list)\n",
    "        \n",
    "        for (_, word) in enumerate(line):\n",
    "            grouped[word[\"x0\"]].append(word)\n",
    "        \n",
    "        merged_words = []\n",
    "        \n",
    "        for x0 in sorted(grouped.keys()):  # Preserve order\n",
    "            words = grouped[x0]\n",
    "            merged_text = \" \".join(w[\"text\"] for w in words)\n",
    "            x1 = max(w[\"x1\"] for w in words)\n",
    "            top = min(w[\"top\"] for w in words)\n",
    "            bottom = max(w[\"bottom\"] for w in words)\n",
    "            \n",
    "            merged_words.append({\"text\": merged_text, \"x0\": x0, \"x1\": x1, \"top\": top, \"bottom\": bottom})\n",
    "        \n",
    "        result.append(merged_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def detect_header_by_uppercase(rows):\n",
    "    \"\"\"Identifies the header row by checking if all words are uppercase.\"\"\"\n",
    "    header_row = []\n",
    "    body_rows = []\n",
    "\n",
    "    for row in rows:\n",
    "        if all(word[\"text\"].isupper() for word in row):  # All words must be uppercase\n",
    "            header_row = header_row + row\n",
    "        else:\n",
    "            body_rows.append(row)\n",
    "\n",
    "    return header_row, body_rows\n",
    "\n",
    "\n",
    "def merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh):\n",
    "    \"\"\"\n",
    "    Groups words into rows and merges horizontally close words.\n",
    "    \"\"\"\n",
    "    rows = group_words_by_row(words, header_y_thresh)\n",
    "    print(\"ROWS ARE\", rows)\n",
    "    trimmed_rows = trim_lines_outside_table(rows, table_top_boundary_text=\"APPENDIX D: LAND USE CATEGORIES\", table_bottom_boundary_text=\"NOTES:\")\n",
    "    header_row, body_rows = detect_header_by_uppercase(trimmed_rows)\n",
    "    merged_header = merge_words_in_row(header_row, header_x_thresh)\n",
    "    # merged_rows = [merge_words_in_row(row, body_x_thresh) for row in body_rows]\n",
    "    merged_rows = [fix_row(row) for row in body_rows]\n",
    "    # merged_rows = merge_lines_in_row(merged_rows, body_y_thresh)\n",
    "    all_rows = [merged_header] + merged_rows\n",
    "    return all_rows\n",
    "    # return merged_rows\n",
    "\n",
    "def assign_columns_to_blocks(merged_rows, column_gap_thresh=20, ncol=3):\n",
    "    \"\"\"\n",
    "    Assigns a column index to each merged text block by detecting significant gaps in x0 values.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_rows: List of lists of merged word blocks.\n",
    "    - column_gap_thresh: Minimum gap to consider as a column boundary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list where each element is a tuple (column_index, word_block_dict).\n",
    "    \"\"\"\n",
    "    all_x_values = sorted(set(block[\"x0\"] for row in merged_rows for block in row))\n",
    "\n",
    "    # Detect gaps to determine column boundaries\n",
    "    column_boundaries = [all_x_values[0]]\n",
    "    for i in range(1, len(all_x_values)):\n",
    "        if all_x_values[i] - all_x_values[i - 1] > column_gap_thresh:\n",
    "            column_boundaries.append(all_x_values[i])\n",
    "\n",
    "    def get_column_index(x0):\n",
    "        \"\"\"Finds the appropriate column index for a given x0 value.\"\"\"\n",
    "        for i, boundary in enumerate(column_boundaries):\n",
    "            if x0 < boundary:\n",
    "                return max(i - 1, 0)\n",
    "        return len(column_boundaries) - 1\n",
    "\n",
    "    structured_output = []\n",
    "    for idx,row in enumerate(merged_rows):\n",
    "        # row_output = [((idx, (get_column_index(block[\"x0\"]))), block) for block in row] # Store row and column indices for each block with that block\n",
    "        # row_output = [cell['text'] for cell in row]\n",
    "        row_output = [cell for cell in row]\n",
    "        # row_output = [row for row in row_output if row[0] <= ncol]\n",
    "        structured_output.append(row_output)\n",
    "\n",
    "    return structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 20\n",
    "body_x_thresh = 10\n",
    "body_y_thresh = 10\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "ncol = 3\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_into_sublists(row, x_thresh=10):\n",
    "#     \"\"\"\n",
    "#     Splits the list of words into sublists based on a threshold difference between x1 of a word and x0 of the next word.\n",
    "    \n",
    "#     Returns:\n",
    "#     - A list of sublists, each containing words that are close to each other horizontally.\n",
    "#     \"\"\"\n",
    "#     sublists = []\n",
    "#     current_sublist = []\n",
    "\n",
    "#     for i, word in enumerate(row):\n",
    "#         if current_sublist:\n",
    "#             last_word = current_sublist[-1]\n",
    "#             if (word['x0'] - last_word['x1']) > x_thresh:\n",
    "#                 sublists.append(current_sublist)\n",
    "#                 current_sublist = [word]\n",
    "#             else:\n",
    "#                 current_sublist.append(word)\n",
    "#         else:\n",
    "#             current_sublist = [word]\n",
    "\n",
    "#     if current_sublist:\n",
    "#         sublists.append(current_sublist)\n",
    "\n",
    "#     return sublists\n",
    "\n",
    "# def merge_words_in_sublists(sublists):\n",
    "#     \"\"\"\n",
    "#     Merges words within each sublist, sorting by x0 and then by top when x0 values are the same.\n",
    "    \n",
    "#     Returns:\n",
    "#     - A list of merged text blocks, each with the merged text and bounding box.\n",
    "#     \"\"\"\n",
    "#     merged_blocks = []\n",
    "\n",
    "#     for sublist in sublists:\n",
    "#         sublist.sort(key=lambda w: (w['x0'], w['top']))\n",
    "#         merged_text = \" \".join(w[\"text\"] for w in sublist)\n",
    "#         merged_block = {\n",
    "#             \"text\": merged_text,\n",
    "#             \"x0\": min(w[\"x0\"] for w in sublist),\n",
    "#             \"x1\": max(w[\"x1\"] for w in sublist),\n",
    "#             \"top\": min(w[\"top\"] for w in sublist),\n",
    "#             \"bottom\": max(w[\"bottom\"] for w in sublist),\n",
    "#         }\n",
    "#         merged_blocks.append(merged_block)\n",
    "\n",
    "#     return merged_blocks\n",
    "\n",
    "# def process_row(row, x_thresh=10):\n",
    "#     \"\"\"\n",
    "#     Processes a single row by splitting it into sublists and merging words within each sublist.\n",
    "    \n",
    "#     Returns:\n",
    "#     - A list of merged text blocks for the row.\n",
    "#     \"\"\"\n",
    "#     sublists = split_into_sublists(row, x_thresh)\n",
    "#     merged_blocks = merge_words_in_sublists(sublists)\n",
    "#     return merged_blocks\n",
    "\n",
    "# # Example usage\n",
    "# # last_table = [process_row(row, x_thresh=10) for row in merged_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 20\n",
    "body_x_thresh = 10\n",
    "body_y_thresh = 10\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "ncol = 3\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)\n",
    "    last_table = assign_columns_to_blocks(merged_rows, column_gap_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict['building_class'].definitions = last_table[1:] # Exclude the first row, which is the column headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"wb\") as f:\n",
    "    dill.dump({'column_customizations': column_customizations, 'table_dicts': table_dicts, 'last_table': last_table}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
