{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "import dill\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Import third-party libraries\n",
    "import geopandas as gpd\n",
    "from geoalchemy2 import Geometry\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.event import listen\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import dbapi2 as sqlite\n",
    "\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the environment\n",
    "with open(\"environment_data/select.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    print(dataset.short_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dictionaries that define things specific to each dataset.\n",
    "* `prefix` is the directory where the dataset in question is expected to be found.\n",
    "*`cols_to_drop` are columns that I am sure I will not need.\n",
    "* `cols_to_rename` are key-value pairs where keys are column names found in the dataset and values are the names they are to be changed to. This is sometimes necessary to standardize column names that have the same type of data but different spellings between datasets. I also choose to rename some columns to standard names that are not abbreviated if I think the abbreviation meanings are not obvious. \n",
    "* `dtype_exceptions` are key-value pairs where the keys are the column names and the values are the datatype that I want to specify for the column when it is put in the database. For example date could appear in the json dataset as an integer or a string. When inserted to a table, I want it to be a proper date type.\n",
    "* `lookup_columns` are categorical variables (usually strings, but in some cases like zip code they can be numbers.). For these, I replace them with an integer foreign key and put the actual name (as well as any additional information about the category) into a lookup (join) table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shared dataset configurations\n",
    "SHARED_DATASET_CONFIGS = {\n",
    "    \"prefix\": PROJECT_DATA,\n",
    "    \"cols_to_drop\": [ # These columns are dropped from all datasets because I decided I won't need them\n",
    "        \"id\",\n",
    "        \"sid\",\n",
    "        \"position\",\n",
    "        \"created_at\",\n",
    "        \"created_meta\",\n",
    "        \"updated_at\",\n",
    "        \"updated_meta\",\n",
    "        \"borough\",\n",
    "        \"meta\",\n",
    "    ],\n",
    "    \"cols_to_rename\": {}, # These are columns that are to be renamed in all datasets\n",
    "    \"lookup_columns\": [],\n",
    "    \"dtype_mappings\": { # These are columns that are to be cast to a specific data type\n",
    "        \"zip_code\": String,\n",
    "        \"meta_data\": String,\n",
    "        \"postcode\": String,\n",
    "        \"calendar_date\": Date,\n",
    "        \"number\": Integer,\n",
    "        \"text\": String,\n",
    "        \"point\": String,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define specific dataset configurations\n",
    "SPECIFIC_DATASET_CONFIGS = {\n",
    "    \"lien_data\": {\n",
    "        \"prefix\": f\"{PROJECT_DATA}/intermediate_files\",\n",
    "        \"cols_to_drop\": [],\n",
    "        \"cols_to_rename\": {\"BORO\": \"borough\"},\n",
    "        \"lookup_columns\": [],\n",
    "        \"dtype_mappings\": {},\n",
    "    },\n",
    "    \"assessment_data\": {\n",
    "        \"prefix\": f\"{PROJECT_DATA}/intermediate_files\",\n",
    "        \"cols_to_drop\": [],\n",
    "        \"cols_to_rename\": {\n",
    "            \"BLDGCL\": \"building_class\",\n",
    "            \"TAXCLASS\": \"tax_class_code\",\n",
    "            \"Zip Codes\": \"zip_code\",\n",
    "        },\n",
    "        \"lookup_columns\": [\"building_class\", \"street_name\", \"owner\", \"zip_code\"],\n",
    "        \"dtype_mappings\": {},\n",
    "    },\n",
    "    \"PLUTO\": {\n",
    "        \"prefix\": None,\n",
    "        \"cols_to_drop\": [],\n",
    "        \"cols_to_rename\": {\n",
    "            \"BldgClass\": \"building_class\",\n",
    "            \"ZipCode\": \"zip_code\",\n",
    "            \"SchoolDist\": \"school_district\",\n",
    "            \"PolicePrct\": \"police_precinct\",\n",
    "            \"Council\": \"council_district\",\n",
    "            \"OwnerName\": \"owner\",\n",
    "            \"HistDist\": \"historic_district\",\n",
    "            \"SanitDistrict\": \"sanitation_district\",\n",
    "            \"SanitSub\": \"sanitation_subdistrict\",\n",
    "            \"Borough\" : \"borough\"\n",
    "        },\n",
    "        \"lookup_columns\": [\n",
    "            \"borough\"\n",
    "            \"building_class\",\n",
    "            \"street_name\",\n",
    "            \"owner\",\n",
    "            \"zip_code\",\n",
    "            \"school_district\",\n",
    "            \"council_district\",\n",
    "            \"police_precinct\",\n",
    "        ],\n",
    "        \"dtype_mappings\" : {\n",
    "            \"school_district\": Integer,\n",
    "            \"council_district\": Integer,\n",
    "            \"police_precinct\": Integer,\n",
    "            \"YearBuilt\": Date,\n",
    "            \"YearAlter1\": Date,\n",
    "            \"YearAlter2\": Date,\n",
    "            \"APPDate\": Date,\n",
    "            \"geometry\": LargeBinary,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'{SQLITE_PATH}?check_same_thread=False', echo=False)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Configure the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@event.listens_for(engine, \"connect\")\n",
    "def load_spatialite(dbapi_conn, connection_record):\n",
    "    print(\"Loading SpatiaLite extension\")\n",
    "    dbapi_conn.enable_load_extension(True)\n",
    "    dbapi_conn.load_extension(\"mod_spatialite\")\n",
    "    dbapi_conn.enable_load_extension(False)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connection established\")\n",
    "    result = conn.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n",
    "# Enable WAL mode\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
    "\n",
    "# Initialize spatial metadata if not already present\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually create the borough codes lookup table\n",
    "* These are standardized and available many places, however I could not find a single official source of record to programatically get them from, since there are only five of them, I enter them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_codes = {'Manhattan' : 1,\n",
    "'Bronx' : 2,\n",
    "'Brooklyn' : 3,\n",
    "'Queens' : 4,\n",
    "'Staten Island' : 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "def create_lookup_table_simple(engine=engine, metadata=metadata, lookup_table_name='new_lookup_table', lookup_column_name='name'):\n",
    "    lookup_table = Table(\n",
    "        lookup_table_name,\n",
    "        metadata,\n",
    "        Column('id', Integer, primary_key=True, autoincrement=False),\n",
    "        Column(lookup_column_name, String, unique=True, nullable=False, default=\"NO DATA\"),\n",
    "        extend_existing = True\n",
    "    )\n",
    "    if table_exists(engine, lookup_table_name):\n",
    "        print(\"Table exists\")\n",
    "    else:\n",
    "        lookup_table.create(engine)\n",
    "    return lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Populate the table with the borough codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_lookup_table = create_lookup_table_simple(engine=engine, metadata=metadata, lookup_table_name='boroughs', lookup_column_name='borough')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    for key,value in borough_codes.items():\n",
    "        stmt = insert(borough_lookup_table).values(id = value, borough = key).on_conflict_do_nothing()\n",
    "        connection.execute(stmt)\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file uses some non-standard borough codes, so we need to replace them\n",
    "replacement_dict = {\"MN\": 1, \"BX\": 2, \"BN\": 3, \"QN\": 4, \"SI\": 5}\n",
    "\n",
    "\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO24v4.gdb\"\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "layer_dict = process_layers(gdb_path, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in layer_dict.keys():\n",
    "    layer_dict[k] = layer_dict[k]\n",
    "    layer_dict[k].rename(columns=SPECIFIC_DATASET_CONFIGS['PLUTO']['cols_to_rename'], inplace=True)\n",
    "    print(layer_dict[k].columns)\n",
    "    layer_dict[k]['borough'] = layer_dict[k]['borough'].replace(replacement_dict)\n",
    "    # Convert geometries to WKT\n",
    "    if 'geometry' in layer_dict:\n",
    "        layer_dict[k]['wkb'] = layer_dict[k]['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "\n",
    "# Create ORM classes\n",
    "orm_classes = create_orm_classes(layer_dict, Base, SPECIFIC_DATASET_CONFIGS['PLUTO']['dtype_exceptions'])\n",
    "\n",
    "# Create tables in the database\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SessionLocal() as session:\n",
    "    for layer in layer_dict.keys():\n",
    "        write_layer_to_db(layer_dict, layer, orm_classes, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    if 'the_geom' in dataset.columns:\n",
    "        print(dataset.name, dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create lookup tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lot zoning info\n",
    "* The main datasets used in this project all reference specific lots, so I thought it would be good to have the zoning for those lots.\n",
    "* The most official source I could find for this was a pdf attached to the NYC Open Data dataset listing zoning for property lots in NYC.\n",
    "* The extraction code is by necessity unique and unlikely to be reusable. If using this notebook as a template for future analyses, I would put here any tasks that are unique to the project and which I don't expect to be reusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download an additional data dictionary I found for another dataset that explains some of the codes used in PLUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = \"https://data.cityofnewyork.us/api/views/fdkv-4t4z/files/997a4707-2e53-48bc-9652-3d69badca007?download=true&filename=zoningtaxlotdatabase_datadictionary\"\n",
    "filename = \"zoning_definition_dict.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader(\n",
    "            url=url,\n",
    "            download_path=f\"{PROJECT_DATA}/dictionaries/\",\n",
    "            outfile_name=filename,\n",
    "            bigfile=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract the tables from the zoning definition dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = parse_zoning_def_dict(f\"{PROJECT_DATA}/dictionaries/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create lookup table for zoning code definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    for key in all_tables.keys():\n",
    "        zoning_lookup_table = create_lookup_table(engine, lookup_table_name=key, text_column_name='code')\n",
    "        for value in all_tables[key]:\n",
    "            stmt = insert(zoning_lookup_table).values(code = value[0], info = value[1]).on_conflict_do_nothing()\n",
    "            connection.execute(stmt)\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explanations of zoning codes.\n",
    "* I could only find this information in pdf form.\n",
    "* I discovered how hard PDFs can be to parse.\n",
    "* I had to do a lot of customization for just this specific pdf. I could have just manually cut and pasted the data from the pdf in the amount of time it took me to do that.\n",
    "* I still think it was good to do for reproducibility reasons, but in the future I will try to avoid working with datasets that have important information only in PDF format.\n",
    "* The following functions extract the tables from the pdf, detecting footnotes, and then subsitute the foonote number for the footnote text within the dataframe (so that it will end up as part of the relevant record in the databasee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/assets/bronxcb8/pdf/zoning_table_all.pdf\"\n",
    "filename = \"zoning_table_all.pdf\"  # Path to save the pdf containing the info we need\n",
    "\n",
    "downloader(\n",
    "            url=url,\n",
    "            download_path=f\"{PROJECT_DATA}/dictionaries/\",\n",
    "            outfile_name=filename,\n",
    "            bigfile=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the above functions to extract the data from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes = parse_zoning_details(f\"{PROJECT_DATA}/dictionaries/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a MetaData instance\n",
    "# metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# # Reflect the table\n",
    "zoning_districts_lookup = Table(\"zoning_districts\", metadata, autoload_with=engine)\n",
    "\n",
    "for tablename in tables_and_footnotes.keys():\n",
    "    print(tablename)\n",
    "    df = tables_and_footnotes[tablename]['df']\n",
    "    df.name = df.index.name\n",
    "    with engine.connect() as conn:\n",
    "        for series_name, series in df.items():\n",
    "            tdf = pd.DataFrame(series)\n",
    "            tdf.reset_index(inplace=True)\n",
    "            jstring = pd.DataFrame(tdf).to_json()\n",
    "            stmt = insert(zoning_districts_lookup).values(code=series_name, info=jstring).prefix_with(\"OR IGNORE\")\n",
    "            conn.execute(stmt)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PDF parsed above still has some definitions that are in text outside the tables. From `zoning_table_all.pdf`:\n",
    "\n",
    ">C1-1 through C1-5 and C2-1 through C2-5 are commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\n",
    "\n",
    "* I need to manually create the object to hold this information and put it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_zones = {}\n",
    "info = \"Commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\"\n",
    "for i in range(1,6):\n",
    "    more_zones[f'C1-{i}'] = info\n",
    "    more_zones[f'C2-{i}'] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    for key in more_zones.keys():\n",
    "        print(more_zones[key])\n",
    "        stmt = insert(zoning_districts_lookup).values(code=key, info=more_zones[key]).prefix_with(\"OR IGNORE\")\n",
    "        conn.execute(stmt)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a few more code meanings \n",
    "* From [NYC Department of Tax and Finance Data Dictionary](https://www.nyc.gov/assets/finance/downloads/tar/tarfieldcodes.pdf):\n",
    "    * LandUse\n",
    "    * OwnerType\n",
    "    * Easment code\n",
    "* Additional information about commercial zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/commercial_zoning_data_tables.pdf).\n",
    "* Additional information about residential zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/residence_zoning_data_tables.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and add to the database the zoning information for parcels of land in NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = \"fdkv-4t4z\"\n",
    "url = f\"https://data.cityofnewyork.us/resource/{dataset}.json\" \n",
    "count = getDatasetRowCount(url)\n",
    "metadata_url = f\"https://data.cityofnewyork.us/api/views/{dataset}.json\"\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "metadata = reader(urlopen(metadata_url))\n",
    "metadata = json.load(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"NYC Zoning Tax Lot Database\"\n",
    "tabname = re.sub(' ', '_', title)\n",
    "datatype_mappings = SHARED_DATASET_CONFIGS['dtype_mappings']\n",
    "\n",
    "column_info =  [{'fieldName': entry['fieldName'], 'dataType' : datatype_mappings.get(entry['dataTypeName'])} for entry in metadata['columns']]\n",
    "\n",
    "allvals = []\n",
    "\n",
    "for info in column_info:\n",
    "    if info['dataType'] is String:\n",
    "        url = f'https://data.cityofnewyork.us/resource/{dataset}.json?$select=distinct({info['fieldName']})'\n",
    "        unique_vals = json.load(reader(urlopen(url)))\n",
    "        allvals.append(unique_vals)\n",
    "\n",
    "vals_list = [val for val in allvals if len(val) > 0]\n",
    "\n",
    "newlist = []\n",
    "\n",
    "for vals_list in allvals:\n",
    "    vals_list = [val for val in vals_list if len(val) > 0]\n",
    "    if len(vals_list) > 0:\n",
    "        vals_list = {list(vals_list[0].keys())[0]: [d[list(vals_list[0].keys())[0]] for d in vals_list  if len(d) > 0]}\n",
    "        newlist.append(vals_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add to the database. I had to set journling mode to WAL to avoid concurrency issues, merely waiting between commits was not sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
