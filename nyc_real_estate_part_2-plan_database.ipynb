{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the database based on the data\n",
    "* #### This notebook parses metadata associated with some of the datasets, most especially the PLUTO dataset, which contains columns that are also in many other datasets I looked at on NYCOpenData.\n",
    "* #### In some cases I had to search around to find more complete definitions than were included in the data dictionary associated with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import dill\n",
    "time.sleep(5) # This seems to be necessary to avoid an error about dill not being loaded in the next cell. Sometimes even that is not enough and this cell needs to be run again.\n",
    "from bisect import bisect_left\n",
    "from itertools import tee\n",
    "from src.models import ColCustomization\n",
    "import src.helpers\n",
    "from src.helpers import *\n",
    "from src.pdfutils import *\n",
    "from geoalchemy2 import Geometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/select.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = dataset_info_dict['assessments'].col_types\n",
    "print(column_types)\n",
    "print(len(column_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from sqlalchemy import Integer, String, Date, Float, JSON\n",
    "# from your_module import Geometry  # Assuming you have Geometry defined somewhere\n",
    "\n",
    "def is_number(value):\n",
    "    \"\"\"Check if a string represents a number (integer or float).\"\"\"\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def set_dtypes(filename, column_types):\n",
    "    print(f'Starting {filename}')\n",
    "    if not filename.endswith('.json'):\n",
    "        print(f'{filename} is not a json file, skipping for now...')\n",
    "        return column_types\n",
    "\n",
    "    # Build the initial mapping of column names to SQLAlchemy types.\n",
    "    # Here, non-numeric types are handled in a nested conditional expression.\n",
    "    new_column_types = {\n",
    "        column: (\n",
    "            Integer if dtype == \"number\" or column in ['created_at', 'updated_at'] else\n",
    "            String if dtype == \"text\" or column == \"sid\" else\n",
    "            Date if dtype == \"calendar_date\" else\n",
    "            JSON if dtype == \"location\" else\n",
    "            Geometry(geometry_type='POINT', srid=4326) if dtype == \"point\" else\n",
    "            Geometry(geometry_type='MULTIPOLYGON', srid=4326) if dtype == \"multipolygon\" else\n",
    "            (Geometry(geometry_type='POLYGON', srid=4326) if dtype == \"polygon\" else dtype)\n",
    "\n",
    "        )\n",
    "        for column, dtype in column_types.items() if not column.startswith(':@')\n",
    "    }\n",
    "    print(f'Initial new_column_types: {new_column_types}')\n",
    "\n",
    "    # Compute numeric column indices (assumes order of keys corresponds to data positions)\n",
    "    column_names = list(column_types.keys())\n",
    "    print(f'column_names: {column_names}')\n",
    "    numeric_indices = [\n",
    "        idx for idx, col in enumerate(column_names)\n",
    "        if column_types[col] == \"number\"\n",
    "    ]\n",
    "\n",
    "    # Open the file and iterate through rows one by one.\n",
    "    with open(filename, \"r\") as f:\n",
    "        # Assuming the JSON file is a top-level array.\n",
    "        rows = ijson.items(f, \"item\")\n",
    "        for row in rows:\n",
    "            # (Optional) Convert the values for known types.\n",
    "            # This section can be expanded as needed.\n",
    "            for col, dtype in column_types.items():\n",
    "                # When the dataset is represented as a dict per row:\n",
    "                if dtype == \"Integer\":\n",
    "                    row[col] = int(row[col]) if row[col] is not None else None\n",
    "                elif dtype == \"Float\":\n",
    "                    row[col] = float(row[col]) if row[col] is not None else None\n",
    "                elif dtype == \"Date\":\n",
    "                    row[col] = pd.to_datetime(row[col]) if row[col] is not None else None\n",
    "\n",
    "            # Check numeric columns for float support.\n",
    "            # If any numeric column has a value with decimals, update the type to Float.\n",
    "            for idx in numeric_indices:\n",
    "                # Assume row is a list-like object.\n",
    "                value = row[idx]\n",
    "\n",
    "                if value is None:\n",
    "                    continue  # Skip null values\n",
    "\n",
    "                # Check integer vs. float\n",
    "                if isinstance(value, (int, float)):\n",
    "                    if float(value) % 1 != 0:\n",
    "                        # Update the column type to Float.\n",
    "                        colname = column_names[idx]\n",
    "                        new_column_types[colname] = Float\n",
    "                        break  # No need to check further for this column\n",
    "\n",
    "                elif isinstance(value, str):\n",
    "                    value = value.strip()\n",
    "                    # If the numeric value in string contains a decimal, cast it as Float.\n",
    "                    if not value.isdigit() and is_number(value):\n",
    "                        if float(value) % 1 != 0:\n",
    "                            colname = column_names[idx]\n",
    "                            new_column_types[colname] = Float\n",
    "                            break\n",
    "\n",
    "    return new_column_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_info in dataset_info_dict.values():\n",
    "    # column_types = dataset_info.col_types\n",
    "    dataset_info.col_types = set_dtypes(filename=dataset_info.dataset_path, column_types = dataset_info.col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info.col_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### The MapPLUTO dictionary contains most of the information we need to interpret various codes and categories meaningfully.\n",
    "* #### Unfortunately, it is in PDF format (as are many of the data dictionaries on NYCOpenData), which made extracting all the relevant data a real pain, and I don't expect most of these functions will be fully reusable for other PDFs I may encounter in the future. My hope is that it will still give me a head start when I need to make custom functions for future PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"{PROJECT_DATA}/dictionaries/mapPLUTO_data_dictionary.pdf\"\n",
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section = map_pdf(filename, same_line_tolerance=0.3, start_page=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_markers = ['code', 'category', 'class', 'district', 'precinct', 'company', 'name', 'health_area', 'type', 'borough', 'name', 'health_area', 'health_center_district', 'overlay']\n",
    "\n",
    "column_customizations=[]\n",
    "\n",
    "for section in pdf_by_section:\n",
    "    in_table = False\n",
    "    in_description = False\n",
    "    header_added = False\n",
    "    table = None\n",
    "    col_mods = None  # Initialize col_mods here\n",
    "    for value in section:\n",
    "        line = ' '.join([word['text'] for word in value])\n",
    "        print(line)\n",
    "        if (line.startswith(\"Field Name: CENSUS\") or line.startswith(\"Field Name: HEALTH\")) and col_mods is not None: # Handle special cases where the dividing line (rectangle object) is not present in between column descriptions\n",
    "            if col_mods.short_name is not None:\n",
    "                print(\"Appending col_mods\", col_mods)\n",
    "                column_customizations.append(col_mods)\n",
    "        if line.startswith('Field Name:') and len(value) > 2: # Exclude the explanation of \"Field Name\" itself on page 3\n",
    "            col_mods = ColCustomization(short_name=value[-1]['text'][1:-1]) # Get the field name minus the enclosing parentheses\n",
    "            full_name = ' '.join(word['text'] for word in value[2:-1])\n",
    "            print('full_name', full_name)\n",
    "            new_name = clean_name(full_name.lower())\n",
    "            print('new_name', new_name)\n",
    "            col_mods.is_category = any([word in new_name for word in category_markers])\n",
    "            col_mods.new_name = new_name\n",
    "            if any([w in col_mods.new_name for w in ['year', 'number', 'precinct']]):\n",
    "                col_mods.dtype = \"Integer\"\n",
    "            if 'date' in col_mods.new_name:\n",
    "                col_mods.dtype = \"Date\"\n",
    "            print('col_mods', col_mods)\n",
    "        elif line.startswith('Format:') and not col_mods.dtype:\n",
    "            if \"Alphanumeric\" in line:\n",
    "                col_mods.dtype = \"String\"\n",
    "            if \"Numeric\" in line and not col_mods.dtype:\n",
    "                col_mods.dtype = \"Float\"\n",
    "        elif line.startswith('Description:'):\n",
    "            in_description = True\n",
    "        if in_description is True:\n",
    "            # print(\"LINE is\", line)\n",
    "            # if (line.startswith('Value') or line.startswith('VALUE') or line.startswith('BOROUGH JIA NAME')) and len(value) <= 3 and header_added is True: # Check if the line is a redundant table header, for when tables are split across pages\n",
    "            #     continue\n",
    "            if (line.startswith('Value') or line.startswith('VALUE')) and len(value) <= 3 and header_added is False: # Maximum number of words in a column heading\n",
    "                print(\"Detected table\")\n",
    "                col_starts = get_word_starts_x(value)\n",
    "                in_table = True\n",
    "                table = [(line, value)]\n",
    "                header_added = True # This is for dealing with tables that go across pages, and have the header again on the second page.\n",
    "            elif in_table is True and (abs(col_starts[0] - get_word_starts_x(value)[0]) < .5 or abs(col_starts[1] - get_word_starts_x(value)[0]) < .5):\n",
    "                table.append((line, value))\n",
    "                print(\"Appended line\", line)\n",
    "            elif in_table is True:\n",
    "                print(\"Table is\", table)\n",
    "                table = parse_table(table)\n",
    "                print(\"Now the table is\", table)\n",
    "                print('table[0][0] is', table[0][0])\n",
    "                if table[0][0].isdigit():\n",
    "                    print(\"Digits detected!\")\n",
    "                    col_mods.is_fk = True\n",
    "                col_mods.definitions = table\n",
    "                in_table = False\n",
    "                header_added = False\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    if col_mods is not None:\n",
    "        if not col_mods.definitions and table:\n",
    "            col_mods.definitions = parse_table(table)\n",
    "        if col_mods.definitions:\n",
    "            col_mods.is_category = True\n",
    "        if col_mods.dtype == \"Float\" and col_mods.is_category == True:\n",
    "            col_mods.dtype = \"Integer\"\n",
    "        print(\"Appending col_mods\", col_mods)\n",
    "        column_customizations.append(col_mods)\n",
    "    else:\n",
    "        print(\"col_mods was NONE!, col_mods is: \", col_mods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def parse_zoning(pdf_path):\n",
    "#     all_tables = {}\n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         for page_num, page in enumerate(pdf.pages, start=1):\n",
    "#             # Extract raw text as lines\n",
    "#             lines = page.extract_text().splitlines()\n",
    "#             # Extract tables\n",
    "#             tables = page.extract_tables()\n",
    "#             for table_index, table in enumerate(tables):\n",
    "#                 # Find the position of the table in the raw text\n",
    "#                 table_start_line = find_table_start(lines, table)\n",
    "#                 # Extract the line before the table, if available\n",
    "#                 label_line = (\n",
    "#                     lines[table_start_line - 2] if table_start_line > 0 else None\n",
    "#                 )\n",
    "#                 table = [row for row in table if \"Abbreviation\" not in row]\n",
    "#                 if label_line is not None:\n",
    "#                     if \"APPENDIX\" in label_line:\n",
    "#                         label_line = re.sub(\"APPENDIX.*: \", \"\", label_line)\n",
    "#                         label_line = re.sub(\" +\", \"_\", label_line.lower())\n",
    "#                         label_line = re.sub(\"s$\", \"\", label_line.lower()) # remove trailing plural s so as to match column names\n",
    "#                         prev_label_line = label_line\n",
    "#                     elif \"PLUTO DATA DICTIONARY\" in label_line:\n",
    "#                         label_line = None\n",
    "#                     elif \"APPENDIX\" not in label_line:\n",
    "#                         print(\"what's this?: \", print('label_line is', label_line))\n",
    "#                         table = [row for row in table if \"Abbreviation\" not in row]\n",
    "#                     if label_line != None:\n",
    "#                         all_tables[label_line] = table\n",
    "#                     else:\n",
    "#                         all_tables[prev_label_line] = all_tables[prev_label_line] + table\n",
    "#                 else:\n",
    "#                     print('table_index is', table_index)\n",
    "#                     print('missed:', lines[table_start_line])\n",
    "#     return all_tables\n",
    "\n",
    "\n",
    "# def find_table_start(lines, table):\n",
    "#     \"\"\"\n",
    "#     Identify the start of the table in the text by matching table rows\n",
    "#     \"\"\"\n",
    "#     for i, line in enumerate(lines):\n",
    "#         # Convert the table's first row into a string and search for it in the text\n",
    "#         table_row = \" \".join(str(cell) for cell in table[1] if cell)  # Skip empty cells\n",
    "#         if line in table_row:\n",
    "#             return i\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add tables from appendixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = parse_zoning(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess dictionary keys by truncating last letter (for singular/plural matching)\n",
    "truncated_keys = {key[:-1]: value for key, value in table_dicts.items()}\n",
    "\n",
    "# Create a sorted list of `new_name` for efficient prefix search\n",
    "sorted_new_names = sorted(item.new_name for item in column_customizations)\n",
    "col_customization_dict = {item.new_name: item for item in column_customizations}\n",
    "\n",
    "# Function to find matching prefixes using bisect\n",
    "def find_matching_keys(prefix):\n",
    "    i = bisect_left(sorted_new_names, prefix)\n",
    "    matches = []\n",
    "    while i < len(sorted_new_names) and sorted_new_names[i].startswith(prefix):\n",
    "        matches.append(sorted_new_names[i])\n",
    "        i += 1\n",
    "    return matches\n",
    "\n",
    "# Apply updates\n",
    "for key, value in truncated_keys.items():\n",
    "    print(key)\n",
    "    matches = find_matching_keys(key)\n",
    "    print(matches)\n",
    "    for match in matches:\n",
    "        col_customization_dict[match].definitions = value  # Update definitions\n",
    "        col_customization_dict[match].is_category = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set BBL to not be a category\n",
    "col_customization_dict['borough_tax_block_and_lot'].is_category = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Appendix D:\n",
    "### Extract the last table, which isn't actually a table, just text arranged in a table-like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 20\n",
    "body_x_thresh = 10\n",
    "body_y_thresh = 10\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "ncol = 3\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_table = []\n",
    "for idx,row in enumerate(merged_rows[1:]):\n",
    "    new_row = []\n",
    "    for idx2,cell in enumerate(row[0]):\n",
    "        new_row.append(merge_text_in_cell(cell))\n",
    "    last_table.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict['land_use_category'].definitions = last_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explanations of zoning codes.\n",
    "* I could only find this information in pdf form.\n",
    "* I discovered how hard PDFs can be to parse.\n",
    "* I had to do a lot of customization for just this specific pdf. I could have just manually cut and pasted the data from the pdf in the amount of time it took me to do that.\n",
    "* I still think it was good to do for reproducibility reasons, but in the future I will try to avoid working with datasets that have important information only in PDF format.\n",
    "* The following functions extract the tables from the pdf, detecting footnotes, and then subsitute the foonote number for the footnote text within the dataframe (so that it will end up as part of the relevant record in the databasee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/assets/bronxcb8/pdf/zoning_table_all.pdf\"\n",
    "filename = \"zoning_table_all.pdf\"  # Path to save the pdf containing the info we need\n",
    "\n",
    "downloader(\n",
    "            url=url,\n",
    "            download_path=f\"{PROJECT_DATA}/dictionaries/\",\n",
    "            outfile_name=filename,\n",
    "            bigfile=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the above functions to extract the data from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes = parse_zoning_details(f\"{PROJECT_DATA}/dictionaries/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename in tables_and_footnotes.keys():\n",
    "    print(tablename)\n",
    "    df = tables_and_footnotes[tablename]['df']\n",
    "    df.name = df.index.name\n",
    "    # with engine.connect() as conn:\n",
    "    for series_name, series in df.items():\n",
    "        tdf = pd.DataFrame(series)\n",
    "        tdf.reset_index(inplace=True)\n",
    "        jstring = pd.DataFrame(tdf).to_json()\n",
    "        col_customization_dict['zoning_district_1'].definitions.append([series_name, jstring])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PDF parsed above still has some definitions that are in text outside the tables. From `zoning_table_all.pdf`:\n",
    "\n",
    ">C1-1 through C1-5 and C2-1 through C2-5 are commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\n",
    "\n",
    "* I need to manually create the object to hold this information and put it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_zones = {}\n",
    "info = \"Commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\"\n",
    "for i in range(1,6):\n",
    "    more_zones[f'C1-{i}'] = info\n",
    "    more_zones[f'C2-{i}'] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in more_zones.keys():\n",
    "    col_customization_dict['commercial_overlay_1'].definitions.append([key, more_zones[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a few more code meanings \n",
    "* From [NYC Department of Tax and Finance Data Dictionary](https://www.nyc.gov/assets/finance/downloads/tar/tarfieldcodes.pdf):\n",
    "    * LandUse\n",
    "    * OwnerType\n",
    "    * Easment code\n",
    "* Additional information about commercial zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/commercial_zoning_data_tables.pdf).\n",
    "* Additional information about residential zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/residence_zoning_data_tables.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the meanings of the building classification codes from the City of New York website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request #, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "webpage = \"https://www.nyc.gov/assets/finance/jump/hlpbldgcode.html\"\n",
    "\n",
    "def get_table_rows(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup('tr')\n",
    "\n",
    "\n",
    "trs = get_table_rows(webpage)\n",
    "\n",
    "class_codes = []\n",
    "d = None\n",
    "for tr in trs:    \n",
    "    # Check if 'a' with 'name' exists\n",
    "    a = tr.find('a', attrs={'name': True})\n",
    "    if a:\n",
    "        if d:\n",
    "            class_codes.append(d)\n",
    "        supercategory = tr.find_all('th')[1].text.capitalize()\n",
    "        d = {\"supercategory\": supercategory}\n",
    "    \n",
    "    # Check if 'td' exists and update 'd'\n",
    "    cells = tr.find_all('td')\n",
    "    if cells:\n",
    "        d = {}\n",
    "        code, name = cells[:2]\n",
    "        d['code'] = code.text.strip()\n",
    "        d['name'] = name.text.capitalize().strip()\n",
    "        class_codes.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in class_codes:\n",
    "    col_customization_dict['building_class'].definitions.append([row['code'], row['name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually add columns that I will later add to the dataset, such as building number and address derived from the \"address\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict['building_num'] = ColCustomization(short_name='building_num', new_name='building_num', is_category=False, dtype='Integer')\n",
    "col_customization_dict['street'] = ColCustomization(short_name='street', new_name='street', is_category=True, dtype='String')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict['mapPLUTO'].col_customizations = col_customization_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,info in dataset_info_dict.items():\n",
    "    # print(info.col_customizations)\n",
    "    if info.col_types.items():\n",
    "        if not info.col_customizations:\n",
    "            info.col_customizations = {short_name : ColCustomization(short_name=short_name, dtype=dtype) for short_name,dtype in info.col_types.items()}\n",
    "        for key,val in info.cardinality_ratios.items():\n",
    "            if val > 20 and info.col_customizations is not None and info.col_types[key] == String:\n",
    "                info.col_customizations[key].is_category = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"wb\") as f:\n",
    "    dill.dump(\n",
    "        {\n",
    "            \"col_customization_dict\": col_customization_dict,\n",
    "            \"dataset_info_dict\": dataset_info_dict,\n",
    "            \"PROJECT_PATH\": PROJECT_PATH,\n",
    "            \"PROJECT_DATA\": PROJECT_DATA,\n",
    "            \"SQLITE_PATH\": SQLITE_PATH,\n",
    "            \"DATADIR\": DATADIR,\n",
    "            \"PROJECT_NAME\": PROJECT_NAME,\n",
    "            \"PROJECT_DATA\": PROJECT_DATA,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
