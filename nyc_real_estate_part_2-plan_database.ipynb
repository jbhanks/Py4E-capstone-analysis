{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the database based on the data\n",
    "* #### This notebook parses metadata associated with some of the datasets, most especially the PLUTO dataset, which contains columns that are also in many other datasets I looked at on NYCOpenData.\n",
    "* #### In some cases I had to search around to find more complete definitions than were included in the data dictionary associated with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import dill\n",
    "time.sleep(5)\n",
    "from bisect import bisect_left\n",
    "from itertools import tee\n",
    "from src.models import ColCustomization\n",
    "import src.helpers\n",
    "import src.pdfutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/select.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### The MapPLUTO dictionary contains most of the information we need to interpret various codes and categories meaningfully.\n",
    "* #### Unfortunately, it is in PDF format (as are many of the data dictionaries on NYCOpenData), which made extracting all the relevant data a real pain, and I don't expect most of these functions will be fully reusable for other PDFs I may encounter in the future. My hope is that it will still give me a head start when I need to make custom functions for future PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"{PROJECT_DATA}/dictionaries/mapPLUTO_data_dictionary.pdf\"\n",
    "filename = '/home/james/Massive/PROJECTDATA/nyc_real_estate_data/dictionaries/mapPLUTO_data_dictionary.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at the PLUTO data dictionary, it seems that most category variables are labeled as \"alpahnumeric\" even if they only contain numbers, such as zip codes.\n",
    "* There are some exceptions, police precincts and districts are numeric and listed as such. However as there a limited number of repeating variables, I wil treat them as categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section = src.pdfutils.map_pdf(filename, same_line_tolerance=0.3, start_page=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_by_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (re.compile(r'[ ,â€“]+',  flags=re.IGNORECASE), '_'),\n",
    "    (re.compile(r'#',  flags=re.IGNORECASE), 'num'),\n",
    "    (re.compile(r'/',  flags=re.IGNORECASE), '_or_'),\n",
    "    (re.compile(r'&',  flags=re.IGNORECASE), 'and'),\n",
    "    (re.compile(r'!(altered)_[0-9]$',  flags=re.IGNORECASE), ''),\n",
    "    (re.compile(r\"\\bboro(?!ugh)\",  flags=re.IGNORECASE), 'borough')\n",
    "]\n",
    "\n",
    "\n",
    "category_markers = ['code', 'category', 'class', 'district', 'precinct', 'company', 'name', 'health_area', 'type', 'borough', 'name', 'health_area', 'health_center_district', 'overlay']\n",
    "\n",
    "column_customizations=[]\n",
    "\n",
    "for section in pdf_by_section:\n",
    "    in_table = False\n",
    "    in_description = False\n",
    "    header_added = False\n",
    "    table = None\n",
    "    col_mods = None  # Initialize col_mods here\n",
    "    for value in section:\n",
    "        line = ' '.join([word['text'] for word in value])\n",
    "        print(line)\n",
    "        if (line.startswith(\"Field Name: CENSUS\") or line.startswith(\"Field Name: HEALTH\")) and col_mods is not None: # Handle special cases where the dividing line (rectangle object) is not present in between column descriptions\n",
    "            if col_mods.short_name is not None:\n",
    "                print(\"Appending col_mods\", col_mods)\n",
    "                column_customizations.append(col_mods)\n",
    "        if line.startswith('Field Name:') and len(value) > 2: # Exclude the explanation of \"Field Name\" itself on page 3\n",
    "            col_mods = ColCustomization(short_name=value[-1]['text'][1:-1]) # Get the field name minus the enclosing parentheses\n",
    "            full_name = ' '.join(word['text'] for word in value[2:-1])\n",
    "            print('full_name', full_name)\n",
    "            new_name = src.pdfutils.clean_name(full_name.lower(), patterns=patterns)\n",
    "            print('new_name', new_name)\n",
    "            col_mods.is_category = any([word in new_name for word in category_markers])\n",
    "            col_mods.new_name = new_name\n",
    "            if any([w in col_mods.new_name for w in ['year', 'number', 'precinct']]):\n",
    "                col_mods.dtype = \"Integer\"\n",
    "            if 'date' in col_mods.new_name:\n",
    "                col_mods.dtype = \"Date\"\n",
    "            print('col_mods', col_mods)\n",
    "        elif line.startswith('Format:') and not col_mods.dtype:\n",
    "            if \"Alphanumeric\" in line:\n",
    "                col_mods.dtype = \"String\"\n",
    "            if \"Numeric\" in line and not col_mods.dtype:\n",
    "                col_mods.dtype = \"Float\"\n",
    "        elif line.startswith('Description:'):\n",
    "            in_description = True\n",
    "        if in_description is True:\n",
    "            # print(\"LINE is\", line)\n",
    "            # if (line.startswith('Value') or line.startswith('VALUE') or line.startswith('BOROUGH JIA NAME')) and len(value) <= 3 and header_added is True: # Check if the line is a redundant table header, for when tables are split across pages\n",
    "            #     continue\n",
    "            if (line.startswith('Value') or line.startswith('VALUE')) and len(value) <= 3 and header_added is False: # Maximum number of words in a column heading\n",
    "                print(\"Detected table\")\n",
    "                col_starts = src.pdfutils.get_word_starts_x(value)\n",
    "                in_table = True\n",
    "                table = [(line, value)]\n",
    "                header_added = True # This is for dealing with tables that go across pages, and have the header again on the second page.\n",
    "            elif in_table is True and (abs(col_starts[0] - src.pdfutils.get_word_starts_x(value)[0]) < .5 or abs(col_starts[1] - src.pdfutils.get_word_starts_x(value)[0]) < .5):\n",
    "                table.append((line, value))\n",
    "                print(\"Appended line\", line)\n",
    "            elif in_table is True:\n",
    "                print(\"Table is\", table)\n",
    "                table = src.pdfutils.parse_table(table)\n",
    "                print(\"Now the table is\", table)\n",
    "                print('table[0][0] is', table[0][0])\n",
    "                if table[0][0].isdigit():\n",
    "                    print(\"Digits detected!\")\n",
    "                    col_mods.is_fk = True\n",
    "                col_mods.definitions = table\n",
    "                in_table = False\n",
    "                header_added = False\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    if col_mods is not None:\n",
    "        if not col_mods.definitions and table:\n",
    "            col_mods.definitions = src.pdfutils.parse_table(table)\n",
    "        if col_mods.definitions:\n",
    "            col_mods.is_category = True\n",
    "        if col_mods.dtype == \"Float\" and col_mods.is_category == True:\n",
    "            col_mods.dtype = \"Integer\"\n",
    "        print(\"Appending col_mods\", col_mods)\n",
    "        column_customizations.append(col_mods)\n",
    "    else:\n",
    "        print(\"col_mods was NONE!, col_mods is: \", col_mods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_zoning(pdf_path):\n",
    "    all_tables = {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            # Extract raw text as lines\n",
    "            lines = page.extract_text().splitlines()\n",
    "            # Extract tables\n",
    "            tables = page.extract_tables()\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Find the position of the table in the raw text\n",
    "                table_start_line = find_table_start(lines, table)\n",
    "                # Extract the line before the table, if available\n",
    "                label_line = (\n",
    "                    lines[table_start_line - 2] if table_start_line > 0 else None\n",
    "                )\n",
    "                table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                if label_line is not None:\n",
    "                    if \"APPENDIX\" in label_line:\n",
    "                        label_line = re.sub(\"APPENDIX.*: \", \"\", label_line)\n",
    "                        label_line = re.sub(\" +\", \"_\", label_line.lower())\n",
    "                        prev_label_line = label_line\n",
    "                    elif \"PLUTO DATA DICTIONARY\" in label_line:\n",
    "                        label_line = None\n",
    "                    elif \"APPENDIX\" not in label_line:\n",
    "                        print(\"what's this?: \", print('label_line is', label_line))\n",
    "                        table = [row for row in table if \"Abbreviation\" not in row]\n",
    "                    if label_line != None:\n",
    "                        all_tables[label_line] = table\n",
    "                    else:\n",
    "                        all_tables[prev_label_line] = all_tables[prev_label_line] + table\n",
    "                else:\n",
    "                    print('table_index is', table_index)\n",
    "                    print('missed:', lines[table_start_line])\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "def find_table_start(lines, table):\n",
    "    \"\"\"\n",
    "    Identify the start of the table in the text by matching table rows\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        # Convert the table's first row into a string and search for it in the text\n",
    "        table_row = \" \".join(str(cell) for cell in table[1] if cell)  # Skip empty cells\n",
    "        if line in table_row:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add tables from appendixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dicts = parse_zoning(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess dictionary keys by truncating last letter (for singular/plural matching)\n",
    "truncated_keys = {key[:-1]: value for key, value in table_dicts.items()}\n",
    "\n",
    "# Create a sorted list of `new_name` for efficient prefix search\n",
    "sorted_new_names = sorted(item.short_name for item in column_customizations)\n",
    "col_customization_dict = {item.short_name: item for item in column_customizations}\n",
    "\n",
    "# Function to find matching prefixes using bisect\n",
    "def find_matching_keys(prefix):\n",
    "    i = bisect_left(sorted_new_names, prefix)\n",
    "    matches = []\n",
    "    while i < len(sorted_new_names) and sorted_new_names[i].startswith(prefix):\n",
    "        matches.append(sorted_new_names[i])\n",
    "        i += 1\n",
    "    return matches\n",
    "\n",
    "# Apply updates\n",
    "for key, value in truncated_keys.items():\n",
    "    matches = find_matching_keys(key)\n",
    "    for match in matches:\n",
    "        col_customization_dict[match].definitions = value  # Update definitions\n",
    "        col_customization_dict[match].is_category = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Appendix D:\n",
    "### Extract the last table, which isn't actually a table, just text arranged in a table-like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_by_top_with_tolerance(elements, tolerance=.1):\n",
    "    groups = []\n",
    "    for elem in sorted(elements, key=lambda x: x[\"top\"]):\n",
    "        matched = False\n",
    "        for group in groups:\n",
    "            if abs(group[0][\"top\"] - elem[\"top\"]) <= tolerance:\n",
    "                group.append(elem)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            groups.append([elem])\n",
    "    return groups\n",
    "\n",
    "\n",
    "def restructure_data(data):\n",
    "    result = []\n",
    "    for group in data:\n",
    "        subgroups = []\n",
    "        subgroup = [group[0]]\n",
    "        for item in group[1:]:\n",
    "            if item[\"x0\"] - subgroup[-1][\"x1\"] <= 10:\n",
    "                subgroup.append(item)\n",
    "            else:\n",
    "                subgroups.append(subgroup)\n",
    "                subgroup = [item]\n",
    "        subgroups.append(subgroup)\n",
    "        result.append(subgroups)\n",
    "    return result\n",
    "\n",
    "def merge_sublists(data, x_misalignment_tolerance=.1):\n",
    "    # Extract the first sublist\n",
    "    first_sublist = copy.deepcopy(data[0])\n",
    "\n",
    "    # Iterate over the remaining sublists\n",
    "    for sublist in data[1:]:\n",
    "        for subsublist in sublist:\n",
    "            # Determine the x-range of the sub-sub-list\n",
    "            start = min(item[\"x0\"] for item in subsublist)\n",
    "            # stop = max(item[\"x1\"] for item in subsublist)\n",
    "\n",
    "            # Find the appropriate sub-sub-list in the first sublist to append to\n",
    "            for target_subsublist in first_sublist:\n",
    "                target_start = min(item[\"x0\"] for item in target_subsublist)\n",
    "                target_stop = max(item[\"x1\"] for item in target_subsublist)\n",
    "\n",
    "                if target_start - x_misalignment_tolerance <= start <= target_stop + x_misalignment_tolerance:\n",
    "                    target_subsublist.extend(subsublist)\n",
    "                    break\n",
    "\n",
    "    return [first_sublist]\n",
    "\n",
    "\n",
    "def fix_row(row, x_misalignment_tolerance=.1, y_misalignment_tolerance=.1):\n",
    "    first_sort = sorted(row, key=lambda x: (x[\"top\"], x[\"x0\"]))  # `row` instead of `lst`\n",
    "    grouped_by_top = group_by_top_with_tolerance(first_sort, tolerance=y_misalignment_tolerance)\n",
    "    restructured_data = restructure_data(grouped_by_top)\n",
    "    merged_data = merge_sublists(restructured_data, x_misalignment_tolerance=x_misalignment_tolerance)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def trim_lines_outside_table(lines, table_top_boundary_text=None, table_bottom_boundary_text=None):\n",
    "    \"\"\"Returns the index of the first line to contain the specified table_top_boundary_text\n",
    "\n",
    "    Args:\n",
    "        lines (_type_): _description_\n",
    "        table_top_boundary_text (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for idx, line in enumerate(lines):\n",
    "        if table_top_boundary_text is not None and table_top_boundary_text in \" \".join(\n",
    "            [word[\"text\"] for word in line]\n",
    "        ):\n",
    "            top_trim_line = idx\n",
    "            continue\n",
    "        elif (\n",
    "            table_bottom_boundary_text is not None\n",
    "            and table_bottom_boundary_text in \" \".join([word[\"text\"] for word in line])\n",
    "        ):\n",
    "            bottom_trim_line = idx\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    trimmed_lines = [\n",
    "        line\n",
    "        for idx, line in enumerate(lines)\n",
    "        if idx > top_trim_line and idx < bottom_trim_line\n",
    "    ]\n",
    "    return trimmed_lines\n",
    "\n",
    "\n",
    "def group_words_by_row(words, y_thresh=5):\n",
    "    \"\"\"Groups words into rows based on vertical proximity, allowing small deviations in top values.\"\"\"\n",
    "    words = sorted(words, key=lambda w: w['top'])  # Sort words top-to-bottom\n",
    "    rows = []\n",
    "    for word in words:\n",
    "        added = False\n",
    "        for row in rows:\n",
    "            # Compare with first word in the row for stability\n",
    "            if abs(word['top'] - max([w['top'] for w in row])) <= y_thresh:\n",
    "                row.append(word)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            rows.append([word])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def merge_words_in_row(row, x_thresh=10):\n",
    "    \"\"\"\n",
    "    Merges words in a single row, considering the provided x_thresh for horizontal grouping.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of merged text blocks, each with the merged text and bounding box.\n",
    "    \"\"\"\n",
    "    row.sort(key=lambda w: (w['x0'], w['top']))  # Sort words left-to-right\n",
    "    merged_blocks = []\n",
    "    current_block = []\n",
    "    for word in row:\n",
    "        if current_block and (word['x0'] - current_block[-1]['x1']) <= x_thresh:\n",
    "            current_block.append(word)\n",
    "        else:\n",
    "            if current_block:\n",
    "                current_block.sort(key=lambda w: w['top']) # Sort block by top coordinate to get text in each table cell correctly ordered.\n",
    "                merged_blocks.append(current_block)\n",
    "            current_block = [word]\n",
    "\n",
    "    if current_block:\n",
    "        merged_blocks.append(current_block)\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"text\": \" \".join(w[\"text\"] for w in block),\n",
    "            \"x0\": min(w[\"x0\"] for w in block),\n",
    "            \"x1\": max(w[\"x1\"] for w in block),\n",
    "            \"top\": min(w[\"top\"] for w in block),\n",
    "            \"bottom\": max(w[\"bottom\"] for w in block),\n",
    "        }\n",
    "        for block in merged_blocks\n",
    "    ]\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_lines_in_row(lines, y_thresh):\n",
    "    merged_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if not merged_lines:\n",
    "            merged_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        prev_line = merged_lines[-1]\n",
    "        \n",
    "        # Compute merging condition\n",
    "        min_top_current = min(word[\"top\"] for word in line)\n",
    "        max_bottom_prev = max(word[\"bottom\"] for word in prev_line)\n",
    "        \n",
    "        if min_top_current - max_bottom_prev < y_thresh:\n",
    "            # Merge into the previous line\n",
    "            merged_lines[-1].extend(line)\n",
    "        else:\n",
    "            # Start a new line\n",
    "            merged_lines.append(line)\n",
    "\n",
    "    # Now merge words by `x0` within each line\n",
    "    result = []\n",
    "    \n",
    "    for line in merged_lines:\n",
    "        grouped = defaultdict(list)\n",
    "        \n",
    "        for (_, word) in enumerate(line):\n",
    "            grouped[word[\"x0\"]].append(word)\n",
    "        \n",
    "        merged_words = []\n",
    "        \n",
    "        for x0 in sorted(grouped.keys()):  # Preserve order\n",
    "            words = grouped[x0]\n",
    "            merged_text = \" \".join(w[\"text\"] for w in words)\n",
    "            x1 = max(w[\"x1\"] for w in words)\n",
    "            top = min(w[\"top\"] for w in words)\n",
    "            bottom = max(w[\"bottom\"] for w in words)\n",
    "            \n",
    "            merged_words.append({\"text\": merged_text, \"x0\": x0, \"x1\": x1, \"top\": top, \"bottom\": bottom})\n",
    "        \n",
    "        result.append(merged_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def detect_header_by_uppercase(rows):\n",
    "    \"\"\"Identifies the header row by checking if all words are uppercase.\"\"\"\n",
    "    header_row = []\n",
    "    body_rows = []\n",
    "\n",
    "    for row in rows:\n",
    "        if all(word[\"text\"].isupper() for word in row):  # All words must be uppercase\n",
    "            header_row = header_row + row\n",
    "        else:\n",
    "            body_rows.append(row)\n",
    "\n",
    "    return header_row, body_rows\n",
    "\n",
    "\n",
    "def merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh):\n",
    "    \"\"\"\n",
    "    Groups words into rows and merges horizontally close words.\n",
    "    \"\"\"\n",
    "    rows = group_words_by_row(words, header_y_thresh)\n",
    "    print(\"ROWS ARE\", rows)\n",
    "    trimmed_rows = trim_lines_outside_table(rows, table_top_boundary_text=\"APPENDIX D: LAND USE CATEGORIES\", table_bottom_boundary_text=\"NOTES:\")\n",
    "    header_row, body_rows = detect_header_by_uppercase(trimmed_rows)\n",
    "    merged_header = merge_words_in_row(header_row, header_x_thresh)\n",
    "    # merged_rows = [merge_words_in_row(row, body_x_thresh) for row in body_rows]\n",
    "    merged_rows = [fix_row(row) for row in body_rows]\n",
    "    # merged_rows = merge_lines_in_row(merged_rows, body_y_thresh)\n",
    "    all_rows = [merged_header] + merged_rows\n",
    "    return all_rows\n",
    "    # return merged_rows\n",
    "\n",
    "def assign_columns_to_blocks(merged_rows, column_gap_thresh=20, ncol=3):\n",
    "    \"\"\"\n",
    "    Assigns a column index to each merged text block by detecting significant gaps in x0 values.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_rows: List of lists of merged word blocks.\n",
    "    - column_gap_thresh: Minimum gap to consider as a column boundary.\n",
    "    \n",
    "    Returns:\n",
    "    - A list where each element is a tuple (column_index, word_block_dict).\n",
    "    \"\"\"\n",
    "    all_x_values = sorted(set(block[\"x0\"] for row in merged_rows for block in row))\n",
    "\n",
    "    # Detect gaps to determine column boundaries\n",
    "    column_boundaries = [all_x_values[0]]\n",
    "    for i in range(1, len(all_x_values)):\n",
    "        if all_x_values[i] - all_x_values[i - 1] > column_gap_thresh:\n",
    "            column_boundaries.append(all_x_values[i])\n",
    "\n",
    "    # def get_column_index(x0):\n",
    "    #     \"\"\"Finds the appropriate column index for a given x0 value.\"\"\"\n",
    "    #     for i, boundary in enumerate(column_boundaries):\n",
    "    #         if x0 < boundary:\n",
    "    #             return max(i - 1, 0)\n",
    "    #     return len(column_boundaries) - 1\n",
    "\n",
    "    structured_output = []\n",
    "    for idx,row in enumerate(merged_rows):\n",
    "        row_output = [cell for cell in row]\n",
    "        structured_output.append(row_output)\n",
    "\n",
    "    return structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "header_x_thresh = 10\n",
    "header_y_thresh = 20\n",
    "body_x_thresh = 10\n",
    "body_y_thresh = 10\n",
    "column_gap_thresh = 20  # Adjust based on observed spacing\n",
    "ncol = 3\n",
    "\n",
    "with pdfplumber.open(filename) as pdf:\n",
    "    words = pdf.pages[-1].extract_words()  # Extract words from page 0\n",
    "    merged_rows = merge_words_into_rows(words, header_x_thresh, header_y_thresh, body_x_thresh, body_y_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_objects_in_cell(list_of_objects):\n",
    "    return {\n",
    "        \"text\": \" \".join(w[\"text\"] for w in list_of_objects),\n",
    "        \"x0\": min(w[\"x0\"] for w in list_of_objects),\n",
    "        \"x1\": max(w[\"x1\"] for w in list_of_objects),\n",
    "        \"top\": min(w[\"top\"] for w in list_of_objects),\n",
    "        \"bottom\": max(w[\"bottom\"] for w in list_of_objects),\n",
    "    }\n",
    "\n",
    "def merge_text_in_cell(list_of_objects):\n",
    "    return \" \".join(w[\"text\"] for w in list_of_objects)\n",
    "\n",
    "last_table = []\n",
    "for idx,row in enumerate(merged_rows[1:]):\n",
    "    new_row = []\n",
    "    for idx2,cell in enumerate(row[0]):\n",
    "        new_row.append(merge_text_in_cell(cell))\n",
    "    last_table.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in col_customization_dict.keys() if 'Land' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict['LandUse'].definitions = last_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explanations of zoning codes.\n",
    "* I could only find this information in pdf form.\n",
    "* I discovered how hard PDFs can be to parse.\n",
    "* I had to do a lot of customization for just this specific pdf. I could have just manually cut and pasted the data from the pdf in the amount of time it took me to do that.\n",
    "* I still think it was good to do for reproducibility reasons, but in the future I will try to avoid working with datasets that have important information only in PDF format.\n",
    "* The following functions extract the tables from the pdf, detecting footnotes, and then subsitute the foonote number for the footnote text within the dataframe (so that it will end up as part of the relevant record in the databasee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/assets/bronxcb8/pdf/zoning_table_all.pdf\"\n",
    "filename = \"zoning_table_all.pdf\"  # Path to save the pdf containing the info we need\n",
    "\n",
    "src.helpers.downloader(\n",
    "            url=url,\n",
    "            download_path=f\"{PROJECT_DATA}/dictionaries/\",\n",
    "            outfile_name=filename,\n",
    "            bigfile=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the above functions to extract the data from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes = src.pdfutils.parse_zoning_details(f\"{PROJECT_DATA}/dictionaries/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a MetaData instance\n",
    "# metadata = MetaData()\n",
    "# Base.metadata.reflect(bind=engine)\n",
    "\n",
    "# zoning_district_lookup = create_lookup_table(engine, \"zoning_district\", \"code\")\n",
    "# # Reflect the table\n",
    "# zoning_district_lookup = Table(\"zoning_district\", metadata, autoload_with=engine)\n",
    "\n",
    "for tablename in tables_and_footnotes.keys():\n",
    "    print(tablename)\n",
    "    df = tables_and_footnotes[tablename]['df']\n",
    "    df.name = df.index.name\n",
    "    # with engine.connect() as conn:\n",
    "    for series_name, series in df.items():\n",
    "        tdf = pd.DataFrame(series)\n",
    "        tdf.reset_index(inplace=True)\n",
    "        jstring = pd.DataFrame(tdf).to_json()\n",
    "        col_customization_dict['ZoneDist1'].definitions.append([series_name, jstring])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col_customization_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PDF parsed above still has some definitions that are in text outside the tables. From `zoning_table_all.pdf`:\n",
    "\n",
    ">C1-1 through C1-5 and C2-1 through C2-5 are commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\n",
    "\n",
    "* I need to manually create the object to hold this information and put it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_zones = {}\n",
    "info = \"Commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\"\n",
    "for i in range(1,6):\n",
    "    more_zones[f'C1-{i}'] = info\n",
    "    more_zones[f'C2-{i}'] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in more_zones.keys():\n",
    "    print(more_zones[key])\n",
    "    col_customization_dict['ZoneDist1'].definitions.append([key, more_zones[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a few more code meanings \n",
    "* From [NYC Department of Tax and Finance Data Dictionary](https://www.nyc.gov/assets/finance/downloads/tar/tarfieldcodes.pdf):\n",
    "    * LandUse\n",
    "    * OwnerType\n",
    "    * Easment code\n",
    "* Additional information about commercial zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/commercial_zoning_data_tables.pdf).\n",
    "* Additional information about residential zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/residence_zoning_data_tables.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the meanings of the building classification codes from the City of New York website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request #, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "webpage = \"https://www.nyc.gov/assets/finance/jump/hlpbldgcode.html\"\n",
    "\n",
    "def get_table_rows(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup('tr')\n",
    "\n",
    "\n",
    "trs = get_table_rows(webpage)\n",
    "\n",
    "class_codes = []\n",
    "d = None\n",
    "for tr in trs:    \n",
    "    # Check if 'a' with 'name' exists\n",
    "    a = tr.find('a', attrs={'name': True})\n",
    "    if a:\n",
    "        if d:\n",
    "            class_codes.append(d)\n",
    "        supercategory = tr.find_all('th')[1].text.capitalize()\n",
    "        d = {\"supercategory\": supercategory}\n",
    "    \n",
    "    # Check if 'td' exists and update 'd'\n",
    "    cells = tr.find_all('td')\n",
    "    if cells:\n",
    "        d = {}\n",
    "        code, name = cells[:2]\n",
    "        d['code'] = code.text.strip()\n",
    "        d['name'] = name.text.capitalize().strip()\n",
    "        class_codes.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in class_codes:\n",
    "    col_customization_dict['BldgClass'].definitions.append([row['code'], row['name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"wb\") as f:\n",
    "    dill.dump({'col_customization_dict': col_customization_dict, 'table_dicts': table_dicts}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
