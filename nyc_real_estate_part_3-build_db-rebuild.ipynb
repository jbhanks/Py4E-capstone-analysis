{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import time\n",
    "import dill\n",
    "time.sleep(3)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary, ForeignKey\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112db1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'{SQLITE_PATH}?check_same_thread=False', echo=False)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@event.listens_for(engine, \"connect\")\n",
    "def load_spatialite(dbapi_conn, connection_record):\n",
    "    print(\"Loading SpatiaLite extension\")\n",
    "    dbapi_conn.enable_load_extension(True)\n",
    "    dbapi_conn.load_extension(\"mod_spatialite\")\n",
    "    dbapi_conn.enable_load_extension(False)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connection established\")\n",
    "    result = conn.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n",
    "# Enable WAL mode\n",
    "with SessionLocal() as session:\n",
    "    session.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
    "    session.execute(text(\"PRAGMA synchronous = NORMAL\"))\n",
    "    session.execute(text(\"PRAGMA temp_store = MEMORY\"))\n",
    "    session.execute(text(\"PRAGMA wal_autocheckpoint=1000;\"))  # Reduce I/O load\n",
    "    session.execute(text(\"PRAGMA mmap_size = 30000000000;\"))\n",
    "\n",
    "\n",
    "# Initialize spatial metadata if not already present\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))\n",
    "with SessionLocal() as session:\n",
    "    result = session.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for name,repetitions in multicolumns.items():\n",
    "        print(name)\n",
    "        print(f\"Setting {name} columns\")\n",
    "        for k in dataset.col_customizations.keys():\n",
    "            if dataset.col_customizations[k].new_name is None:\n",
    "                dataset.col_customizations[k].new_name = dataset.col_customizations[k].short_name\n",
    "        cols = {k:v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.startswith(name)}\n",
    "        print(f'cols for {name} are: {cols}')\n",
    "        main_col = [v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.endswith(\"_1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e372f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b878de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MapPLUTO data from geo database file (.gdb)\n",
    "pluto_version = \"25v2_1\"\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO{pluto_version}.gdb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geodata = {}\n",
    "# List layers in the GDB file\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers in the GDB file:\")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    # gdf['borough'] = gdf['Borough'].replace(replacement_dict)\n",
    "    try:\n",
    "        gdf['wkb'] = gdf['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geodata[f'MapPLUTO_{pluto_version}_clipped']\n",
    "is_whole_number = {(gdf[col].notna() & (gdf[col] % 1 == 0)).all() for col in gdf.columns if gdf[col].dtype == 'float'}\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ecb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over columns and change dtype to int where applicable\n",
    "for col in gdf.columns:\n",
    "    if  gdf[col].dtype == float and is_whole_number_series(gdf[col]):\n",
    "        print(f'Column {col} is {is_whole_number_series(gdf[col])}')\n",
    "        print(f'Converting {col} to integer')\n",
    "        gdf[col] = gdf[col].astype('Int64')  # 'Int64' for nullable integer type in Pandas\n",
    "    else:\n",
    "        print(f\"Skipping {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())  # Ensure \"basement_type_or_grade_lookup\" is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f002fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns=rename_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fe619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few of the column names did not exactly match up due to slightly different field names than specified in the data dictionary, so these need to be renamed manually:\n",
    "\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, MetaData, Column, Integer, String, ForeignKey, LargeBinary, Float, Date\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "# Reflect the existing database tables once\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Function to map custom dtype to SQLAlchemy types\n",
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        return Integer\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Float':\n",
    "        return Float\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "# Function to dynamically create the table class\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(String),  \n",
    "        'wkb': Column(LargeBinary),  # Use LargeBinary for WKB\n",
    "        'Shape_Leng' : Column(Float), # Add columns not listed in the data dictionary\n",
    "        'Shape_Area' : Column(Float),\n",
    "    }\n",
    "    attrs['__table_args__'] = {'extend_existing': True}\n",
    "    \n",
    "    for k, v in col_customization_dict.items():\n",
    "        if any([name for name in multicolumns if name in k]):\n",
    "            k = re.sub('_[0-9]$', '', k)\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        attrs[v.new_name] = Column(col_type)\n",
    "    \n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "# Create the MapPLUTO clipped table class\n",
    "MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "\n",
    "# Reflect the metadata again to ensure it includes the new table class\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Create all tables in the database\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2651f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from shapely import wkb\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "# gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "def format_float(value):\n",
    "    return str(value).rstrip('0').rstrip('.') if '.' in str(value) else str(value)\n",
    "\n",
    "batch_size = 100000\n",
    "with SessionLocal() as session:\n",
    "    for start in range(0, len(gdf), batch_size):\n",
    "        batch = gdf.iloc[start:start + batch_size]\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                if row['apportionment_date']:\n",
    "                    row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "                for col in gdf.columns:\n",
    "                    val = row[col]\n",
    "                    if isinstance(val, pd.Series):\n",
    "                        try:\n",
    "                            first_value = row[col].iloc[0]\n",
    "                            row[col] = first_value\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing Series in column {col} at row {idx}: {e}\")\n",
    "                    # Replace NA values with None so that SQLAlchemy inserts them as NULL:\n",
    "                    if pd.isna(val):\n",
    "                        row[col] = None\n",
    "                # Prepare the geometry and entry object\n",
    "                geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "                pluto_entry = MapPLUTO_Clipped(\n",
    "                    geometry=geometry_wkb,\n",
    "                    **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "                )\n",
    "                session.add(pluto_entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row index {idx}\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        print(f\"Column: {col}, Value: {row[col]}, Type: {type(row[col])}\")\n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"Error printing column {col}: {sub_e}\")\n",
    "                raise e  # re-raise after logging for further debugging\n",
    "        session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ca1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "query = f\"SELECT zip_code, geometry FROM MapPLUTO_{pluto_version}_clipped\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(f\"{PROJECT_DATA}/figures/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_mappings = {\"meta_data\" : String, \"calendar_date\" : Date, \"number\" : Float, \"text\" : String, \"point\" : String}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import orjson\n",
    "import time\n",
    "from shapely import from_wkt  # Vectorized conversion function in Shapely 2.0\n",
    "from geoalchemy2.shape import from_shape\n",
    "from sqlalchemy.engine import Engine\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "def split_address(row_data, address_col):\n",
    "    if address_col in row_data.keys():\n",
    "        if row_data[address_col] is None:\n",
    "            row_data[\"building_num\"], row_data[\"street\"] = None, None\n",
    "            row_data.pop(address_col)\n",
    "            return row_data\n",
    "        else:\n",
    "            if row_data[address_col][0].isdigit():\n",
    "                try:\n",
    "                    addr = row_data[address_col].split(\" \", 1)\n",
    "                    if len(addr) == 1:\n",
    "                        addr = [None] + addr\n",
    "                    row_data[\"building_num\"], row_data[\"street\"] = addr\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(row_data[address_col])\n",
    "            else:\n",
    "                row_data[\"building_num\"], row_data[\"street_name\"] = (\n",
    "                    None,\n",
    "                    row_data[address_col],\n",
    "                )\n",
    "        row_data.pop(address_col)\n",
    "    return row_data\n",
    "\n",
    "def convert_wkt(rows_to_insert):\n",
    "    # Batch convert geometries.\n",
    "    raw_wkts = [r.get('_raw_geocoded_column') for r in rows_to_insert]\n",
    "    try:\n",
    "        shapely_geoms = from_wkt(raw_wkts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting batch geometry at row {idx}: {e}\")\n",
    "        shapely_geoms = [None] * len(rows_to_insert)\n",
    "    geoms = [\n",
    "        from_shape(geom, srid=4326) if geom is not None else None \n",
    "        for geom in shapely_geoms\n",
    "    ]\n",
    "    for r, geom in zip(rows_to_insert, geoms):\n",
    "        r['geocoded_column'] = geom\n",
    "        r.pop('_raw_geocoded_column', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_info_dict['tax_liens']\n",
    "pprint.pprint(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e027e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataset(engine, dataset, jsonfile, columns, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process the JSON Lines file in batches using pandas vectorized operations.\n",
    "    Uses engine.begin() to leverage executemany().\n",
    "    \"\"\"\n",
    "    col_names = list(columns.keys())\n",
    "    expected_width = len(col_names)\n",
    "    rows_buffer = []\n",
    "    insert_stmt = None\n",
    "    DynamicTable = None\n",
    "\n",
    "    def custom_loads(s):\n",
    "        return orjson.loads(s.encode(\"utf-8\"))\n",
    "\n",
    "    def sanitize_rows(rows):\n",
    "        cleaned = []\n",
    "        for i, row in enumerate(rows):\n",
    "            new_row = {}\n",
    "            for k, v in row.items():\n",
    "                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "                    print(f\"Offending value in row {i}, column '{k}': {repr(v)}\")\n",
    "                    v = None\n",
    "                new_row[k] = v\n",
    "            cleaned.append(new_row)\n",
    "        return cleaned\n",
    "\n",
    "    def validate_no_nans(rows):\n",
    "        for i, row in enumerate(rows):\n",
    "            for k, v in row.items():\n",
    "                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "                    print(f\"❌ Still found NaN/Inf at row {i}, column '{k}' — value: {repr(v)}\")\n",
    "                    raise ValueError(\"NaN or Inf sneaked through\")\n",
    "\n",
    "    def process_batch(df, nullable_integer_columns):\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "\n",
    "        # Sanitize strings\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].isna().any():\n",
    "                print(f\"⚠️ Warning: {col} contains NaN\")\n",
    "\n",
    "        # Fix nullable INTEGER columns: convert to nullable Int64\n",
    "        for col in nullable_integer_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # Dates\n",
    "        datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                df[col] = df[col].apply(lambda x: x.date() if pd.notnull(x) else None)\n",
    "        if 'geocoded_column' in df.columns:\n",
    "            df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "            df['geocoded_column'] = None\n",
    "\n",
    "        df = split_address(df, 'address')\n",
    "\n",
    "        rows = df.to_dict(orient=\"records\")\n",
    "        rows = sanitize_rows(rows)\n",
    "        convert_wkt(rows)\n",
    "        validate_no_nans(rows)\n",
    "        return rows\n",
    "\n",
    "    with jsonlines.open(jsonfile, mode='r', loads=custom_loads) as reader:\n",
    "        batch_start = time.perf_counter()\n",
    "        for idx, row in enumerate(reader):\n",
    "            if isinstance(row, list) and len(row) > expected_width:\n",
    "                row = row[:expected_width]\n",
    "            rows_buffer.append(row)\n",
    "\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "                t0 = time.perf_counter()\n",
    "                if insert_stmt is None:\n",
    "                    print(f\"📐 Creating table for dataset {dataset.short_name}\")\n",
    "                    DynamicTable = create_table_for_dataset(\n",
    "                        columns=dataset.col_types,\n",
    "                        prefix=dataset.short_name,\n",
    "                        engine=engine\n",
    "                    )\n",
    "                    insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "                    print(\"📋 Table schema:\")\n",
    "                    for col in DynamicTable.__table__.columns:\n",
    "                        print(f\"  {col.name}: {col.type}, nullable={col.nullable}\")\n",
    "                    nullable_integer_columns = [\n",
    "                        col for col, typ in columns.items()\n",
    "                        if typ is Integer and DynamicTable.__table__.columns[col].nullable\n",
    "                    ]\n",
    "                batch_rows = process_batch(df, nullable_integer_columns)\n",
    "                t2 = time.perf_counter()\n",
    "                with engine.begin() as conn:\n",
    "                    conn.execute(insert_stmt, batch_rows)\n",
    "                t3 = time.perf_counter()\n",
    "                print(f\"📤 Inserted batch in {t3 - t2:.2f}s, total time {t3 - batch_start:.2f}s\")\n",
    "\n",
    "                rows_buffer.clear()\n",
    "                batch_start = time.perf_counter()\n",
    "\n",
    "        # Final flush\n",
    "        if rows_buffer:\n",
    "            DynamicTable = create_table_for_dataset(\n",
    "                columns=dataset.col_types,\n",
    "                prefix=dataset.short_name,\n",
    "                engine=engine\n",
    "            )\n",
    "            insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "            print(f'insert_stmt: {insert_stmt}')\n",
    "            nullable_integer_columns = [\n",
    "                col for col, typ in columns.items()\n",
    "                if typ is Integer and DynamicTable.__table__.columns[col].nullable]\n",
    "            print(f'nullable_integer_columns: {nullable_integer_columns}')\n",
    "            df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "            print(df.head())\n",
    "            batch_rows = process_batch(df, nullable_integer_columns)\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(insert_stmt, batch_rows)\n",
    "            print(\"✅ Final batch inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in dataset_info_dict.items():\n",
    "    if dataset.format == 'json':\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        print(f'The dataset to be processed is {dataset}')\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1560cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cbc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_property",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
