{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and populate the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import standard libraries\n",
    "# import os\n",
    "# import re\n",
    "import json\n",
    "import codecs\n",
    "# import requests\n",
    "import dill\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# # Import third-party libraries\n",
    "# import geopandas as gpd\n",
    "# from geoalchemy2 import Geometry\n",
    "# import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# from sqlalchemy.event import listen\n",
    "# from sqlalchemy.engine import Engine\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# import sqlite3\n",
    "# from sqlite3 import dbapi2 as sqlite\n",
    "\n",
    "# import fiona\n",
    "# from fiona.crs import from_epsg\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/select.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    print(dataset.short_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'{SQLITE_PATH}?check_same_thread=False', echo=False)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Configure the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@event.listens_for(engine, \"connect\")\n",
    "def load_spatialite(dbapi_conn, connection_record):\n",
    "    print(\"Loading SpatiaLite extension\")\n",
    "    dbapi_conn.enable_load_extension(True)\n",
    "    dbapi_conn.load_extension(\"mod_spatialite\")\n",
    "    dbapi_conn.enable_load_extension(False)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connection established\")\n",
    "    result = conn.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n",
    "# Enable WAL mode\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
    "\n",
    "# Initialize spatial metadata if not already present\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually create the borough codes lookup table\n",
    "* These are standardized and available many places, however I could not find a single official source of record to programatically get them from, since there are only five of them, I enter them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borough = {'Manhattan' : 1,\n",
    "# 'Bronx' : 2,\n",
    "# 'Brooklyn' : 3,\n",
    "# 'Queens' : 4,\n",
    "# 'Staten Island' : 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = MetaData()\n",
    "# metadata.reflect(bind=engine)\n",
    "\n",
    "# def create_lookup_table_simple(engine=engine, metadata=metadata, lookup_table_name='new_lookup_table', lookup_column_name='name'):\n",
    "#     lookup_table = Table(\n",
    "#         lookup_table_name,\n",
    "#         metadata,\n",
    "#         Column('id', Integer, primary_key=True, autoincrement=False),\n",
    "#         Column(lookup_column_name, String, unique=True, nullable=False, default=\"NO DATA\"),\n",
    "#         extend_existing = True\n",
    "#     )\n",
    "#     if table_exists(engine, lookup_table_name):\n",
    "#         print(\"Table exists\")\n",
    "#     else:\n",
    "#         lookup_table.create(engine)\n",
    "#     return lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Populate the table with the borough codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borough_lookup_table = create_lookup_table_simple(engine=engine, metadata=metadata, lookup_table_name='boroughs', lookup_column_name='borough')\n",
    "\n",
    "# with engine.connect() as connection:\n",
    "#     for key,value in borough_codes.items():\n",
    "#         stmt = insert(borough_lookup_table).values(id = value, borough = key).on_conflict_do_nothing()\n",
    "#         connection.execute(stmt)\n",
    "#     connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get columns related to borough to identify ones that need to be standardized.\n",
    "\n",
    "# borough_cols = [col_customization_dict[k] for k in col_customization_dict.keys() if \"boro\" in k]\n",
    "# borough_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookups = [col for col in column_customizations if col.definitions ]\n",
    "# col_customization_dict\n",
    "\n",
    "# lookups = {k:v for k,v in col_customization_dict.items() if k[v].definitions}\n",
    "lookups = {k:v for k,v in col_customization_dict.items() if col_customization_dict[k].definitions}\n",
    "\n",
    "completed_tables = [] # This is for tracking the names of tables that have been created, which will be used to avoid creating redundant tables for columns that are same-kind repeats (such as \"district_1\" and \"district_2\"), and thus will use the same lookups.\n",
    "\n",
    "\n",
    "# columns = [\n",
    "#     ('category', String, {'unique': True, 'nullable': False, 'default': \"NO DATA\"}),\n",
    "#     ('info', String, {'unique': False, 'nullable': True, 'default': None})\n",
    "# ]\n",
    "\n",
    "# create_lookup_table(engine, 'my_lookup_table', columns)\n",
    "\n",
    "for name,table in lookups.items():\n",
    "    print('table is', table)\n",
    "    lookup_table_name= re.sub('_[0-9]+$', '', table.new_name.title())\n",
    "    print(\"prefixes are\", completed_tables)\n",
    "    print(\"table.new_name is\", table.new_name)\n",
    "    print([table.new_name.startswith(prefix) for prefix in completed_tables])\n",
    "    print(any([table.new_name.startswith(prefix) for prefix in completed_tables]))\n",
    "    if any([table.new_name.startswith(prefix) for prefix in completed_tables]):\n",
    "    # if table.new_name[0:len(table)*75] in completed_tables:\n",
    "        print(\"Lookup table already created, continuing...\")\n",
    "        continue\n",
    "    with engine.connect() as connection:\n",
    "        lookup_table = create_lookup_table(engine=engine, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "        if lookup_table is None:\n",
    "            print(f\"Table {lookup_table_name} was not properly retrieved or created. Perhaps it exists? Skipping for now..\")\n",
    "            continue\n",
    "        for definition in table.definitions:\n",
    "            print(definition)\n",
    "            if len(definition) == 2:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1]).on_conflict_do_nothing()\n",
    "                except ValueError:\n",
    "                    stmt = insert(lookup_table).values(name_or_code=definition[0], info=definition[1]).on_conflict_do_nothing()\n",
    "            elif len(definition) == 3:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(definition)\n",
    "                    # stmt = insert(lookup_table).values(id=definition[0], name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "            else:\n",
    "                print(definition)\n",
    "                raise ValueError(\"Was only expecting two or three columns\")\n",
    "            connection.execute(stmt)\n",
    "        connection.commit()\n",
    "    name_prefix = table.new_name[0:round(len(table.new_name)*.75)] # Hopefully this is a safe threshold to identify when columns are repeats of the same type\n",
    "    completed_tables.append(name_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k,v in col_customization_dict.items() if col_customization_dict[k].definitions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explanations of zoning codes.\n",
    "* I could only find this information in pdf form.\n",
    "* I discovered how hard PDFs can be to parse.\n",
    "* I had to do a lot of customization for just this specific pdf. I could have just manually cut and pasted the data from the pdf in the amount of time it took me to do that.\n",
    "* I still think it was good to do for reproducibility reasons, but in the future I will try to avoid working with datasets that have important information only in PDF format.\n",
    "* The following functions extract the tables from the pdf, detecting footnotes, and then subsitute the foonote number for the footnote text within the dataframe (so that it will end up as part of the relevant record in the databasee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/assets/bronxcb8/pdf/zoning_table_all.pdf\"\n",
    "filename = \"zoning_table_all.pdf\"  # Path to save the pdf containing the info we need\n",
    "\n",
    "downloader(\n",
    "            url=url,\n",
    "            download_path=f\"{PROJECT_DATA}/dictionaries/\",\n",
    "            outfile_name=filename,\n",
    "            bigfile=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the above functions to extract the data from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_and_footnotes = parse_zoning_details(f\"{PROJECT_DATA}/dictionaries/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a MetaData instance\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "zoning_districts_lookup = create_lookup_table(engine, \"zoning_districts\", \"code\")\n",
    "# # Reflect the table\n",
    "# zoning_districts_lookup = Table(\"zoning_districts\", metadata, autoload_with=engine)\n",
    "\n",
    "for tablename in tables_and_footnotes.keys():\n",
    "    print(tablename)\n",
    "    df = tables_and_footnotes[tablename]['df']\n",
    "    df.name = df.index.name\n",
    "    with engine.connect() as conn:\n",
    "        for series_name, series in df.items():\n",
    "            tdf = pd.DataFrame(series)\n",
    "            tdf.reset_index(inplace=True)\n",
    "            jstring = pd.DataFrame(tdf).to_json()\n",
    "            stmt = insert(zoning_districts_lookup).values(code=series_name, info=jstring).prefix_with(\"OR IGNORE\")\n",
    "            conn.execute(stmt)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PDF parsed above still has some definitions that are in text outside the tables. From `zoning_table_all.pdf`:\n",
    "\n",
    ">C1-1 through C1-5 and C2-1 through C2-5 are commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\n",
    "\n",
    "* I need to manually create the object to hold this information and put it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_zones = {}\n",
    "info = \"Commercial districts which are mapped as overlays within residential districts. When a commercial overlay is mapped within an R1 through R5 district, except an R5D district, the commercial FAR is 1.0; within an R5D district or an R6 through R10 district, the commercial FAR is 2.0. The residential FAR for a commercial overlay district is determined by the residential district regulations.\"\n",
    "for i in range(1,6):\n",
    "    more_zones[f'C1-{i}'] = info\n",
    "    more_zones[f'C2-{i}'] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    for key in more_zones.keys():\n",
    "        print(more_zones[key])\n",
    "        stmt = insert(zoning_districts_lookup).values(code=key, info=more_zones[key]).prefix_with(\"OR IGNORE\")\n",
    "        conn.execute(stmt)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a few more code meanings \n",
    "* From [NYC Department of Tax and Finance Data Dictionary](https://www.nyc.gov/assets/finance/downloads/tar/tarfieldcodes.pdf):\n",
    "    * LandUse\n",
    "    * OwnerType\n",
    "    * Easment code\n",
    "* Additional information about commercial zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/commercial_zoning_data_tables.pdf).\n",
    "* Additional information about residential zoning that I have not included can be [found here](https://www.nyc.gov/assets/planning/download/pdf/zoning/districts-tools/residence_zoning_data_tables.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MapPLUTO data from geo database file (.gdb)\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO24v4.gdb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the geodatabase (.gdb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement_dict = {\"MN\": 1, \"BX\": 2, \"BN\": 3, \"QN\": 4, \"SI\": 5}\n",
    "\n",
    "geodata = {}\n",
    "# List layers in the GDB file\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers in the GDB file:\")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    # gdf['borough'] = gdf['Borough'].replace(replacement_dict)\n",
    "    try:\n",
    "        gdf['wkb'] = gdf['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the table in the Sqlite database and insert the (modified) data from the gdb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map Pandas/GeoPandas dtypes to SQLAlchemy types\n",
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return Integer\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return Float\n",
    "    elif pd.api.types.is_string_dtype(dtype):\n",
    "        return String\n",
    "    elif pd.api.types.is_binary_dtype(dtype):\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "# Function to dynamically create the Pluto class\n",
    "def create_pluto_class(gdf, name):\n",
    "    attrs = {\n",
    "        '__tablename__': name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(LargeBinary)  # Use LargeBinary for WKB\n",
    "    }\n",
    "    # Add columns dynamically based on the DataFrame's columns\n",
    "    for col, dtype in zip(gdf.columns, gdf.dtypes):\n",
    "        if col not in ['geometry']:  # Columns created sepearately from the automated pipeline\n",
    "            attrs[col] = Column(map_dtype(dtype))\n",
    "    \n",
    "    return type('Pluto', (Base,), attrs)\n",
    "\n",
    "\n",
    "# Base = sqlalchemy.orm.declarative_base()()\n",
    "\n",
    "# for name,gdf in geodata.items():\n",
    "    # Create the Pluto class\n",
    "Pluto = create_pluto_class(geodata['MapPLUTO_24v4_clipped'], 'MapPLUTO_24v4_clipped')\n",
    "# Create the table in the database\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from shapely import wkb\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "\n",
    "# Prepare the data for insertion\n",
    "batch_size = 1000\n",
    "for start in range(0, len(gdf), batch_size):\n",
    "    batch = gdf.iloc[start:start + batch_size]\n",
    "    for _, row in batch.iterrows():\n",
    "        geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "        pluto_entry = Pluto(\n",
    "            geometry=geometry_wkb,\n",
    "            **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "        )\n",
    "        session.add(pluto_entry)\n",
    "    session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SQLITE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a test plot to verify that the geodata was stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "query = \"SELECT ZipCode, geometry FROM MapPLUTO_24v4_clipped\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='ZipCode', aggfunc={'ZipCode': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['ZipCode']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(\"/home/james/Massive/PROJECTDATA/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
