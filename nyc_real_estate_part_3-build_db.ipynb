{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and populate the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import standard libraries\n",
    "# import os\n",
    "# import re\n",
    "import json\n",
    "import codecs\n",
    "# import requests\n",
    "import time\n",
    "import dill\n",
    "time.sleep(3)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# # Import third-party libraries\n",
    "# import geopandas as gpd\n",
    "# from geoalchemy2 import Geometry\n",
    "# import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary, ForeignKey\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# from sqlalchemy.event import listen\n",
    "# from sqlalchemy.engine import Engine\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# import sqlite3\n",
    "# from sqlite3 import dbapi2 as sqlite\n",
    "\n",
    "# import fiona\n",
    "# from fiona.crs import from_epsg\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/select.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    print(dataset.short_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'{SQLITE_PATH}?check_same_thread=False', echo=False)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Configure the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@event.listens_for(engine, \"connect\")\n",
    "def load_spatialite(dbapi_conn, connection_record):\n",
    "    print(\"Loading SpatiaLite extension\")\n",
    "    dbapi_conn.enable_load_extension(True)\n",
    "    dbapi_conn.load_extension(\"mod_spatialite\")\n",
    "    dbapi_conn.enable_load_extension(False)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connection established\")\n",
    "    result = conn.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n",
    "# Enable WAL mode\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
    "\n",
    "# Initialize spatial metadata if not already present\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = {k:v for k,v in col_customization_dict.items() if col_customization_dict[k].is_category == True and 'overlay' not in col_customization_dict[k].new_name} # Codes for overlays are to go in the same table as other zoning codes\n",
    "\n",
    "completed_tables = [] # This is for tracking the names of tables that have been created, which will be used to avoid creating redundant tables for columns that are same-kind repeats (such as \"district_1\" and \"district_2\"), and thus will use the same lookups.\n",
    "\n",
    "lookup_tables = {}\n",
    "\n",
    "for name,table in lookups.items():\n",
    "    print(f\"processing {table}\")\n",
    "    lookup_table_name= re.sub('_[0-9]+$', '', table.new_name)\n",
    "    if any([table.new_name.startswith(prefix) and table.new_name[-1].isdigit() for prefix in completed_tables]):\n",
    "    # if table.new_name[0:len(table)*75] in completed_tables:\n",
    "        print(f\"Lookup table {lookup_table_name} already created, continuing...\")\n",
    "        continue\n",
    "    with engine.connect() as connection:\n",
    "        print(f\"Creating lookup table {lookup_table_name}...\")\n",
    "        # lookup_table = create_lookup_table(engine=engine, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "        lookup_table = create_lookup_table(Base.metadata, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "        # name_prefix = table.new_name[0:round(len(table.new_name)*.75)] # Hopefully this is a safe threshold to identify when columns are repeats of the same type\n",
    "        name_prefix = lookup_table_name\n",
    "        completed_tables.append(name_prefix)\n",
    "        # lookup_tables[lookup_table_name] = lookup_table\n",
    "        lookups[name].orm = lookup_table\n",
    "\n",
    "# metadata.create_all(engine)\n",
    "Base.metadata.create_all(engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,table in lookups.items():\n",
    "    lookup_table = table.orm\n",
    "    if lookup_table is None:\n",
    "        print(f\"Skipping {name}...\")\n",
    "        continue\n",
    "    print(lookup_table)\n",
    "    with engine.connect() as connection:\n",
    "        for definition in table.definitions:\n",
    "            if len(definition) == 2:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1]).on_conflict_do_nothing()\n",
    "                except ValueError:\n",
    "                    stmt = insert(lookup_table).values(name_or_code=definition[0], info=definition[1]).on_conflict_do_nothing()\n",
    "            elif len(definition) == 3:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(definition)\n",
    "                    # stmt = insert(lookup_table).values(id=definition[0], name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "            else:\n",
    "                print(definition)\n",
    "                raise ValueError(\"Was only expecting two or three columns\")\n",
    "            connection.execute(stmt)\n",
    "        connection.commit()\n",
    "    name_prefix = table.new_name[0:round(len(table.new_name)*.75)] # Hopefully this is a safe threshold to identify when columns are repeats of the same type\n",
    "    completed_tables.append(name_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with engine.connect() as conn:\n",
    "#     for row in class_codes:\n",
    "#         print(row)\n",
    "#         stmt = insert(lookup_tables['building_class']).values(name_or_code=row['code'], info=row['name']).prefix_with(\"OR IGNORE\")\n",
    "#         conn.execute(stmt)\n",
    "#         conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MapPLUTO data from geo database file (.gdb)\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO24v4.gdb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the geodatabase (.gdb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geodata = {}\n",
    "# List layers in the GDB file\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers in the GDB file:\")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    # gdf['borough'] = gdf['Borough'].replace(replacement_dict)\n",
    "    try:\n",
    "        gdf['wkb'] = gdf['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the table in the Sqlite database and insert the (modified) data from the gdb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in gdf.columns if col not in col_customization_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())  # Ensure \"basement_type_or_grade_lookup\" is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}\n",
    "rename_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns=rename_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few of the column names did not exactly match up due to slightly different field names than specified in the data dictionary, so these need to be renamed manually:\n",
    "\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in gdf.columns if col not in [i.new_name for i in col_customization_dict.values()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, MetaData, Column, Integer, String, ForeignKey, LargeBinary, Float, Date\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "# Base = declarative_base()\n",
    "# metadata = MetaData()\n",
    "\n",
    "# Reflect the existing database tables once\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Function to map custom dtype to SQLAlchemy types\n",
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        return Integer\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Float':\n",
    "        return Float\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "# Function to dynamically create the table class\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(String),  \n",
    "        'wkb': Column(LargeBinary),  # Use LargeBinary for WKB\n",
    "        'Shape_Leng' : Column(Float), # Add columns not listed in the data dictionary\n",
    "        'Shape_Area' : Column(Float),\n",
    "    }\n",
    "    \n",
    "    for k, v in col_customization_dict.items():\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        if v.is_fk:\n",
    "            attrs[v.new_name] = Column(col_type, ForeignKey(f'{v.new_name}_lookup.id'))\n",
    "        else:\n",
    "            attrs[v.new_name] = Column(col_type)\n",
    "    \n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "# Create the MapPLUTO_24v4_clipped table class\n",
    "MapPLUTO24v4Clipped = create_dynamic_table_class('MapPLUTO_24v4_clipped', col_customization_dict)\n",
    "\n",
    "# Reflect the metadata again to ensure it includes the new table class\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Create all tables in the database\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_cols = [col.new_name for col in col_customization_dict.values() if col.dtype == 'Date']\n",
    "# datetime_cols = [col for col in datetime_cols if col is not None]\n",
    "datetime_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1000\n",
    "# for start in range(0, len(gdf), batch_size):\n",
    "#     batch = gdf.iloc[start:start + batch_size]\n",
    "#     for _, row in batch.iterrows():\n",
    "#         print(row['zoning_district'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from shapely import wkb\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "# gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "\n",
    "# Prepare the data for insertion\n",
    "batch_size = 1000\n",
    "for start in range(0, len(gdf), batch_size):\n",
    "    batch = gdf.iloc[start:start + batch_size]\n",
    "    for _, row in batch.iterrows():\n",
    "        if row['apportionment_date']:\n",
    "            row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "        for col in gdf.columns:\n",
    "            val = row[col]\n",
    "            if isinstance(val, pd.Series):\n",
    "                print(f\"length: {len(val)}\")\n",
    "                print(f\"Column {col} is a Series: first value is {val.iloc[0]} of length {len(val)}\")\n",
    "                try:\n",
    "                    first_value = row[col].iloc[0]\n",
    "                    new = first_value\n",
    "                    row[col] = new\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print('Printing:')\n",
    "                    for i in row[col]:\n",
    "                        print(i, type(i))\n",
    "                print(\"Before type is\", type(row[col]))\n",
    "                print(\"Type is\", type(row[col]))\n",
    "        # rest of your code...\n",
    "        geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "        pluto_entry = MapPLUTO24v4Clipped(\n",
    "            geometry=geometry_wkb,\n",
    "            **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "        )\n",
    "        session.add(pluto_entry)\n",
    "    # for _, row in batch.iterrows():\n",
    "    #     if row['apportionment_date']:\n",
    "    #         row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "    #     geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "    #     pluto_entry = MapPLUTO24v4Clipped(\n",
    "    #         geometry=geometry_wkb,\n",
    "    #         **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "    #     )\n",
    "    #     session.add(pluto_entry)\n",
    "    session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "def populate_lookup_table(engine, lookup_table, source_table_name, lookup_table_name, text_column_name, chunk_size=100, max_retries=5):\n",
    "    \"\"\"\n",
    "    Populate a lookup table in chunks with retries for database lock issues.\n",
    "    \"\"\"\n",
    "    def retry(func, *args, **kwargs):\n",
    "        \"\"\"Retry function with backoff for SQLite locks.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except OperationalError as e:\n",
    "                if \"database is locked\" in str(e):\n",
    "                    print(f\"Database is locked. Retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(0.2 * (attempt + 1))  # Gradual backoff\n",
    "                else:\n",
    "                    raise\n",
    "        raise Exception(\"Exceeded maximum retries due to database locks.\")\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        # Ensure the new column exists\n",
    "        try:\n",
    "            retry(connection.execute, text(f\"ALTER TABLE {source_table_name} ADD COLUMN {text_column_name}_id INTEGER\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Column creation skipped or failed: {e}\")\n",
    "\n",
    "        # Process unique values in chunks\n",
    "        unique_query = f\"SELECT DISTINCT {text_column_name} FROM {source_table_name}\"\n",
    "        unique_values_iter = pd.read_sql(unique_query, engine, chunksize=chunk_size)\n",
    "        \n",
    "        for chunk in unique_values_iter:\n",
    "            unique_values = chunk[text_column_name].dropna().tolist()\n",
    "\n",
    "            # Insert into the lookup table in small batches\n",
    "            for value in unique_values:\n",
    "                stmt = insert(lookup_table).values({text_column_name: value}).on_conflict_do_nothing()\n",
    "                try:\n",
    "                    retry(connection.execute, stmt)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting value '{value}': {e}\")\n",
    "\n",
    "        # Update the source table with foreign key references\n",
    "        update_stmt = text(f\"\"\"\n",
    "        UPDATE {source_table_name}\n",
    "        SET {text_column_name}_id = (\n",
    "            SELECT id \n",
    "            FROM {lookup_table_name}\n",
    "            WHERE {text_column_name} = {source_table_name}.{text_column_name}\n",
    "        )\n",
    "        \"\"\")\n",
    "        try:\n",
    "            retry(connection.execute, update_stmt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating foreign keys: {e}\")\n",
    "        connection.commit()\n",
    "        # Remove the original text column (optional)\n",
    "        connection.execute(text(f\"ALTER TABLE {source_table_name} DROP COLUMN {text_column_name}\"))\n",
    "        connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SQLITE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a test plot to verify that the geodata was stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "query = \"SELECT zip_code, geometry FROM MapPLUTO_24v4_clipped\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(\"/home/james/Massive/PROJECTDATA/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "def populate_lookup_table(engine, lookup_table, source_table_name, lookup_table_name, text_column_name, chunk_size=100, max_retries=5):\n",
    "    \"\"\"\n",
    "    Populate a lookup table in chunks with retries for database lock issues.\n",
    "    \"\"\"\n",
    "    def retry(func, *args, **kwargs):\n",
    "        \"\"\"Retry function with backoff for SQLite locks.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except OperationalError as e:\n",
    "                if \"database is locked\" in str(e):\n",
    "                    print(f\"Database is locked. Retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(0.2 * (attempt + 1))  # Gradual backoff\n",
    "                else:\n",
    "                    raise\n",
    "        raise Exception(\"Exceeded maximum retries due to database locks.\")\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        # Ensure the new column exists\n",
    "        try:\n",
    "            retry(connection.execute, text(f\"ALTER TABLE {source_table_name} ADD COLUMN {text_column_name}_id INTEGER\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Column creation skipped or failed: {e}\")\n",
    "\n",
    "        # Process unique values in chunks\n",
    "        unique_query = f\"SELECT DISTINCT {text_column_name} FROM {source_table_name}\"\n",
    "        unique_values_iter = pd.read_sql(unique_query, engine, chunksize=chunk_size)\n",
    "        \n",
    "        for chunk in unique_values_iter:\n",
    "            unique_values = chunk[text_column_name].dropna().tolist()\n",
    "\n",
    "            # Insert into the lookup table in small batches\n",
    "            for value in unique_values:\n",
    "                stmt = insert(lookup_table).values({text_column_name: value}).on_conflict_do_nothing()\n",
    "                try:\n",
    "                    retry(connection.execute, stmt)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting value '{value}': {e}\")\n",
    "\n",
    "        # Update the source table with foreign key references\n",
    "        update_stmt = text(f\"\"\"\n",
    "        UPDATE {source_table_name}\n",
    "        SET {text_column_name}_id = (\n",
    "            SELECT id \n",
    "            FROM {lookup_table_name}\n",
    "            WHERE {text_column_name} = {source_table_name}.{text_column_name}\n",
    "        )\n",
    "        \"\"\")\n",
    "        try:\n",
    "            retry(connection.execute, update_stmt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating foreign keys: {e}\")\n",
    "        connection.commit()\n",
    "        # Remove the original text column (optional)\n",
    "        connection.execute(text(f\"ALTER TABLE {source_table_name} DROP COLUMN {text_column_name}\"))\n",
    "        connection.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpd_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
