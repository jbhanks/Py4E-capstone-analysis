{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4604261",
   "metadata": {},
   "source": [
    "### Be sure to install postgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import standard libraries\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "# import requests\n",
    "import time\n",
    "import dill\n",
    "time.sleep(5)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01f2ec",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068524e6",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97923aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script creates a PostgreSQL database and sets up the connection using SQLAlchemy.\n",
    "# Settings\n",
    "db_name = \"nycdata\"\n",
    "user = os.getlogin()  # assumes local login user is the DB user\n",
    "host = \"localhost\"\n",
    "\n",
    "# Step 1: Create the database (if not exists)\n",
    "def create_database_if_not_exists(db_name, user, host):\n",
    "    conn = psycopg2.connect(dbname=\"postgres\", user=user, host=host)\n",
    "    conn.autocommit = True  # required for CREATE DATABASE\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check if DB already exists\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (db_name,))\n",
    "    exists = cursor.fetchone()\n",
    "    if not exists:\n",
    "        cursor.execute(f\"CREATE DATABASE {db_name}\")\n",
    "        print(f\"Database '{db_name}' created.\")\n",
    "    else:\n",
    "        print(f\"Database '{db_name}' already exists.\")\n",
    "    try:\n",
    "        cursor.execute('CREATE EXTENSION postgis;')\n",
    "    except psycopg2.errors.DuplicateObject:\n",
    "        pass\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Step 2: Connect to the new database via SQLAlchemy\n",
    "def get_engine(db_name, user, host):\n",
    "    url = URL.create(\n",
    "        drivername=\"postgresql+psycopg2\",\n",
    "        username=user,\n",
    "        host=host,\n",
    "        database=db_name,\n",
    "    )\n",
    "    engine = create_engine(url, echo=True)  # echo=True shows SQL queries\n",
    "    return engine\n",
    "\n",
    "# Usage\n",
    "create_database_if_not_exists(db_name, user, host)\n",
    "engine = get_engine(db_name, user, host)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d34ffb",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5165a6d",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = {}\n",
    "completed_tables = [] # This is for tracking the names of tables that have been created, which will be used to avoid creating redundant tables for columns that are same-kind repeats (such as \"district_1\" and \"district_2\"), and thus will use the same lookups.\n",
    "\n",
    "for name,dataset in dataset_info_dict.items():\n",
    "    lookups |= {k:v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].is_category == True}\n",
    "\n",
    "for name,table in lookups.items():\n",
    "    print(f\"processing {table}\")\n",
    "    if table.new_name is None:\n",
    "        table.new_name = table.short_name\n",
    "    lookup_table_name= re.sub('_[0-9]+$', '', table.new_name)\n",
    "    print(f\"lookup_table_name: {lookup_table_name}\")\n",
    "    if any([table.new_name.startswith(prefix) and table.new_name[-1].isdigit() for prefix in completed_tables]):\n",
    "        print(f\"Lookup table {lookup_table_name} already created, continuing...\")\n",
    "        continue\n",
    "    with engine.connect() as connection:\n",
    "        print(f\"Creating lookup table {lookup_table_name}...\")\n",
    "        lookup_table = create_lookup_table(Base.metadata, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "        print(f\"Created lookup table: {lookup_table}\")\n",
    "        name_prefix = lookup_table_name\n",
    "        completed_tables.append(name_prefix)\n",
    "        lookups[name].orm = lookup_table\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af702e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for name,repetitions in multicolumns.items():\n",
    "        print(name)\n",
    "        print(f\"Setting {name} columns\")\n",
    "        for k in dataset.col_customizations.keys():\n",
    "            if dataset.col_customizations[k].new_name is None:\n",
    "                dataset.col_customizations[k].new_name = dataset.col_customizations[k].short_name\n",
    "        cols = {k:v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.startswith(name)}\n",
    "        print(f'cols for {name} are: {cols}')\n",
    "        main_col = [v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.endswith(\"_1\")]\n",
    "        if main_col:\n",
    "            print(f'main_col is {main_col}')\n",
    "            for key in cols.keys():\n",
    "                print(f'key is {key}')\n",
    "                lookups[key].orm = main_col[0].orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719dc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = ['zoning_district', 'commercial_overlay', 'special_purpose_district']\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for k in dataset.col_customizations.keys():\n",
    "        print(\"ORM:\", dataset.col_customizations[k].orm)\n",
    "        for name in multicolumns:\n",
    "            if k.startswith(name) and k.endswith(\"_1\"):\n",
    "                print(k)\n",
    "                print(lookups[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742dc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,table in lookups.items():\n",
    "    print(f\"table.orm: {table.orm=}\")\n",
    "    lookup_table = table.orm\n",
    "    print(f'lookup_table {lookup_table}')\n",
    "    if lookup_table is None:\n",
    "        print(f\"Skipping {name}...\")\n",
    "        continue\n",
    "    print(lookup_table)\n",
    "    with engine.connect() as connection:\n",
    "        for definition in table.definitions:\n",
    "            if len(definition) == 2:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1]).on_conflict_do_nothing()\n",
    "                except ValueError:\n",
    "                    stmt = insert(lookup_table).values(name_or_code=definition[0], info=definition[1]).on_conflict_do_nothing()\n",
    "            elif len(definition) == 3:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(definition)\n",
    "            else:\n",
    "                print(definition)\n",
    "                raise ValueError(\"Was only expecting two or three columns\")\n",
    "            connection.execute(stmt)\n",
    "        connection.commit()\n",
    "    name_prefix = table.new_name[0:round(len(table.new_name)*.75)] # Hopefully this is a safe threshold to identify when columns are repeats of the same type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61feb832",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MapPLUTO data from geo database file (.gdb)\n",
    "pluto_version = \"25v1_1\"\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO{pluto_version}.gdb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196376c1",
   "metadata": {},
   "source": [
    "* Import the geodatabase (.gdb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geodata = {}\n",
    "# List layers in the GDB file\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers in the GDB file:\")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    # gdf['borough'] = gdf['Borough'].replace(replacement_dict)\n",
    "    try:\n",
    "        gdf['wkb'] = gdf['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geodata[f'MapPLUTO_{pluto_version}_clipped']\n",
    "is_whole_number = {(gdf[col].notna() & (gdf[col] % 1 == 0)).all() for col in gdf.columns if gdf[col].dtype == 'float'}\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and change dtype to int where applicable\n",
    "for col in gdf.columns:\n",
    "    print(f'Processing column: {col} of dtype {gdf[col].dtype}')\n",
    "    if gdf[col].dtype == float and is_whole_number_series(gdf[col]) and gdf[col].name not in ['wkb', 'geometry', 'Shape_Length', 'Shape_Area']:\n",
    "        print(f'Column {col} is {is_whole_number_series(gdf[col])}')\n",
    "        print(f'Converting {col} to integer')\n",
    "        gdf[col] = gdf[col].astype('Int64')  # 'Int64' for nullable integer type in Pandas\n",
    "    else:\n",
    "        print(f\"Skipping {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())  # Ensure \"basement_type_or_grade_lookup\" is listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c034b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_info_dict['mapPLUTO'].col_customizations['number_of_floors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}\n",
    "gdf = gdf.rename(columns=rename_mappings)\n",
    "\n",
    "# Set data types explicitly based on the data dictionary\n",
    "\n",
    "# 1. Identify object columns\n",
    "# object_cols = gdf.select_dtypes(include=['object']).columns\n",
    "\n",
    "import numpy as np\n",
    "print(gdf['number_of_floors'].map(type).value_counts())\n",
    "\n",
    "non_float = gdf['number_of_floors'][~gdf['number_of_floors'].map(lambda x: isinstance(x, float))]\n",
    "print(non_float.head(20))\n",
    "\n",
    "gdf['number_of_floors'] = pd.to_numeric(gdf['number_of_floors'], errors='coerce')\n",
    "print(gdf['number_of_floors'].map(type).value_counts())\n",
    "gdf['number_of_floors'] = gdf['number_of_floors'].astype('float64')\n",
    "\n",
    "# Step 1: Convert to string\n",
    "gdf['number_of_floors'] = gdf['number_of_floors'].astype(str)\n",
    "\n",
    "# Step 2: Replace all missing value representations with np.nan\n",
    "gdf['number_of_floors'] = gdf['number_of_floors'].replace(\n",
    "    ['nan', 'NaN', 'None', 'NULL', '', ' ', '<NA>', None, pd.NA], np.nan\n",
    ")\n",
    "\n",
    "# Step 3: Convert to float (this will ensure all values are float or np.nan)\n",
    "gdf['number_of_floors'] = gdf['number_of_floors'].astype(float)\n",
    "\n",
    "# Step 4: Now cast to pandas nullable integer type\n",
    "# gdf['number_of_floors'] = gdf['number_of_floors'].astype('Int64')\n",
    "\n",
    "# 2. Decide target types for each column (example: from your data dictionary)\n",
    "target_types = {}\n",
    "for k, v in col_customization_dict.items():\n",
    "    try:\n",
    "        if v.new_name in gdf.columns:\n",
    "            if v.dtype == 'String':\n",
    "                gdf[v.new_name] = gdf[v.new_name].astype('string')\n",
    "            elif v.dtype == 'Integer':\n",
    "                gdf[v.new_name] = pd.to_numeric(gdf[v.new_name], errors='coerce').astype('Int64')\n",
    "            elif v.dtype == 'Float':\n",
    "                gdf[v.new_name] = pd.to_numeric(gdf[v.new_name], errors='coerce').astype('float64')\n",
    "            if v.new_name in ['Shape_Leng', 'Shape_Area']:\n",
    "                gdf['v.new_name'].astype('float64')\n",
    "            # Add more dtype handling as needed\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError for column {v.new_name}: {e}\")\n",
    "        print(v)\n",
    "\n",
    "# # 2. Dynamically create ORM/table from col_customization_dict (as you do now)\n",
    "# MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "# Base.metadata.create_all(engine)\n",
    "\n",
    "# orm_columns = {col: getattr(MapPLUTO_Clipped, col).property.columns[0].type for col in MapPLUTO_Clipped.__table__.columns.keys()}\n",
    "# for col in gdf.columns:\n",
    "#     orm_type = orm_columns.get(col)\n",
    "#     pd_type = gdf[col].dtype\n",
    "#     print(f\"{col}: DataFrame dtype={pd_type}, ORM type={orm_type}\")\n",
    "#     # Add checks for integer overflow, type mismatches, etc.\n",
    "\n",
    "\n",
    "# # 3. Try to cast each object column to its intended type\n",
    "# for col in object_cols:\n",
    "#     if col in target_types:\n",
    "#         try:\n",
    "#             gdf[col] = gdf[col].astype(target_types[col])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to cast column {col} to {target_types[col]}: {e}\")\n",
    "#             # Optionally, inspect problematic values:\n",
    "#             print(gdf[col].map(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf05ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_vals = gdf['number_of_floors'][pd.to_numeric(gdf['number_of_floors'], errors='coerce').isna() & gdf['number_of_floors'].notna()]\n",
    "print(\"Non-numeric values in number_of_floors:\", bad_vals.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few of the column names did not exactly match up due to slightly different field names than specified in the data dictionary, so these need to be renamed manually:\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)\n",
    "print(\"gdf columns:\", gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, MetaData, Column, Integer, String, ForeignKey, LargeBinary, Float, Date\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import BigInteger\n",
    "\n",
    "# Reflect the existing database tables once\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Function to map custom dtype to SQLAlchemy types\n",
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        # Use BigInteger for columns known to exceed 2^31-1\n",
    "        if k in ['bbl', 'bin', 'apportionment_bbl', 'borough_tax_block_and_lot', 'tax_block', 'tax_lot', 'census_block_2020', 'census_block_2010', 'census_block']:\n",
    "            return String\n",
    "        else:\n",
    "            return Integer\n",
    "    elif dtype == 'Float' or dtype == 'Double' or dtype == 'double precision':\n",
    "        return Float\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "# Function to dynamically create the table class\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(String),  \n",
    "        'wkb': Column(LargeBinary),  # Use LargeBinary for WKB\n",
    "        'Shape_Leng' : Column(Float), # Add columns not listed in the data dictionary\n",
    "        'Shape_Area' : Column(Float),\n",
    "        'version_number' : Column(String),\n",
    "        'apportionment_bbl': Column(String),  # Add apportionment_bbl column\n",
    "        'borough_tax_block_and_lot': Column(String),  # Add borough_tax_block_and_lot column\n",
    "        'community_district': Column(String), \n",
    "        'census_tract_2020': Column(String),\n",
    "        'basement_type_or_grade': Column(String),\n",
    "        'sanitation_district_boro': Column(String), \n",
    "        'sanitation_district_number': Column(String),\n",
    "        'health_center_district': Column(String),\n",
    "        '2007_flood_insurance_rate_map_indicator': Column(String),\n",
    "        '2015_preliminary_flood_insurance_rate_map': Column(String),\n",
    "        'police_precinct': Column(String),\n",
    "\n",
    "    }\n",
    "    attrs['__table_args__'] = {'extend_existing': True}\n",
    "    \n",
    "    for k, v in col_customization_dict.items():\n",
    "        if v.new_name == \"version_number\":\n",
    "            continue  # Already defined above\n",
    "        if any([name for name in multicolumns if name in k]):\n",
    "            k = re.sub('_[0-9]$', '', k)\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        if v.is_fk:\n",
    "            attrs[k] = Column(Integer, ForeignKey(f'{v.new_name}_lookup.id'))\n",
    "        elif v.is_category:\n",
    "            print(f'Creating id column for {v.new_name}')\n",
    "            attrs[v.new_name] = Column(col_type)\n",
    "            attrs[f\"{v.new_name}_id\"] = Column(Integer, ForeignKey(f'{k}_lookup.id'))\n",
    "        else:\n",
    "            attrs[v.new_name] = Column(col_type)\n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "# 2. Dynamically create ORM/table from col_customization_dict (as you do now)\n",
    "MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "orm_columns = {col: getattr(MapPLUTO_Clipped, col).property.columns[0].type for col in MapPLUTO_Clipped.__table__.columns.keys()}\n",
    "for col in gdf.columns:\n",
    "    orm_type = orm_columns.get(col)\n",
    "    pd_type = gdf[col].dtype\n",
    "    print(f\"{col}: DataFrame dtype={pd_type}, ORM type={orm_type}\")\n",
    "    # Add checks for integer overflow, type mismatches, etc.\n",
    "\n",
    "# Create the MapPLUTO clipped table class\n",
    "# MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "\n",
    "# # Reflect the metadata again to ensure it includes the new table class\n",
    "# metadata.reflect(bind=engine)\n",
    "\n",
    "# # Create all tables in the database\n",
    "# Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fed189",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['Shape_Leng'] = gdf['Shape_Leng'].astype('float64')\n",
    "gdf['Shape_Leng'].dtype\n",
    "gdf['health_center_district'] = gdf['health_center_district'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MapPLUTO_Clipped.__table__.columns.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Column compatibility check:\")\n",
    "for col in gdf.columns:\n",
    "    orm_type = orm_columns.get(col)\n",
    "    pd_type = gdf[col].dtype\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"  DataFrame dtype: {pd_type}\")\n",
    "    print(f\"  ORM type: {orm_type}\")\n",
    "\n",
    "    # Check for float compatibility\n",
    "    if \"Float\" in str(type(orm_type)):\n",
    "        # Only check with np.issubdtype if pd_type is a numpy dtype\n",
    "        if isinstance(pd_type, np.dtype):\n",
    "            if not np.issubdtype(pd_type, np.floating):\n",
    "                print(\"  WARNING: DataFrame column is not float but ORM expects Float.\")\n",
    "        else:\n",
    "            # For extension dtypes, print a warning or skip\n",
    "            print(\"  WARNING: DataFrame column is not a numpy float dtype but ORM expects Float.\")\n",
    "\n",
    "    # Check for string compatibility\n",
    "    if \"String\" in str(type(orm_type)):\n",
    "        if pd_type not in [\"object\", \"string\"]:\n",
    "            print(f\"  WARNING: DataFrame column is not string but ORM expects String. Column is actually {pd_type}\")\n",
    "\n",
    "    # Check for LargeBinary compatibility\n",
    "    if \"LargeBinary\" in str(type(orm_type)):\n",
    "        if pd_type != \"object\":\n",
    "            print(f\"  WARNING: DataFrame column is not object but ORM expects LargeBinary. Column is actually {pd_type}\")\n",
    "\n",
    "print(\"\\nCheck complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from shapely import wkb\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "# gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "# def format_float(value):\n",
    "#     return str(value).rstrip('0').rstrip('.') if '.' in str(value) else str(value)\n",
    "\n",
    "import traceback\n",
    "\n",
    "batch_size = 100\n",
    "# gdf = gdf.where(pd.notnull(gdf), None)\n",
    "gdf = gdf.applymap(lambda x: None if pd.isna(x) else x)\n",
    "with SessionLocal() as session:\n",
    "    for start in range(0, len(gdf), batch_size):\n",
    "        batch = gdf.iloc[start:start + batch_size]\n",
    "        batch = batch.where(pd.notnull(batch), None)\n",
    "        print(\"gdf columns:\", gdf.columns.tolist())\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                if row['apportionment_date']:\n",
    "                    row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "                for col in gdf.columns:\n",
    "                    val = row[col]\n",
    "                geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "                for col in gdf.columns:\n",
    "                    if col in orm_columns and str(orm_columns[col]) == \"INTEGER\":\n",
    "                        val = row[col]\n",
    "                        if pd.isna(val):\n",
    "                            row[col] = None\n",
    "                        elif isinstance(val, float) and val.is_integer():\n",
    "                            row[col] = int(val)\n",
    "                        elif isinstance(val, (int, np.integer)):\n",
    "                            row[col] = int(val)\n",
    "                        else:\n",
    "                            print(f\"WARNING: {col} has non-integer value: {val} (type: {type(val)})\")\n",
    "                pluto_entry = MapPLUTO_Clipped(\n",
    "                    geometry=geometry_wkb,\n",
    "                    **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "                )\n",
    "                for key, col in pluto_entry.__table__.columns.items():\n",
    "                    val = getattr(pluto_entry, key)\n",
    "                    # Print all values\n",
    "                    print(f\"{key}: {val} (type: {type(val)})\")\n",
    "                    # If column is Integer in the schema, check type and value\n",
    "                    if str(col.type) == \"INTEGER\":\n",
    "                        if val is None:\n",
    "                            continue\n",
    "                        if isinstance(val, float):\n",
    "                            if not val.is_integer():\n",
    "                                print(f\"  ERROR: {key} is float but not integer-valued: {val}\")\n",
    "                            elif val > 2_147_483_647 or val < -2_147_483_648:\n",
    "                                print(f\"  ERROR: {key} float value out of range: {val}\")\n",
    "                        elif isinstance(val, (int, np.integer)):\n",
    "                            if val > 2_147_483_647 or val < -2_147_483_648:\n",
    "                                print(f\"  ERROR: {key} int value out of range: {val}\")\n",
    "                        else:\n",
    "                            print(f\"  ERROR: {key} has unexpected type for INTEGER column: {type(val)} value={val}\")\n",
    "                session.add(pluto_entry)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError at row index {idx}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                print(\"Row data (full values):\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        val = row[col]\n",
    "                        print(f\"  {col}: {repr(val)} (type: {type(val)})\")\n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"  Error printing column {col}: {sub_e}\")\n",
    "                print(\"Full traceback:\")\n",
    "                traceback.print_exc()\n",
    "                raise e  # re-raise after logging for further debugging\n",
    "        # NEW: Wrap commit in try/except to catch batch errors\n",
    "        # Before session.commit()\n",
    "        int_cols = [col for col in batch.columns if str(batch[col].dtype) in ['int64', 'Int64']]\n",
    "        for col in int_cols:\n",
    "            col_max = batch[col].max()\n",
    "            col_min = batch[col].min()\n",
    "            print(f\"[Batch check] {col}: max={col_max}, min={col_min}, dtype={batch[col].dtype}\")\n",
    "            # Optionally, check for out-of-range values\n",
    "            if (col_max is not np.nan and col_max > 2_147_483_647) or (col_min is not np.nan and col_min < -2_147_483_648):\n",
    "                print(f\"  WARNING: {col} has out-of-range value for PostgreSQL INTEGER!\")\n",
    "        try:\n",
    "            session.commit()\n",
    "        except Exception as e:\n",
    "            print(\"\\nException during session.commit()\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(\"Batch start index:\", start)\n",
    "            print(\"Batch end index:\", start + batch_size)\n",
    "            print(\"Dumping example rows in this batch with their version_number and types:\")\n",
    "            for idx, row in batch.iterrows():\n",
    "                orm_type = orm_columns.get(col)\n",
    "                val = row[col]\n",
    "                if \"Integer\" in str(type(orm_type)):\n",
    "                    print(f\"  {col}: {val} (type: {type(val)})\")\n",
    "                print(f\"Row {idx}:\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        val = row[col]\n",
    "                        print(f\"  {col}: {repr(val)} (type: {type(val)})\")\n",
    "                    except Exception as sub_e:\n",
    "                        pass\n",
    "                        print(f\"  Error printing column {col}: {sub_e}\")\n",
    "            print(\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "            session.rollback()  # Rollback the session to avoid partial commits\n",
    "            raise e  # re-raise after logging for further debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f21e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.dtypes)\n",
    "for col in gdf.columns:\n",
    "    if gdf[col].dtype in ['int64', 'Int64', 'float64']:\n",
    "        print(f\"{col}: max={gdf[col].max()}, min={gdf[col].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a220531",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895a506",
   "metadata": {},
   "source": [
    "* Make a test plot to verify that the geodata was stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c23002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "query = f\"SELECT zip_code, geometry FROM MapPLUTO_{pluto_version}_clipped\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(f\"{PROJECT_DATA}/figures/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_property",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
