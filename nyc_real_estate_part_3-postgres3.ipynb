{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import (\n",
    "    create_engine, Column, Integer, String, Date, MetaData, event, Table, text,\n",
    "    LargeBinary,  BigInteger, SmallInteger, ForeignKey, Float, inspect\n",
    ")\n",
    "import ast\n",
    "from sqlalchemy.dialects.postgresql import JSON, JSONB  # if not already imported\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "# If your project structure matches the original, keep these:\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *\n",
    "\n",
    "# Geo / GIS helpers used later\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely import wkb, from_wkt  # Shapely 2.x\n",
    "from geoalchemy2.shape import from_shape\n",
    "\n",
    "import pprint\n",
    "\n",
    "PLUTO_VERSION = \"25v2_1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc084db6",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# your Linux login must match a PostgreSQL role with rights to create DBs on first run\n",
    "PGUSER = \"jam\"  # or: getpass.getuser()\n",
    "PGPASSWORD = None  # not used with peer auth\n",
    "PGHOST = \"/run/postgresql\"  # socket directory on Arch/EndeavourOS; Debian/RHEL often /var/run/postgresql\n",
    "PGPORT = \"5432\"   # ignored for sockets but harmless\n",
    "PGDATABASE = \"nyc_data\"\n",
    "\n",
    "# Force socket usage via query param (URL-encode the path just to be safe)\n",
    "_socket = quote_plus(PGHOST)\n",
    "POSTGRES_URL = None  # leave None so we can build URLs per DB below\n",
    "\n",
    "# def make_url(dbname: str) -> str:\n",
    "#     return f\"postgresql+psycopg2://{PGUSER}@/{dbname}?host={_socket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# admin connection (to create DB if missing)\n",
    "admin_engine = create_engine(make_url(\"postgres\", PGUSER=PGUSER, _socket=_socket), future=True)\n",
    "# target DB connection\n",
    "engine = create_engine(make_url(PGDATABASE, PGUSER=PGUSER, _socket=_socket), future=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sqlalchemy import text\n",
    "\n",
    "SAFE_DB_RE = re.compile(r\"^[a-z_][a-z0-9_]*$\")  # postgres-friendly\n",
    "if not SAFE_DB_RE.fullmatch(PGDATABASE):\n",
    "    raise ValueError(f\"Unsafe database name: {PGDATABASE!r}\")\n",
    "\n",
    "with admin_engine.connect() as conn:\n",
    "    exists = conn.execute(\n",
    "        text(\"SELECT 1 FROM pg_database WHERE datname = :d\"),\n",
    "        {\"d\": PGDATABASE},\n",
    "    ).scalar()\n",
    "\n",
    "if not exists:\n",
    "    # autocommit required for CREATE DATABASE\n",
    "    with admin_engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "        conn.execute(text(f\"CREATE DATABASE {PGDATABASE}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create engine to the target database\n",
    "engine = create_engine(\n",
    "    make_url(PGDATABASE, PGUSER=PGUSER, _socket=_socket),\n",
    "    echo=False,\n",
    "    future=True,\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False, future=True)\n",
    "\n",
    "# Ensure PostGIS is available\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e475239",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c64bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdb8dc",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e9a73",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for name, repetitions in multicolumns.items():\n",
    "        for k in dataset.col_customizations.keys():\n",
    "            if dataset.col_customizations[k].new_name is None:\n",
    "                dataset.col_customizations[k].new_name = dataset.col_customizations[k].short_name\n",
    "        _ = {k: v for k, v in dataset.col_customizations.items()\n",
    "             if dataset.col_customizations[k].new_name.startswith(name)}\n",
    "        _ = [v for k, v in dataset.col_customizations.items()\n",
    "             if dataset.col_customizations[k].new_name.endswith(\"_1\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db0140",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b195f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pluto_version = \"25v2_1\"\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO{PLUTO_VERSION}.gdb\"\n",
    "\n",
    "geodata = {}\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "for layer in layers:\n",
    "    gdf_layer = gpd.read_file(gdb_path, layer=layer)\n",
    "    try:\n",
    "        gdf_layer['wkb'] = gdf_layer['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf_layer\n",
    "\n",
    "gdf = geodata[f'MapPLUTO_{PLUTO_VERSION}_clipped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whole_number_series(s: pd.Series) -> bool:\n",
    "    try:\n",
    "        notna = s.notna()\n",
    "        return (notna & ((s[notna] % 1) == 0)).all()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "for col in gdf.columns:\n",
    "    if gdf[col].dtype == float and is_whole_number_series(gdf[col]):\n",
    "        gdf[col] = gdf[col].astype('Int64')\n",
    "\n",
    "inspector = inspect(engine)\n",
    "\n",
    "col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}\n",
    "gdf = gdf.rename(columns=rename_mappings)\n",
    "\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)\n",
    "gdf = add_padding_to_special_columns(gdf, SPECIAL_PADDING_COLS)\n",
    "gdf.columns = list(map(str.lower, gdf.columns))\n",
    "\n",
    "# after you load dataset_info_dict and before create_dynamic_table_class() / create_all()\n",
    "for k, v in col_customization_dict.items():\n",
    "    if v.new_name == \"version_number\":\n",
    "        v.dtype = \"String\"   # ensure TEXT in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        return Integer\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Float':\n",
    "        return Float\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    # IMPORTANT: Use LargeBinary for 'geometry' because you actually store WKB bytes there.\n",
    "    table_name = table_name.lower()\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(LargeBinary),  # store WKB bytes (BYTEA)\n",
    "        'wkb': Column(LargeBinary),       # kept for parity with original schema\n",
    "        'shape_leng': Column(Float),\n",
    "        'shape_area': Column(Float),\n",
    "        '__table_args__': {'extend_existing': True}\n",
    "    }\n",
    "    for k, v in col_customization_dict.items():\n",
    "        if any(name for name in multicolumns if name in k):\n",
    "            k = re.sub('_[0-9]$', '', k)\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        attrs[v.new_name] = Column(col_type)\n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{PLUTO_VERSION}_clipped', col_customization_dict)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns\n",
    "list(map(str.lower, gdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "\n",
    "def normalize_zip(v):\n",
    "    if v is None: \n",
    "        return None\n",
    "    # floats/ints -> 5-digit string\n",
    "    if isinstance(v, (int, float)) and not (isinstance(v, float) and math.isnan(v)):\n",
    "        return f\"{int(v):05d}\"\n",
    "    s = str(v).strip()\n",
    "    if s == \"\": \n",
    "        return None\n",
    "    # '10013.0' -> '10013'\n",
    "    m = re.fullmatch(r\"\\d+(?:\\.0+)?\", s)\n",
    "    if m:\n",
    "        return f\"{int(float(s)):05d}\"\n",
    "    # '10013-1234' or '10013 1234' -> '10013-1234'\n",
    "    m = re.fullmatch(r\"(\\d{5})[-\\s]?(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}\"\n",
    "    # anything else: leave as-is (rare)\n",
    "    return s\n",
    "\n",
    "\n",
    "batch_size = 100000\n",
    "with SessionLocal() as session:\n",
    "    for start in range(0, len(gdf), batch_size):\n",
    "        batch = gdf.iloc[start:start + batch_size]\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                if 'apportionment_date' in row and row['apportionment_date']:\n",
    "                    row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "                for col in gdf.columns:\n",
    "                    # print(f\"Processing column: {col}\")\n",
    "                    val = row[col]\n",
    "                    # print(f\"Value: {val}, Type: {type(val)}\")\n",
    "                    if isinstance(val, pd.Series):\n",
    "                        try:\n",
    "                            first_value = val.iloc[0]\n",
    "                            row[col] = first_value\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing Series in column {col} at row {idx}: {e}\")\n",
    "                    if pd.isna(val):\n",
    "                        row[col] = None\n",
    "                geometry_wkb = row['geometry'].wkb if row['geometry'] is not None else None\n",
    "                # in your insert loop before constructing MapPLUTO_Clipped(...)\n",
    "                if 'zip_code' in row:\n",
    "                    row['zip_code'] = normalize_zip(row['zip_code'])\n",
    "                pluto_entry = MapPLUTO_Clipped(\n",
    "                    geometry=geometry_wkb,\n",
    "                    **{col: row[col] for col in gdf.columns if col != 'geometry'}\n",
    "                )\n",
    "                session.add(pluto_entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row index {idx}\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        print(f\"Column: {col}, Value: {row[col]}, Type: {type(row[col])}\")\n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"Error printing column {col}: {sub_e}\")\n",
    "                raise e\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(gdf[gdf.eq(\"E-61\").any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkb\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql(f'SELECT zip_code, geometry FROM \"mappluto_{PLUTO_VERSION}_clipped\"', engine)\n",
    "\n",
    "def to_geom(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    # psycopg2 often returns memoryview for bytea\n",
    "    if isinstance(val, memoryview):\n",
    "        return wkb.loads(val.tobytes())\n",
    "    if isinstance(val, (bytes, bytearray)):\n",
    "        return wkb.loads(bytes(val))\n",
    "    # if somehow a hex text like \"\\x010203...\" sneaks through:\n",
    "    if isinstance(val, str) and val.startswith(\"\\\\x\"):\n",
    "        return wkb.loads(bytes.fromhex(val[2:]))\n",
    "    raise TypeError(f\"Unexpected geometry value type: {type(val)}\")\n",
    "\n",
    "df[\"geometry\"] = df[\"geometry\"].apply(to_geom)\n",
    "gdf_map = gpd.GeoDataFrame(df, geometry=\"geometry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(f\"{PROJECT_DATA}/figures/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91968eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pg_int_bounds(sqlatype):\n",
    "    # Map SQL types to numpy bounds\n",
    "    if isinstance(sqlatype, SmallInteger):\n",
    "        return np.iinfo(np.int16).min, np.iinfo(np.int16).max\n",
    "    elif isinstance(sqlatype, BigInteger):\n",
    "        return np.iinfo(np.int64).min, np.iinfo(np.int64).max\n",
    "    else:\n",
    "        # Default Integer in Postgres is 32-bit\n",
    "        return np.iinfo(np.int32).min, np.iinfo(np.int32).max\n",
    "\n",
    "def enforce_integer_bounds(df, DynamicTable):\n",
    "    \"\"\"\n",
    "    For each INTEGER-like column in DynamicTable, coerce df[col] to numeric and\n",
    "    replace out-of-range values with None. Logs offenders for inspection.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in DynamicTable.__table__.columns:\n",
    "            continue\n",
    "        col_type = DynamicTable.__table__.columns[col].type\n",
    "\n",
    "        # Only care about integer-ish SQL column types\n",
    "        if not isinstance(col_type, (SmallInteger, Integer, BigInteger)):\n",
    "            continue\n",
    "\n",
    "        lo, hi = _pg_int_bounds(col_type)\n",
    "\n",
    "        # Coerce to numeric safely; keep strings like \"7\" working\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")  # floats OK here\n",
    "        # If column is conceptually integer, store as pandas nullable Int64 to preserve NULLs\n",
    "        # but do comparisons on the coerced float/int series 's'\n",
    "        mask_oob = (s.notna()) & ((s < lo) | (s > hi))\n",
    "\n",
    "        if mask_oob.any():\n",
    "            bad = df.loc[mask_oob, col]\n",
    "            # Show a small sample for debugging\n",
    "            sample_vals = bad.head(5).tolist()\n",
    "            print(\n",
    "                f\"❗ Out-of-range detected in integer column '{col}' \"\n",
    "                f\"({len(bad)} rows). Bounds [{lo}, {hi}]. Sample: {sample_vals}\"\n",
    "            )\n",
    "            # Null them out so insert won't fail\n",
    "            df.loc[mask_oob, col] = None\n",
    "\n",
    "        # Final cast to pandas nullable integer if column is nullable; else leave as numeric\n",
    "        # (Your pipeline already handles nullable detection; this is safe either way.)\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            # If the column had non-integer strings, keep as object; the DB will cast or fail elsewhere\n",
    "            pass\n",
    "\n",
    "from sqlalchemy.exc import DataError\n",
    "\n",
    "def insert_with_diagnosis(conn, insert_stmt, batch_rows, key_hint='sid'):\n",
    "    try:\n",
    "        # conn.execute(insert_stmt, batch_rows)\n",
    "        insert_with_diagnosis(conn, insert_stmt, batch_rows, key_hint='sid')\n",
    "        return\n",
    "    except DataError as e:\n",
    "        print(\"❌ Batch insert failed; diagnosing offending rows…\", e)\n",
    "        lo, hi = 0, len(batch_rows)\n",
    "        # binary search the smallest failing prefix\n",
    "        while lo + 1 < hi:\n",
    "            mid = (lo + hi) // 2\n",
    "            try:\n",
    "                conn.execute(insert_stmt, batch_rows[:mid])\n",
    "                lo = mid\n",
    "            except DataError:\n",
    "                hi = mid\n",
    "        bad = batch_rows[lo:hi]  # minimal failing slice\n",
    "        print(f\"🚩 Offending rows count: {len(bad)}\")\n",
    "        if key_hint and key_hint in bad[0]:\n",
    "            print(f\"Example offending {key_hint}: {bad[0][key_hint]}\")\n",
    "        # You can drop or sanitize and retry:\n",
    "        # conn.execute(insert_stmt, batch_rows[:lo] + batch_rows[hi:])\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_integer_bounds(df, DynamicTable):\n",
    "    \"\"\"\n",
    "    For each INTEGER-like column in DynamicTable, coerce df[col] to numeric and\n",
    "    replace out-of-range values with None. Logs offenders for inspection.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in DynamicTable.__table__.columns:\n",
    "            continue\n",
    "        col_type = DynamicTable.__table__.columns[col].type\n",
    "\n",
    "        # Only care about integer-ish SQL column types\n",
    "        if not isinstance(col_type, (SmallInteger, Integer, BigInteger)):\n",
    "            continue\n",
    "\n",
    "        lo, hi = _pg_int_bounds(col_type)\n",
    "\n",
    "        # Coerce to numeric safely; keep strings like \"7\" working\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")  # floats OK here\n",
    "        # If column is conceptually integer, store as pandas nullable Int64 to preserve NULLs\n",
    "        # but do comparisons on the coerced float/int series 's'\n",
    "        mask_oob = (s.notna()) & ((s < lo) | (s > hi))\n",
    "\n",
    "        if mask_oob.any():\n",
    "            bad = df.loc[mask_oob, col]\n",
    "            # Show a small sample for debugging\n",
    "            sample_vals = bad.head(5).tolist()\n",
    "            print(\n",
    "                f\"❗ Out-of-range detected in integer column '{col}' \"\n",
    "                f\"({len(bad)} rows). Bounds [{lo}, {hi}]. Sample: {sample_vals}\"\n",
    "            )\n",
    "            # Null them out so insert won't fail\n",
    "            df.loc[mask_oob, col] = None\n",
    "\n",
    "        # Final cast to pandas nullable integer if column is nullable; else leave as numeric\n",
    "        # (Your pipeline already handles nullable detection; this is safe either way.)\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            # If the column had non-integer strings, keep as object; the DB will cast or fail elsewhere\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.dbutils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Run for all JSON datasets\n",
    "for name, dataset in dataset_info_dict.items():\n",
    "    if dataset.format == 'json':\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd95923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_property",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
