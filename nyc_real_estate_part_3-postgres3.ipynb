{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import (\n",
    "    create_engine, Column, Integer, String, Date, MetaData, event, Table, text,\n",
    "    LargeBinary,  BigInteger, SmallInteger, ForeignKey, Float, inspect\n",
    ")\n",
    "import ast\n",
    "from sqlalchemy.dialects.postgresql import JSON, JSONB  # if not already imported\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "# If your project structure matches the original, keep these:\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *\n",
    "\n",
    "# Geo / GIS helpers used later\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely import wkb, from_wkt  # Shapely 2.x\n",
    "from geoalchemy2.shape import from_shape\n",
    "\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dataset_info_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc084db6",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# your Linux login must match a PostgreSQL role with rights to create DBs on first run\n",
    "PGUSER = \"jam\"  # or: getpass.getuser()\n",
    "PGPASSWORD = None  # not used with peer auth\n",
    "PGHOST = \"/run/postgresql\"  # socket directory on Arch/EndeavourOS; Debian/RHEL often /var/run/postgresql\n",
    "PGPORT = \"5432\"   # ignored for sockets but harmless\n",
    "PGDATABASE = \"nyc_data\"\n",
    "\n",
    "# Force socket usage via query param (URL-encode the path just to be safe)\n",
    "_socket = quote_plus(PGHOST)\n",
    "POSTGRES_URL = None  # leave None so we can build URLs per DB below\n",
    "\n",
    "def make_url(dbname: str) -> str:\n",
    "    return f\"postgresql+psycopg2://{PGUSER}@/{dbname}?host={_socket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# admin connection (to create DB if missing)\n",
    "admin_engine = create_engine(make_url(\"postgres\"), future=True)\n",
    "# target DB connection\n",
    "engine = create_engine(make_url(PGDATABASE), future=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sqlalchemy import text\n",
    "\n",
    "SAFE_DB_RE = re.compile(r\"^[a-z_][a-z0-9_]*$\")  # postgres-friendly\n",
    "if not SAFE_DB_RE.fullmatch(PGDATABASE):\n",
    "    raise ValueError(f\"Unsafe database name: {PGDATABASE!r}\")\n",
    "\n",
    "with admin_engine.connect() as conn:\n",
    "    exists = conn.execute(\n",
    "        text(\"SELECT 1 FROM pg_database WHERE datname = :d\"),\n",
    "        {\"d\": PGDATABASE},\n",
    "    ).scalar()\n",
    "\n",
    "if not exists:\n",
    "    # autocommit required for CREATE DATABASE\n",
    "    with admin_engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "        conn.execute(text(f\"CREATE DATABASE {PGDATABASE}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create engine to the target database\n",
    "engine = create_engine(\n",
    "    make_url(PGDATABASE),\n",
    "    echo=False,\n",
    "    future=True,\n",
    "    # Keep executemany behavior similar to your original bulk inserts.\n",
    "    # (No PG-specific fast paths enabled unless you ask for them.)\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False, future=True)\n",
    "\n",
    "# Ensure PostGIS is available\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e475239",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c64bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdb8dc",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e9a73",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for name, repetitions in multicolumns.items():\n",
    "        for k in dataset.col_customizations.keys():\n",
    "            if dataset.col_customizations[k].new_name is None:\n",
    "                dataset.col_customizations[k].new_name = dataset.col_customizations[k].short_name\n",
    "        _ = {k: v for k, v in dataset.col_customizations.items()\n",
    "             if dataset.col_customizations[k].new_name.startswith(name)}\n",
    "        _ = [v for k, v in dataset.col_customizations.items()\n",
    "             if dataset.col_customizations[k].new_name.endswith(\"_1\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db0140",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b195f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pluto_version = \"25v2_1\"\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO{pluto_version}.gdb\"\n",
    "\n",
    "geodata = {}\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "for layer in layers:\n",
    "    gdf_layer = gpd.read_file(gdb_path, layer=layer)\n",
    "    try:\n",
    "        gdf_layer['wkb'] = gdf_layer['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf_layer\n",
    "\n",
    "gdf = geodata[f'MapPLUTO_{pluto_version}_clipped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whole_number_series(s: pd.Series) -> bool:\n",
    "    # Your helper might already implement this; fallback:\n",
    "    try:\n",
    "        notna = s.notna()\n",
    "        return (notna & ((s[notna] % 1) == 0)).all()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "for col in gdf.columns:\n",
    "    if gdf[col].dtype == float and is_whole_number_series(gdf[col]):\n",
    "        gdf[col] = gdf[col].astype('Int64')\n",
    "\n",
    "inspector = inspect(engine)\n",
    "\n",
    "col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}\n",
    "gdf = gdf.rename(columns=rename_mappings)\n",
    "\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)\n",
    "\n",
    "# after you load dataset_info_dict and before create_dynamic_table_class() / create_all()\n",
    "for k, v in col_customization_dict.items():\n",
    "    if v.new_name == \"version_number\":\n",
    "        v.dtype = \"String\"   # ensure TEXT in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        return Integer\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Float':\n",
    "        return Float\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    # IMPORTANT: Use LargeBinary for 'geometry' because you actually store WKB bytes there.\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(LargeBinary),  # store WKB bytes (BYTEA)\n",
    "        'wkb': Column(LargeBinary),       # kept for parity with original schema\n",
    "        'Shape_Leng': Column(Float),\n",
    "        'Shape_Area': Column(Float),\n",
    "        '__table_args__': {'extend_existing': True}\n",
    "    }\n",
    "    for k, v in col_customization_dict.items():\n",
    "        if any(name for name in multicolumns if name in k):\n",
    "            k = re.sub('_[0-9]$', '', k)\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        attrs[v.new_name] = Column(col_type)\n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "\n",
    "def normalize_zip(v):\n",
    "    if v is None: \n",
    "        return None\n",
    "    # floats/ints -> 5-digit string\n",
    "    if isinstance(v, (int, float)) and not (isinstance(v, float) and math.isnan(v)):\n",
    "        return f\"{int(v):05d}\"\n",
    "    s = str(v).strip()\n",
    "    if s == \"\": \n",
    "        return None\n",
    "    # '10013.0' -> '10013'\n",
    "    m = re.fullmatch(r\"\\d+(?:\\.0+)?\", s)\n",
    "    if m:\n",
    "        return f\"{int(float(s)):05d}\"\n",
    "    # '10013-1234' or '10013 1234' -> '10013-1234'\n",
    "    m = re.fullmatch(r\"(\\d{5})[-\\s]?(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}\"\n",
    "    # anything else: leave as-is (rare)\n",
    "    return s\n",
    "\n",
    "\n",
    "batch_size = 100000\n",
    "with SessionLocal() as session:\n",
    "    for start in range(0, len(gdf), batch_size):\n",
    "        batch = gdf.iloc[start:start + batch_size]\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                if 'apportionment_date' in row and row['apportionment_date']:\n",
    "                    row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "                for col in gdf.columns:\n",
    "                    # print(f\"Processing column: {col}\")\n",
    "                    val = row[col]\n",
    "                    # print(f\"Value: {val}, Type: {type(val)}\")\n",
    "                    if isinstance(val, pd.Series):\n",
    "                        try:\n",
    "                            first_value = val.iloc[0]\n",
    "                            row[col] = first_value\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing Series in column {col} at row {idx}: {e}\")\n",
    "                    if pd.isna(val):\n",
    "                        row[col] = None\n",
    "                geometry_wkb = row['geometry'].wkb if row['geometry'] is not None else None\n",
    "                # in your insert loop before constructing MapPLUTO_Clipped(...)\n",
    "                if 'zip_code' in row:\n",
    "                    row['zip_code'] = normalize_zip(row['zip_code'])\n",
    "                pluto_entry = MapPLUTO_Clipped(\n",
    "                    geometry=geometry_wkb,\n",
    "                    **{col: row[col] for col in gdf.columns if col != 'geometry'}\n",
    "                )\n",
    "                session.add(pluto_entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row index {idx}\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        print(f\"Column: {col}, Value: {row[col]}, Type: {type(row[col])}\")\n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"Error printing column {col}: {sub_e}\")\n",
    "                raise e\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(gdf[gdf.eq(\"E-61\").any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkb\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql('SELECT zip_code, geometry FROM \"MapPLUTO_25v2_1_clipped\"', engine)\n",
    "\n",
    "def to_geom(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    # psycopg2 often returns memoryview for bytea\n",
    "    if isinstance(val, memoryview):\n",
    "        return wkb.loads(val.tobytes())\n",
    "    if isinstance(val, (bytes, bytearray)):\n",
    "        return wkb.loads(bytes(val))\n",
    "    # if somehow a hex text like \"\\x010203...\" sneaks through:\n",
    "    if isinstance(val, str) and val.startswith(\"\\\\x\"):\n",
    "        return wkb.loads(bytes.fromhex(val[2:]))\n",
    "    raise TypeError(f\"Unexpected geometry value type: {type(val)}\")\n",
    "\n",
    "df[\"geometry\"] = df[\"geometry\"].apply(to_geom)\n",
    "gdf_map = gpd.GeoDataFrame(df, geometry=\"geometry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_gdf = gdf_map.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "# query = f\"SELECT zip_code, geometry FROM MapPLUTO_{pluto_version}_clipped\"\n",
    "# df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "# df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(f\"{PROJECT_DATA}/figures/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91968eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pg_int_bounds(sqlatype):\n",
    "    # Map SQL types to numpy bounds\n",
    "    if isinstance(sqlatype, SmallInteger):\n",
    "        return np.iinfo(np.int16).min, np.iinfo(np.int16).max\n",
    "    elif isinstance(sqlatype, BigInteger):\n",
    "        return np.iinfo(np.int64).min, np.iinfo(np.int64).max\n",
    "    else:\n",
    "        # Default Integer in Postgres is 32-bit\n",
    "        return np.iinfo(np.int32).min, np.iinfo(np.int32).max\n",
    "\n",
    "def enforce_integer_bounds(df, DynamicTable):\n",
    "    \"\"\"\n",
    "    For each INTEGER-like column in DynamicTable, coerce df[col] to numeric and\n",
    "    replace out-of-range values with None. Logs offenders for inspection.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in DynamicTable.__table__.columns:\n",
    "            continue\n",
    "        col_type = DynamicTable.__table__.columns[col].type\n",
    "\n",
    "        # Only care about integer-ish SQL column types\n",
    "        if not isinstance(col_type, (SmallInteger, Integer, BigInteger)):\n",
    "            continue\n",
    "\n",
    "        lo, hi = _pg_int_bounds(col_type)\n",
    "\n",
    "        # Coerce to numeric safely; keep strings like \"7\" working\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")  # floats OK here\n",
    "        # If column is conceptually integer, store as pandas nullable Int64 to preserve NULLs\n",
    "        # but do comparisons on the coerced float/int series 's'\n",
    "        mask_oob = (s.notna()) & ((s < lo) | (s > hi))\n",
    "\n",
    "        if mask_oob.any():\n",
    "            bad = df.loc[mask_oob, col]\n",
    "            # Show a small sample for debugging\n",
    "            sample_vals = bad.head(5).tolist()\n",
    "            print(\n",
    "                f\"❗ Out-of-range detected in integer column '{col}' \"\n",
    "                f\"({len(bad)} rows). Bounds [{lo}, {hi}]. Sample: {sample_vals}\"\n",
    "            )\n",
    "            # Null them out so insert won't fail\n",
    "            df.loc[mask_oob, col] = None\n",
    "\n",
    "        # Final cast to pandas nullable integer if column is nullable; else leave as numeric\n",
    "        # (Your pipeline already handles nullable detection; this is safe either way.)\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            # If the column had non-integer strings, keep as object; the DB will cast or fail elsewhere\n",
    "            pass\n",
    "\n",
    "from sqlalchemy.exc import DataError\n",
    "\n",
    "def insert_with_diagnosis(conn, insert_stmt, batch_rows, key_hint='sid'):\n",
    "    try:\n",
    "        # conn.execute(insert_stmt, batch_rows)\n",
    "        insert_with_diagnosis(conn, insert_stmt, batch_rows, key_hint='sid')\n",
    "        return\n",
    "    except DataError as e:\n",
    "        print(\"❌ Batch insert failed; diagnosing offending rows…\", e)\n",
    "        lo, hi = 0, len(batch_rows)\n",
    "        # binary search the smallest failing prefix\n",
    "        while lo + 1 < hi:\n",
    "            mid = (lo + hi) // 2\n",
    "            try:\n",
    "                conn.execute(insert_stmt, batch_rows[:mid])\n",
    "                lo = mid\n",
    "            except DataError:\n",
    "                hi = mid\n",
    "        bad = batch_rows[lo:hi]  # minimal failing slice\n",
    "        print(f\"🚩 Offending rows count: {len(bad)}\")\n",
    "        if key_hint and key_hint in bad[0]:\n",
    "            print(f\"Example offending {key_hint}: {bad[0][key_hint]}\")\n",
    "        # You can drop or sanitize and retry:\n",
    "        # conn.execute(insert_stmt, batch_rows[:lo] + batch_rows[hi:])\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_integer_bounds(df, DynamicTable):\n",
    "    \"\"\"\n",
    "    For each INTEGER-like column in DynamicTable, coerce df[col] to numeric and\n",
    "    replace out-of-range values with None. Logs offenders for inspection.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in DynamicTable.__table__.columns:\n",
    "            continue\n",
    "        col_type = DynamicTable.__table__.columns[col].type\n",
    "\n",
    "        # Only care about integer-ish SQL column types\n",
    "        if not isinstance(col_type, (SmallInteger, Integer, BigInteger)):\n",
    "            continue\n",
    "\n",
    "        lo, hi = _pg_int_bounds(col_type)\n",
    "\n",
    "        # Coerce to numeric safely; keep strings like \"7\" working\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")  # floats OK here\n",
    "        # If column is conceptually integer, store as pandas nullable Int64 to preserve NULLs\n",
    "        # but do comparisons on the coerced float/int series 's'\n",
    "        mask_oob = (s.notna()) & ((s < lo) | (s > hi))\n",
    "\n",
    "        if mask_oob.any():\n",
    "            bad = df.loc[mask_oob, col]\n",
    "            # Show a small sample for debugging\n",
    "            sample_vals = bad.head(5).tolist()\n",
    "            print(\n",
    "                f\"❗ Out-of-range detected in integer column '{col}' \"\n",
    "                f\"({len(bad)} rows). Bounds [{lo}, {hi}]. Sample: {sample_vals}\"\n",
    "            )\n",
    "            # Null them out so insert won't fail\n",
    "            df.loc[mask_oob, col] = None\n",
    "\n",
    "        # Final cast to pandas nullable integer if column is nullable; else leave as numeric\n",
    "        # (Your pipeline already handles nullable detection; this is safe either way.)\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        except Exception:\n",
    "            # If the column had non-integer strings, keep as object; the DB will cast or fail elsewhere\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jsonlines\n",
    "# import orjson\n",
    "\n",
    "datatype_mappings = {\"meta_data\": String, \"calendar_date\": Date, \"number\": Float, \"text\": String, \"point\": String}\n",
    "\n",
    "def split_address(row_data, address_col):\n",
    "    if address_col in row_data.keys():\n",
    "        if row_data[address_col] is None:\n",
    "            row_data[\"building_num\"], row_data[\"street_name\"] = None, None\n",
    "        else:\n",
    "            if row_data[address_col] and row_data[address_col][0].isdigit():\n",
    "                try:\n",
    "                    addr = row_data[address_col].split(\" \", 1)\n",
    "                    if len(addr) == 1:\n",
    "                        addr = [None] + addr\n",
    "                    row_data[\"building_num\"], row_data[\"street_name\"] = addr\n",
    "                except Exception as e:\n",
    "                    print(e, row_data[address_col])\n",
    "    return row_data\n",
    "\n",
    "def convert_wkt(rows_to_insert):\n",
    "    raw_wkts = [r.get('_raw_geocoded_column') for r in rows_to_insert]\n",
    "    try:\n",
    "        shapely_geoms = from_wkt(raw_wkts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting batch geometry: {e}\")\n",
    "        shapely_geoms = [None] * len(rows_to_insert)\n",
    "    geoms = [from_shape(geom, srid=4326) if geom is not None else None for geom in shapely_geoms]\n",
    "    for r, geom in zip(rows_to_insert, geoms):\n",
    "        r['geocoded_column'] = geom\n",
    "        r.pop('_raw_geocoded_column', None)\n",
    "\n",
    "def insert_dataset(engine: Engine, dataset, jsonfile, columns, batch_size=100000):\n",
    "    import io\n",
    "    import jsonlines\n",
    "    import orjson\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import Date  # ensure Date is in scope\n",
    "    # ensure textClean is imported from your helpers at module level\n",
    "    from shapely.geometry import shape as shapely_from_mapping\n",
    "    from shapely import from_wkt as shapely_from_wkt\n",
    "\n",
    "    # --- create the target table once and reuse ---\n",
    "    DynamicTable = create_table_for_dataset(\n",
    "        columns=dataset.col_types,   # your dict of logical dtypes\n",
    "        prefix=dataset.short_name,   # table name\n",
    "        engine=engine\n",
    "    )\n",
    "\n",
    "    col_names = list(columns.keys())\n",
    "    expected_width = len(col_names)\n",
    "    rows_buffer = []\n",
    "    geom_cols = []  # (col, srid)\n",
    "\n",
    "    def custom_loads(s):\n",
    "        return orjson.loads(s.encode(\"utf-8\"))\n",
    "\n",
    "    def normalize_rows(rows, col_names):\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            if isinstance(r, list):\n",
    "                out.append({col_names[i]: (r[i] if i < len(col_names) else None) for i in range(len(col_names))})\n",
    "            elif isinstance(r, dict):\n",
    "                out.append(r)\n",
    "            else:\n",
    "                out.append(r)\n",
    "        return out\n",
    "\n",
    "    from geoalchemy2.types import Geometry as GA2Geometry  # add with other shapely imports\n",
    "\n",
    "    def detect_geometry_columns(col_types):\n",
    "        found = []\n",
    "        for col, typ in col_types.items():\n",
    "            # 1) Handle real Geometry types\n",
    "            if isinstance(typ, GA2Geometry):\n",
    "                found.append((col, getattr(typ, \"srid\", 4326) or 4326))\n",
    "                continue\n",
    "            # 2) Handle string-y declarations (e.g., \"multipolygon\")\n",
    "            t = str(typ).lower()\n",
    "            if t in (\"multipolygon\", \"polygon\", \"point\", \"multilinestring\", \"linestring\", \"multipoint\"):\n",
    "                found.append((col, 4326))\n",
    "        # 3) Fallback by common name if nothing detected (covers Socrata exports)\n",
    "        if not found and \"the_geom\" in col_types:\n",
    "            print(\"ℹ️ No declared geometry type found; treating 'the_geom' as geometry (SRID 4326).\")\n",
    "            found.append((\"the_geom\", 4326))\n",
    "        return found\n",
    "\n",
    "\n",
    "    # def geom_to_wkt(raw):\n",
    "    #     if raw in (None, \"\", \"NULL\"):\n",
    "    #         return None\n",
    "    #     try:\n",
    "    #         if isinstance(raw, dict) and \"type\" in raw and \"coordinates\" in raw:\n",
    "    #             shp = shapely_from_mapping(raw)\n",
    "    #         elif isinstance(raw, str):\n",
    "    #             s = raw.strip()\n",
    "    #             if not s:\n",
    "    #                 return None\n",
    "    #             if s.upper().startswith(\"SRID=\") and \";\" in s:\n",
    "    #                 s = s.split(\";\", 1)[1]\n",
    "    #             shp = shapely_from_wkt(s)\n",
    "    #         else:\n",
    "    #             return None\n",
    "    #         return shp.wkt if shp is not None else None\n",
    "    #     except Exception:\n",
    "    #         return None\n",
    "\n",
    "    def geom_to_wkt(raw):\n",
    "        # Treat obvious numerics as \"no geometry\" to avoid parse errors\n",
    "        if raw is None:\n",
    "            return None\n",
    "        if isinstance(raw, (int, float)):\n",
    "            return None\n",
    "        if isinstance(raw, str):\n",
    "            s = raw.strip()\n",
    "            if not s or s.upper() == \"NULL\":\n",
    "                return None\n",
    "            # Numeric-looking string? bail out.\n",
    "            try:\n",
    "                float(s)\n",
    "                return None\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Strip optional SRID prefix\n",
    "            if s.upper().startswith(\"SRID=\") and \";\" in s:\n",
    "                s = s.split(\";\", 1)[1]\n",
    "            try:\n",
    "                shp = shapely_from_wkt(s)\n",
    "                return f\"SRID=4326;{shp.wkt}\"\n",
    "            except Exception:\n",
    "                return None\n",
    "        if isinstance(raw, dict) and \"type\" in raw and \"coordinates\" in raw:\n",
    "            try:\n",
    "                shp = shapely_from_mapping(raw)\n",
    "                return f\"SRID=4326;{shp.wkt}\"\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    _WKT_RX = re.compile(\n",
    "        r'(?i)(?:SRID=\\d+;)?\\s*(?:MULTI(?:POINT|LINESTRING|POLYGON)|POINT|LINESTRING|POLYGON)\\s*\\('\n",
    "        )\n",
    "\n",
    "    def looks_like_wkt_series(s: pd.Series) -> float:\n",
    "        sample = s.dropna().astype(str).head(200)\n",
    "        if sample.empty:\n",
    "            return 0.0\n",
    "        return sample.str.contains(_WKT_RX, na=False).mean()\n",
    "\n",
    "\n",
    "    def looks_like_geojson_series(s: pd.Series) -> float:\n",
    "        \"\"\"Return ratio of values that look like GeoJSON dicts with type/coordinates.\"\"\"\n",
    "        vals = s.dropna().head(200).tolist()\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "        hits = 0\n",
    "        for v in vals:\n",
    "            if isinstance(v, dict) and \"type\" in v and \"coordinates\" in v:\n",
    "                hits += 1\n",
    "        return hits / len(vals)\n",
    "\n",
    "    def is_mostly_numeric_series(s: pd.Series) -> float:\n",
    "        sample = s.dropna().astype(str).head(200)\n",
    "        if sample.empty:\n",
    "            return 0.0\n",
    "        def _num(x):\n",
    "            try:\n",
    "                float(x)\n",
    "                return True\n",
    "            except Exception:\n",
    "                return False\n",
    "        return sample.apply(_num).mean()\n",
    "\n",
    "    def detect_json_columns(col_types):\n",
    "        cols = []\n",
    "        for col, typ in col_types.items():\n",
    "            # SQLAlchemy types\n",
    "            if isinstance(typ, (JSON, JSONB)):\n",
    "                cols.append(col)\n",
    "                continue\n",
    "            # String-y declarations like \"json\"/\"jsonb\"\n",
    "            t = str(typ).lower()\n",
    "            if t in (\"json\", \"jsonb\"):\n",
    "                cols.append(col)\n",
    "        return cols\n",
    "\n",
    "    def _coerce_socrata_location(value):\n",
    "        \"\"\"\n",
    "        Socrata sometimes provides location as:\n",
    "        [human_address_json_string, latitude, longitude, <unused>, needs_recoding_bool]\n",
    "        Convert to a single dict.\n",
    "        \"\"\"\n",
    "        if not isinstance(value, list):\n",
    "            return value\n",
    "        if len(value) >= 3:\n",
    "            human = value[0]\n",
    "            # human_address may itself be a JSON string\n",
    "            if isinstance(human, str):\n",
    "                try:\n",
    "                    human = orjson.loads(human)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            out = {\n",
    "                \"human_address\": human,\n",
    "                \"latitude\": value[1],\n",
    "                \"longitude\": value[2],\n",
    "            }\n",
    "            if len(value) >= 5:\n",
    "                out[\"needs_recoding\"] = bool(value[4])\n",
    "            return out\n",
    "        return value\n",
    "\n",
    "    # add with your imports if not already present\n",
    "    import ast\n",
    "    from sqlalchemy.dialects.postgresql import JSON, JSONB  # you already used these in detect_json_columns\n",
    "\n",
    "    def detect_json_like_columns_in_df(df: pd.DataFrame, declared_json_cols: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Return columns that either:\n",
    "        - were declared JSON/JSONB, or\n",
    "        - contain dict/list objects, or\n",
    "        - are strings that look like JSON/Python-lists that we can parse.\n",
    "        \"\"\"\n",
    "        cols = set(declared_json_cols)\n",
    "\n",
    "        # Heuristic: common Socrata name\n",
    "        if \"location\" in df.columns:\n",
    "            cols.add(\"location\")\n",
    "\n",
    "        sample_n = 100\n",
    "        for c in df.columns:\n",
    "            s = df[c].dropna().head(sample_n)\n",
    "            if s.empty:\n",
    "                continue\n",
    "            # already Python dict/list?\n",
    "            if any(isinstance(v, (dict, list)) for v in s):\n",
    "                cols.add(c)\n",
    "                continue\n",
    "            # string that looks like JSON / Python list?\n",
    "            looks_like = s.astype(str).str.strip().str.startswith((\"{\", \"[\")).mean() > 0.2\n",
    "            if looks_like:\n",
    "                # prove we can parse at least one value\n",
    "                ok = False\n",
    "                for v in s.astype(str):\n",
    "                    t = v.strip()\n",
    "                    try:\n",
    "                        orjson.loads(t)\n",
    "                        ok = True\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            _ = ast.literal_eval(t)\n",
    "                            ok = True\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if ok:\n",
    "                    cols.add(c)\n",
    "        return list(cols)\n",
    "\n",
    "\n",
    "    def normalize_json_value(v):\n",
    "        if v is None or (isinstance(v, str) and not v.strip()):\n",
    "            return None\n",
    "        py = v\n",
    "        # Parse strings into Python values\n",
    "        if isinstance(v, str):\n",
    "            s = v.strip()\n",
    "            try:\n",
    "                py = orjson.loads(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    # Handle Python reprs like \"['...', None, False]\"\n",
    "                    py = ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    # Leave as plain string\n",
    "                    py = s\n",
    "        # Fix Socrata location list -> dict\n",
    "        py = _coerce_socrata_location(py)\n",
    "        # Dump to proper JSON text (double quotes, true/false/null)\n",
    "        try:\n",
    "            return orjson.dumps(py).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def ensure_geometry_columns(df: pd.DataFrame, geom_cols: list[tuple[str,int]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Guarantee that each declared geometry column contains EWKT or NULL.\n",
    "        If a geometry column is numeric/malformed, try to source geometry from\n",
    "        another column that looks like WKT/GeoJSON. Convert to EWKT via geom_to_wkt.\n",
    "        \"\"\"\n",
    "        # 1) Try to fix obvious misalignment by copying from the best-looking geometry column\n",
    "        candidates = []\n",
    "        for c in df.columns:\n",
    "            wkt_ratio = looks_like_wkt_series(df[c])\n",
    "            gj_ratio  = looks_like_geojson_series(df[c])\n",
    "            if (wkt_ratio >= 0.2) or (gj_ratio >= 0.2):\n",
    "                candidates.append((c, wkt_ratio, gj_ratio))\n",
    "        # Rank candidates by (geojson or wkt ratio), favor WKT a little\n",
    "        candidates.sort(key=lambda x: (max(x[1], x[2]), x[1]), reverse=True)\n",
    "\n",
    "        for gcol, _srid in geom_cols:\n",
    "            if gcol not in df.columns:\n",
    "                # If missing, create and fill from best candidate if any\n",
    "                src = candidates[0][0] if candidates else None\n",
    "                df[gcol] = df[src] if src else None\n",
    "                if src:\n",
    "                    print(f\"🔧 geometry column '{gcol}' missing — copying from '{src}'\")\n",
    "                continue\n",
    "\n",
    "            num_ratio = is_mostly_numeric_series(df[gcol])\n",
    "            wkt_ratio = looks_like_wkt_series(df[gcol])\n",
    "            gj_ratio  = looks_like_geojson_series(df[gcol])\n",
    "\n",
    "            if num_ratio > 0.7 and max(wkt_ratio, gj_ratio) < 0.2:\n",
    "                # very likely mis-mapped; try best candidate\n",
    "                if candidates:\n",
    "                    src = candidates[0][0]\n",
    "                    if src != gcol:\n",
    "                        print(f\"🔧 geometry misalignment: mapping '{src}' → '{gcol}'\")\n",
    "                        df[gcol] = df[src]\n",
    "\n",
    "            # Finally: convert whatever is in gcol to EWKT (or None)\n",
    "            df[gcol] = df[gcol].apply(geom_to_wkt)\n",
    "\n",
    "            # Fail-fast guard: no numeric leftovers\n",
    "            after_num_ratio = is_mostly_numeric_series(df[gcol])\n",
    "            if after_num_ratio > 0.0:\n",
    "                # Nuke numeric stragglers to NULL so COPY can't choke\n",
    "                def _clean_num_to_none(v):\n",
    "                    if v is None:\n",
    "                        return None\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        return None\n",
    "                    if isinstance(v, str):\n",
    "                        try:\n",
    "                            float(v.strip())\n",
    "                            return None\n",
    "                        except Exception:\n",
    "                            return v\n",
    "                    return v\n",
    "                df[gcol] = df[gcol].apply(_clean_num_to_none)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def relink_geometry_sources(df: pd.DataFrame, geom_cols: list[str]) -> pd.DataFrame:\n",
    "        \"\"\"If a declared geometry column is numeric/malformed, try to find the real geometry column in df.\"\"\"\n",
    "        for gcol, _srid in geom_cols:\n",
    "            if gcol not in df.columns:\n",
    "                continue\n",
    "            col = df[gcol]\n",
    "            # If this geometry column is numeric for most rows, it's likely mis-mapped\n",
    "            numeric_ratio = pd.to_numeric(col, errors=\"coerce\").notna().mean() if len(col) else 0.0\n",
    "            looks_like_wkt = looks_like_wkt_series(col)\n",
    "            looks_like_geojson = looks_like_geojson_series(col)\n",
    "\n",
    "            if numeric_ratio > 0.7 and not looks_like_wkt and not looks_like_geojson:\n",
    "                # try to discover a better source\n",
    "                candidates = []\n",
    "                for c in df.columns:\n",
    "                    if c == gcol:\n",
    "                        continue\n",
    "                    if looks_like_wkt_series(df[c]) or looks_like_geojson_series(df[c]):\n",
    "                        candidates.append(c)\n",
    "                if candidates:\n",
    "                    src = candidates[0]\n",
    "                    print(f\"🔧 Detected geometry misalignment: mapping '{src}' → '{gcol}'\")\n",
    "                    df[gcol] = df[src]\n",
    "        return df\n",
    "\n",
    "\n",
    "    # def process_batch(df, geom_cols):\n",
    "    #     df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    #     # Sanitize strings\n",
    "    #     for col in df.columns:\n",
    "    #         if df[col].dtype == object:\n",
    "    #             df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "\n",
    "    #     # Dates → date objects\n",
    "    #     datetime_cols = [key for key, typ in columns.items() if typ is Date]\n",
    "    #     for col in datetime_cols:\n",
    "    #         if col in df.columns:\n",
    "    #             df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    #             df[col] = df[col].apply(lambda x: x.date() if pd.notnull(x) else None)\n",
    "\n",
    "    #     # Geometry → WKT (PostGIS can parse text into geometry columns)\n",
    "    #     for col, _srid in geom_cols:\n",
    "    #         if col in df.columns:\n",
    "    #             df[col] = df[col].apply(geom_to_wkt)\n",
    "    #     return df\n",
    "\n",
    "    def process_batch(df, geom_cols, json_cols):\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "\n",
    "        # Sanitize strings\n",
    "        NUMERICISH_COLS = {\"latitude\", \"longitude\", \"lat\", \"lon\", \"lng\", \"x\", \"y\"}\n",
    "\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                # If this column is mostly numeric, convert to numeric and skip textClean\n",
    "                numeric_ratio = pd.to_numeric(df[col], errors=\"coerce\").notna().mean()\n",
    "                if col.lower() in NUMERICISH_COLS or numeric_ratio >= 0.8:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                else:\n",
    "                    df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "\n",
    "        # Dates → date objects\n",
    "        datetime_cols = [key for key, typ in columns.items() if typ is Date]\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                df[col] = df[col].apply(lambda x: x.date() if pd.notnull(x) else None)\n",
    "\n",
    "        # 🔎 Try to fix obvious geometry mis-mapping before conversion\n",
    "        df = relink_geometry_sources(df, geom_cols)\n",
    "\n",
    "        # Geometry → EWKT text (explicit SRID)\n",
    "        for col, _srid in geom_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(geom_to_wkt)\n",
    "          \n",
    "        # Normalize JSON columns to valid JSON text\n",
    "        for col in json_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(normalize_json_value)\n",
    "\n",
    "        json_like_cols = detect_json_like_columns_in_df(df, json_cols)\n",
    "        for col in json_like_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(normalize_json_value)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def verify_batch(engine, table_like, sid_series, sid_col=\"sid\", sample_missing=10):\n",
    "        import io\n",
    "        import pandas as pd\n",
    "\n",
    "        table = getattr(table_like, \"__table__\", table_like)\n",
    "        tgt_schema = getattr(table, \"schema\", None)\n",
    "        tgt_table = f'\"{tgt_schema}\".\"{table.name}\"' if tgt_schema else f'\"{table.name}\"'\n",
    "\n",
    "        # Build a one-column CSV buffer for fast COPY\n",
    "        buf = io.StringIO()\n",
    "        pd.Series(sid_series, name=\"sid\").to_csv(buf, index=False, header=False)\n",
    "        buf.seek(0)\n",
    "\n",
    "        tmp = \"tmp_expected_sids\"\n",
    "\n",
    "        conn = engine.raw_connection()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            # NOTE: no \"ON COMMIT DROP\"\n",
    "            cur.execute(f'CREATE TEMP TABLE {tmp} (sid text);')\n",
    "\n",
    "            # Copy the SIDs into the temp table (same transaction)\n",
    "            cur.copy_expert(f\"COPY {tmp} (sid) FROM STDIN WITH (FORMAT CSV)\", buf)\n",
    "\n",
    "            # Now run the comparisons BEFORE any commit\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {tmp};\")\n",
    "            expected = cur.fetchone()[0]\n",
    "\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {tgt_table} t JOIN {tmp} x ON t.\"{sid_col}\" = x.sid;')\n",
    "            found = cur.fetchone()[0]\n",
    "\n",
    "            missing = []\n",
    "            if found != expected:\n",
    "                cur.execute(\n",
    "                    f'''\n",
    "                    SELECT x.sid\n",
    "                    FROM {tmp} x\n",
    "                    LEFT JOIN {tgt_table} t ON t.\"{sid_col}\" = x.sid\n",
    "                    WHERE t.\"{sid_col}\" IS NULL\n",
    "                    LIMIT %s;\n",
    "                    ''',\n",
    "                    (sample_missing,)\n",
    "                )\n",
    "                missing = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "            print(f\"🔍 Batch verify: expected={expected}, found={found}, missing={expected - found}\")\n",
    "            if missing:\n",
    "                print(\"   e.g. missing SIDs:\", missing)\n",
    "\n",
    "            # Clean up and commit at the end\n",
    "            cur.execute(f\"DROP TABLE IF EXISTS {tmp};\")\n",
    "            conn.commit()\n",
    "            return expected, found, missing\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                cur.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "    def enforce_integer_compat(df, table_class):\n",
    "        \"\"\"Coerce whole-number floats/strings to Int64 for integer/bigint columns.\n",
    "        If fractional values exist in an integer column, raise with examples.\n",
    "        \"\"\"\n",
    "        int_cols = [\n",
    "            c.name for c in table_class.__table__.columns\n",
    "            if isinstance(getattr(c, \"type\", None), (Integer, BigInteger)) and c.name != \"id\"\n",
    "        ]\n",
    "        for c in int_cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "            # detect true fractional values\n",
    "            frac_mask = s.notna() & ((s % 1) != 0)\n",
    "            if frac_mask.any():\n",
    "                examples = s[frac_mask].head(5).tolist()\n",
    "                raise ValueError(\n",
    "                    f\"Column '{c}' is INTEGER/BIGINT but has fractional values, e.g. {examples}. \"\n",
    "                    f\"Either clean your data or change the column to NUMERIC(14,2) (recommended for money-like fields).\"\n",
    "                )\n",
    "\n",
    "            # coerce whole numbers (including '85392.00') to nullable int\n",
    "            df[c] = s.astype(\"Int64\")\n",
    "        return df\n",
    "\n",
    "    # def copy_batch(engine, df, table_class):\n",
    "    #     # Build the column list from the actual table (exclude serial PKs like \"id\")\n",
    "    #     copy_cols = [c.name for c in table_class.__table__.columns if c.name != \"id\"]\n",
    "\n",
    "    #     # Ensure all COPY columns exist in the DF (add missing as None), then order\n",
    "    #     for cname in copy_cols:\n",
    "    #         if cname not in df.columns:\n",
    "    #             df[cname] = None\n",
    "    #     df = df[copy_cols]\n",
    "\n",
    "    #     buf = io.StringIO()\n",
    "    #     df.to_csv(buf, index=False, header=False, na_rep=\"\\\\N\")\n",
    "    #     buf.seek(0)\n",
    "\n",
    "    #     conn = engine.raw_connection()  # not a context manager\n",
    "    #     try:\n",
    "    #         cur = conn.cursor()\n",
    "    #         # Qualify table if you set a schema in __table_args__ on the class\n",
    "    #         tname = table_class.__tablename__\n",
    "    #         if getattr(table_class.__table__, \"schema\", None):\n",
    "    #             tname = f'\"{table_class.__table__.schema}\".\"{tname}\"'\n",
    "    #         else:\n",
    "    #             tname = f'\"{tname}\"'\n",
    "\n",
    "    #         collist_sql = \", \".join(f'\"{c}\"' for c in copy_cols)\n",
    "    #         cur.copy_expert(\n",
    "    #             f\"COPY {tname} ({collist_sql}) FROM STDIN WITH (FORMAT CSV, NULL '\\\\N')\",\n",
    "    #             buf\n",
    "    #         )\n",
    "    #         conn.commit()\n",
    "    #     finally:\n",
    "    #         try:\n",
    "    #             cur.close()\n",
    "    #         except Exception:\n",
    "    #             pass\n",
    "    #         conn.close()\n",
    "\n",
    "    def copy_batch(engine, df, table_like):\n",
    "        \"\"\"\n",
    "        COPY df -> table_like. Accepts either a Declarative class (with __table__)\n",
    "        or a plain sqlalchemy.Table.\n",
    "        \"\"\"\n",
    "        import io\n",
    "\n",
    "        # Resolve to a sqlalchemy.Table\n",
    "        table = getattr(table_like, \"__table__\", table_like)\n",
    "        if not hasattr(table, \"name\"):\n",
    "            raise TypeError(\"copy_batch expected a Declarative class or sqlalchemy.Table\")\n",
    "\n",
    "        # Build fully-qualified table name\n",
    "        name = table.name\n",
    "        schema = getattr(table, \"schema\", None)\n",
    "        if schema:\n",
    "            fqtn = f'\"{schema}\".\"{name}\"'\n",
    "        else:\n",
    "            fqtn = f'\"{name}\"'\n",
    "\n",
    "        # Columns to COPY (skip autoincrement PKs like 'id' if present)\n",
    "        copy_cols = [c.name for c in table.columns if c.name != \"id\"]\n",
    "\n",
    "        # Ensure df has all COPY columns and in correct order\n",
    "        for col in copy_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        df = df[copy_cols]\n",
    "\n",
    "        # Prepare CSV buffer (NULL -> \\N for COPY)\n",
    "        buf = io.StringIO()\n",
    "        df.to_csv(buf, index=False, header=False, na_rep=\"\\\\N\")\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Execute COPY\n",
    "        conn = engine.raw_connection()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            collist_sql = \", \".join(f'\"{c}\"' for c in copy_cols)\n",
    "            sql = f\"COPY {fqtn} ({collist_sql}) FROM STDIN WITH (FORMAT CSV, NULL '\\\\N')\"\n",
    "            cur.copy_expert(sql, buf)\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            try:\n",
    "                cur.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "    # Detect geometry columns once\n",
    "    geom_cols = detect_geometry_columns(dataset.col_types)\n",
    "    # Detect JSON columns once\n",
    "    json_cols = detect_json_columns(dataset.col_types)\n",
    "\n",
    "\n",
    "    # --- Main loop ---\n",
    "    with jsonlines.open(jsonfile, mode=\"r\", loads=custom_loads) as reader:\n",
    "        batch_start = time.perf_counter()\n",
    "        for idx, row in enumerate(reader):\n",
    "            if isinstance(row, list) and len(row) > expected_width:\n",
    "                row = row[:expected_width]\n",
    "            rows_buffer.append(row)\n",
    "\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                print(f\"⏳ Processing batch ending at row {idx + 1}…\", end=\" \", flush=True)\n",
    "                normed = normalize_rows(rows_buffer, col_names)\n",
    "                df = pd.DataFrame(normed)\n",
    "                # print(f\"First df.head is {df.head().to_dict(orient='records')}\")\n",
    "                if not geom_cols and \"the_geom\" in df.columns:\n",
    "                    geom_cols = [(\"the_geom\", 4326)]\n",
    "                df = process_batch(df, geom_cols, json_cols)\n",
    "                # print(f\"Second df.head is {df.head().to_dict(orient='records')}\")\n",
    "                df = ensure_geometry_columns(df, geom_cols)\n",
    "                df = enforce_integer_compat(df, DynamicTable)\n",
    "                print(f\"Third df.head is {df.head().to_dict(orient='records')}\")\n",
    "                # Optional: quick peek to prove it's not numeric anymore\n",
    "                print(\"Filtered dict: \", df.filter(items=[\"the_geom\"]).head(3).to_dict(orient=\"records\"))\n",
    "                print(\"GEOM sample:\",\n",
    "                    df.filter(items=[c for c in [\"new_georeferenced_column\", \"location\", \"the_geom\"] if c in df.columns])\n",
    "                        .head(2).to_dict(orient=\"records\"))\n",
    "                print(\"LAT/LON sample:\",\n",
    "                    df.filter(items=[c for c in [\"latitude\", \"longitude\", \"lat\", \"lon\"] if c in df.columns])\n",
    "                        .head(2).to_dict(orient=\"records\"))\n",
    "                copy_batch(engine, df, DynamicTable)\n",
    "                # Verify this batch by SID\n",
    "                if \"sid\" in df.columns:\n",
    "                    _exp, _found, _missing = verify_batch(engine, DynamicTable, df[\"sid\"])\n",
    "                else:\n",
    "                    print(\"⚠️ No 'sid' column available to verify this batch.\")\n",
    "                t3 = time.perf_counter()\n",
    "                print(f\"📤 Inserted {len(df)} rows in {t3 - batch_start:.2f}s\")\n",
    "                rows_buffer.clear()\n",
    "                batch_start = time.perf_counter()\n",
    "\n",
    "        # Final flush\n",
    "        if rows_buffer:\n",
    "            normed = normalize_rows(rows_buffer, col_names)\n",
    "            df = pd.DataFrame(normed)\n",
    "            if not geom_cols and \"the_geom\" in df.columns:\n",
    "                geom_cols = [(\"the_geom\", 4326)]\n",
    "            df = process_batch(df, geom_cols, json_cols)\n",
    "            df = ensure_geometry_columns(df, geom_cols)\n",
    "            df = enforce_integer_compat(df, DynamicTable)\n",
    "            # Optional: quick peek to prove it's not numeric anymore\n",
    "            print(\"Filtered dict (last):\", df.filter(items=[\"the_geom\"]).head(3).to_dict(orient=\"records\"))\n",
    "            print(\"GEOM sample:\",\n",
    "                df.filter(items=[c for c in [\"new_georeferenced_column\", \"location\", \"the_geom\"] if c in df.columns])\n",
    "                    .head(2).to_dict(orient=\"records\"))\n",
    "\n",
    "            print(\"LAT/LON sample:\",\n",
    "                df.filter(items=[c for c in [\"latitude\", \"longitude\", \"lat\", \"lon\"] if c in df.columns])\n",
    "                    .head(2).to_dict(orient=\"records\"))\n",
    "            copy_batch(engine, df, DynamicTable)\n",
    "            if \"sid\" in df.columns:\n",
    "                _exp, _found, _missing = verify_batch(engine, DynamicTable, df[\"sid\"])\n",
    "            else:\n",
    "                print(\"⚠️ No 'sid' column available to verify this batch.\")\n",
    "            print(f\"✅ Final batch inserted ({len(df)} rows)\")\n",
    "\n",
    "    # Final sanity check: count source vs target rows\n",
    "    # Count source rows (stream)\n",
    "    src_count = 0\n",
    "    with jsonlines.open(dataset.dataset_path, mode=\"r\", loads=lambda s: orjson.loads(s.encode(\"utf-8\"))) as rdr:\n",
    "        for _ in rdr:\n",
    "            src_count += 1\n",
    "\n",
    "    # Count target rows\n",
    "    from sqlalchemy import text\n",
    "    with engine.connect() as conn:\n",
    "        tgt_count = conn.execute(text(f'SELECT COUNT(*) FROM \"{DynamicTable.__table__.name}\"')).scalar_one()\n",
    "\n",
    "    print(f\"Source rows: {src_count}  |  Target rows: {tgt_count}\")\n",
    "\n",
    "\n",
    "# # Run for all JSON datasets\n",
    "for name, dataset in dataset_info_dict.items():\n",
    "    if dataset.format == 'json':\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33754b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for name, dataset in dataset_info_dict.items():\n",
    "#     if dataset.format == 'json':\n",
    "#         print(f'Starting dataset {dataset.short_name}')\n",
    "#         if name == 'NTAs2020':\n",
    "#             insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61465e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_property",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
