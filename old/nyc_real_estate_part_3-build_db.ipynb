{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and populate the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import standard libraries\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "# import requests\n",
    "import time\n",
    "import dill\n",
    "time.sleep(3)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# # Import third-party libraries\n",
    "# import geopandas as gpd\n",
    "# from geoalchemy2 import Geometry\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Date, MetaData, event, Table, text, LargeBinary, ForeignKey\n",
    "from sqlalchemy.dialects.sqlite import insert\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# from sqlalchemy.event import listen\n",
    "# from sqlalchemy.engine import Engine\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# import sqlite3\n",
    "# from sqlite3 import dbapi2 as sqlite\n",
    "\n",
    "# import fiona\n",
    "# from fiona.crs import from_epsg\n",
    "\n",
    "from src.helpers import *\n",
    "from src.dbutils import *\n",
    "from src.ORMutils import *\n",
    "from src.models import *\n",
    "from src.geo import *\n",
    "from src.pdfutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Load the objects created in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "with open(\"environment_data/table_dicts.pkl\", \"rb\") as f:\n",
    "    env = dill.load(f)\n",
    "\n",
    "# Restore the environment\n",
    "globals().update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create the database engine that will be used throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'{SQLITE_PATH}?check_same_thread=False', echo=False)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Configure the database\n",
    "* ## Be sure that spatialite-tools is installed on your system (it is a system package, not a Python package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@event.listens_for(engine, \"connect\")\n",
    "def load_spatialite(dbapi_conn, connection_record):\n",
    "    print(\"Loading SpatiaLite extension\")\n",
    "    dbapi_conn.enable_load_extension(True)\n",
    "    dbapi_conn.load_extension(\"mod_spatialite\")\n",
    "    dbapi_conn.enable_load_extension(False)\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connection established\")\n",
    "    result = conn.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n",
    "# Enable WAL mode\n",
    "with SessionLocal() as session:\n",
    "    session.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
    "    session.execute(text(\"PRAGMA synchronous = NORMAL\"))\n",
    "    session.execute(text(\"PRAGMA temp_store = MEMORY\"))\n",
    "    session.execute(text(\"PRAGMA wal_autocheckpoint=1000;\"))  # Reduce I/O load\n",
    "    session.execute(text(\"PRAGMA mmap_size = 30000000000;\"))\n",
    "\n",
    "\n",
    "# Initialize spatial metadata if not already present\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))\n",
    "with SessionLocal() as session:\n",
    "    result = session.execute(text(\"SELECT spatialite_version()\"))\n",
    "    spatialite_version = result.fetchone()\n",
    "    print(f\"SpatiaLite version: {spatialite_version[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize spatialite metadata for better performance later (Thanks Copilot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT InitSpatialMetaData(1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "Base.metadata.reflect(bind=engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Create lookup tables variables identified as categorical and for which definitions were extracted from the metadata in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are borough codes in the PLUTO dataset, but annyoingly, in contrast to most other datasets, the borough code is a two letter inital like \"BK\" or \"BX\". Also in the PLUTO dataset, \"Sanitation Borough\" does use the standard numeric codes that most other NYC OpenData datasets use. All this is is to say that it requires special handling separate from my system to extract categories and create lookup tables for them programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = {}\n",
    "completed_tables = [] # This is for tracking the names of tables that have been created, which will be used to avoid creating redundant tables for columns that are same-kind repeats (such as \"district_1\" and \"district_2\"), and thus will use the same lookups.\n",
    "\n",
    "for name,dataset in dataset_info_dict.items():\n",
    "    lookups |= {k:v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].is_category == True}\n",
    "\n",
    "for name,table in lookups.items():\n",
    "    print(f\"processing {table}\")\n",
    "    if table.new_name is None:\n",
    "        table.new_name = table.short_name\n",
    "    lookup_table_name= re.sub('_[0-9]+$', '', table.new_name)\n",
    "    print(f\"lookup_table_name: {lookup_table_name}\")\n",
    "    if any([table.new_name.startswith(prefix) and table.new_name[-1].isdigit() for prefix in completed_tables]):\n",
    "        print(f\"Lookup table {lookup_table_name} already created, continuing...\")\n",
    "        continue\n",
    "    with engine.connect() as connection:\n",
    "        print(f\"Creating lookup table {lookup_table_name}...\")\n",
    "        lookup_table = create_lookup_table(Base.metadata, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "        print(f\"Created lookup table: {lookup_table}\")\n",
    "        name_prefix = lookup_table_name\n",
    "        completed_tables.append(name_prefix)\n",
    "        lookups[name].orm = lookup_table\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict['mapPLUTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "\n",
    "# lookups = {k:v for k,v in col_customization_dict.items() if col_customization_dict[k].is_category == True} # Codes for overlays are to go in the same table as other zoning codes\n",
    "# print(lookups)\n",
    "\n",
    "# completed_tables = [] # This is for tracking the names of tables that have been created, which will be used to avoid creating redundant tables for columns that are same-kind repeats (such as \"district_1\" and \"district_2\"), and thus will use the same lookups.\n",
    "\n",
    "# for name,table in lookups.items():\n",
    "#     print(f\"processing {table}\")\n",
    "#     lookup_table_name= re.sub('_[0-9]+$', '', table.new_name)\n",
    "#     print(f\"lookup_table_name: {lookup_table_name}\")\n",
    "#     if any([table.new_name.startswith(prefix) and table.new_name[-1].isdigit() for prefix in completed_tables]):\n",
    "#         print(f\"Lookup table {lookup_table_name} already created, continuing...\")\n",
    "#         continue\n",
    "#     with engine.connect() as connection:\n",
    "#         print(f\"Creating lookup table {lookup_table_name}...\")\n",
    "#         lookup_table = create_lookup_table(Base.metadata, lookup_table_name=lookup_table_name, text_column_name='name_or_code')\n",
    "#         print(f\"Created lookup table: {lookup_table}\")\n",
    "#         name_prefix = lookup_table_name\n",
    "#         completed_tables.append(name_prefix)\n",
    "#         lookups[name].orm = lookup_table\n",
    "\n",
    "# Base.metadata.create_all(engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for name,repetitions in multicolumns.items():\n",
    "        print(name)\n",
    "        print(f\"Setting {name} columns\")\n",
    "        for k in dataset.col_customizations.keys():\n",
    "            if dataset.col_customizations[k].new_name is None:\n",
    "                dataset.col_customizations[k].new_name = dataset.col_customizations[k].short_name\n",
    "        cols = {k:v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.startswith(name)}\n",
    "        print(f'cols for {name} are: {cols}')\n",
    "        main_col = [v for k,v in dataset.col_customizations.items() if dataset.col_customizations[k].new_name.endswith(\"_1\")]\n",
    "        if main_col:\n",
    "            print(f'main_col is {main_col}')\n",
    "            for key in cols.keys():\n",
    "                print(f'key is {key}')\n",
    "                lookups[key].orm = main_col[0].orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "\n",
    "multicolumns = ['zoning_district', 'commercial_overlay', 'special_purpose_district']\n",
    "\n",
    "for dataset in dataset_info_dict.values():\n",
    "    for k in dataset.col_customizations.keys():\n",
    "        print(\"ORM:\", dataset.col_customizations[k].orm)\n",
    "        for name in multicolumns:\n",
    "            if k.startswith(name) and k.endswith(\"_1\"):\n",
    "                print(k)\n",
    "                print(lookups[k])\n",
    "        # for k in dataset.col_customizations.keys():\n",
    "        #     print([col for col in multicolumns.keys() if k.startswith(col)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicolumns = {'zoning_district': 4, 'commercial_overlay': 2, 'special_purpose_district': 3}\n",
    "# for name,repetitions in multicolumns.items():\n",
    "#     print(f\"Setting {name} columns\")\n",
    "#     cols = {k:v for k,v in col_customization_dict.items() if col_customization_dict[k].new_name.startswith(name)}\n",
    "#     main_col = [v for k,v in col_customization_dict.items() if col_customization_dict[k].new_name.endswith(\"_1\")][0]\n",
    "#     for key in cols.keys():\n",
    "#         lookups[key].orm = main_col.orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,table in lookups.items():\n",
    "    lookup_table = table.orm\n",
    "    print(f'lookup_table {lookup_table}')\n",
    "    if lookup_table is None:\n",
    "        print(f\"Skipping {name}...\")\n",
    "        continue\n",
    "    print(lookup_table)\n",
    "    with engine.connect() as connection:\n",
    "        for definition in table.definitions:\n",
    "            if len(definition) == 2:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1]).on_conflict_do_nothing()\n",
    "                except ValueError:\n",
    "                    stmt = insert(lookup_table).values(name_or_code=definition[0], info=definition[1]).on_conflict_do_nothing()\n",
    "            elif len(definition) == 3:\n",
    "                try:\n",
    "                    stmt = insert(lookup_table).values(id=int(definition[0]), name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(definition)\n",
    "                    # stmt = insert(lookup_table).values(id=definition[0], name_or_code=definition[1], info=definition[2]).on_conflict_do_nothing()\n",
    "            else:\n",
    "                print(definition)\n",
    "                raise ValueError(\"Was only expecting two or three columns\")\n",
    "            connection.execute(stmt)\n",
    "        connection.commit()\n",
    "    name_prefix = table.new_name[0:round(len(table.new_name)*.75)] # Hopefully this is a safe threshold to identify when columns are repeats of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the MaPLUTO data:\n",
    "* List the layers in the file\n",
    "* In this case there is only one layer, so it isn't necessary to know and specify which one to import, but including anyway for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MapPLUTO data from geo database file (.gdb)\n",
    "pluto_version = \"25v1_1\"\n",
    "gdb_path = f\"{PROJECT_DATA}/files_to_use/MapPLUTO{pluto_version}.gdb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the geodatabase (.gdb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geodata = {}\n",
    "# List layers in the GDB file\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers in the GDB file:\")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    # gdf['borough'] = gdf['Borough'].replace(replacement_dict)\n",
    "    try:\n",
    "        gdf['wkb'] = gdf['geometry'].apply(lambda geom: geom.wkb if geom else None)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    geodata[layer] = gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the table in the Sqlite database and insert the (modified) data from the gdb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geodata[f'MapPLUTO_{pluto_version}_clipped']\n",
    "is_whole_number = {(gdf[col].notna() & (gdf[col] % 1 == 0)).all() for col in gdf.columns if gdf[col].dtype == 'float'}\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to check if all non-NaN values are whole numbers\n",
    "# def is_whole_number_series(s):\n",
    "#     return (s.dropna() % 1 == 0).all() \n",
    "\n",
    "# Iterate over columns and change dtype to int where applicable\n",
    "for col in gdf.columns:\n",
    "    if  gdf[col].dtype == float and is_whole_number_series(gdf[col]):\n",
    "        print(f'Column {col} is {is_whole_number_series(gdf[col])}')\n",
    "        print(f'Converting {col} to integer')\n",
    "        gdf[col] = gdf[col].astype('Int64')  # 'Int64' for nullable integer type in Pandas\n",
    "    else:\n",
    "        print(f\"Skipping {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())  # Ensure \"basement_type_or_grade_lookup\" is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_customization_dict = dataset_info_dict['mapPLUTO'].col_customizations\n",
    "rename_mappings = {v.short_name: v.new_name for v in col_customization_dict.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns=rename_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few of the column names did not exactly match up due to slightly different field names than specified in the data dictionary, so these need to be renamed manually:\n",
    "\n",
    "more_mappings = {\n",
    "    \"HealthCenterDistrict\": \"health_center_district\",\n",
    "    \"SanitDistrict\": \"sanitation_district_number\",\n",
    "    \"Sanitboro\": \"sanitation_district_boro\",\n",
    "    \"FIRM07_FLAG\": \"2007_flood_insurance_rate_map_indicator\",\n",
    "    \"PFIRM15_FLAG\": \"2015_preliminary_flood_insurance_rate_map\",\n",
    "}\n",
    "gdf = gdf.rename(columns=more_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, MetaData, Column, Integer, String, ForeignKey, LargeBinary, Float, Date\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "# Reflect the existing database tables once\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Function to map custom dtype to SQLAlchemy types\n",
    "def map_custom_dtype(dtype):\n",
    "    if dtype == 'Integer':\n",
    "        return Integer\n",
    "    elif dtype == 'String':\n",
    "        return String\n",
    "    elif dtype == 'Float':\n",
    "        return Float\n",
    "    elif dtype == 'Date':\n",
    "        return Date\n",
    "    elif dtype == 'LargeBinary':\n",
    "        return LargeBinary\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "# Function to dynamically create the table class\n",
    "def create_dynamic_table_class(table_name, col_customization_dict):\n",
    "    attrs = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True),\n",
    "        'geometry': Column(String),  \n",
    "        'wkb': Column(LargeBinary),  # Use LargeBinary for WKB\n",
    "        'Shape_Leng' : Column(Float), # Add columns not listed in the data dictionary\n",
    "        'Shape_Area' : Column(Float),\n",
    "    }\n",
    "    attrs['__table_args__'] = {'extend_existing': True}\n",
    "    \n",
    "    for k, v in col_customization_dict.items():\n",
    "        if any([name for name in multicolumns if name in k]):\n",
    "            k = re.sub('_[0-9]$', '', k)\n",
    "        col_type = map_custom_dtype(v.dtype)\n",
    "        if v.is_fk:\n",
    "            attrs[k] = Column(Integer, ForeignKey(f'{v.new_name}_lookup.id'))\n",
    "        elif v.is_category:\n",
    "            print(f'Creating id column for {v.new_name}')\n",
    "            attrs[v.new_name] = Column(col_type)\n",
    "            attrs[f\"{v.new_name}_id\"] = Column(Integer, ForeignKey(f'{k}_lookup.id'))\n",
    "        else:\n",
    "            attrs[v.new_name] = Column(col_type)\n",
    "    \n",
    "    return type(table_name, (Base,), attrs)\n",
    "\n",
    "# Create the MapPLUTO clipped table class\n",
    "MapPLUTO_Clipped = create_dynamic_table_class(f'MapPLUTO_{pluto_version}_clipped', col_customization_dict)\n",
    "\n",
    "# Reflect the metadata again to ensure it includes the new table class\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Create all tables in the database\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "from shapely import wkb\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "# gdf = geodata['MapPLUTO_24v4_clipped']\n",
    "def format_float(value):\n",
    "    return str(value).rstrip('0').rstrip('.') if '.' in str(value) else str(value)\n",
    "\n",
    "batch_size = 100000\n",
    "with SessionLocal() as session:\n",
    "    for start in range(0, len(gdf), batch_size):\n",
    "        batch = gdf.iloc[start:start + batch_size]\n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                if row['apportionment_date']:\n",
    "                    row['apportionment_date'] = parseDateString(row['apportionment_date'])\n",
    "                for col in gdf.columns:\n",
    "                    val = row[col]\n",
    "                    if isinstance(val, pd.Series):\n",
    "                        try:\n",
    "                            first_value = row[col].iloc[0]\n",
    "                            row[col] = first_value\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing Series in column {col} at row {idx}: {e}\")\n",
    "                    # Replace NA values with None so that SQLAlchemy inserts them as NULL:\n",
    "                    if pd.isna(val):\n",
    "                        row[col] = None\n",
    "                # Prepare the geometry and entry object\n",
    "                geometry_wkb = row['geometry'].wkb if row['geometry'] else None\n",
    "                pluto_entry = MapPLUTO_Clipped(\n",
    "                    geometry=geometry_wkb,\n",
    "                    **{col: row[col] for col in gdf.columns if col not in ['geometry']}\n",
    "                )\n",
    "                session.add(pluto_entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row index {idx}\")\n",
    "                for col in gdf.columns:\n",
    "                    try:\n",
    "                        print(f\"Column: {col}, Value: {row[col]}, Type: {type(row[col])}\")\n",
    "                    except Exception as sub_e:\n",
    "                        print(f\"Error printing column {col}: {sub_e}\")\n",
    "                raise e  # re-raise after logging for further debugging\n",
    "        session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a test plot to verify that the geodata was stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# Read the data from the database\n",
    "query = f\"SELECT zip_code, geometry FROM MapPLUTO_{pluto_version}_clipped\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Debug: Print the DataFrame columns\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Convert the geometry column from WKB to Shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if x else None)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(gdf.head())\n",
    "\n",
    "# Ensure that zip_code is preserved during the dissolve process\n",
    "merged_gdf = gdf.dissolve(by='zip_code', aggfunc={'zip_code': 'first'})  # Explicit aggregation of zip_code\n",
    "\n",
    "# Check if zip_code is now present after dissolving\n",
    "print(merged_gdf.columns)  # Should include 'zip_code'\n",
    "\n",
    "# Create a new adjacency graph based on the merged geometries\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on adjacency of merged shapes\n",
    "for i, shape1 in merged_gdf.iterrows():\n",
    "    for j, shape2 in merged_gdf.iterrows():\n",
    "        if i != j and shape1.geometry.touches(shape2.geometry):\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Perform graph coloring to ensure adjacent shapes don't share the same color\n",
    "color_map = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "\n",
    "# Plot the map with the colors assigned\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Normalize the color map to cover the full range of the node indices\n",
    "norm = mcolors.Normalize(vmin=min(color_map.values()), vmax=max(color_map.values()))\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.tab20, norm=norm)\n",
    "\n",
    "# Color the merged geometries based on the graph coloring using the full palette\n",
    "merged_gdf['color'] = merged_gdf.index.map(color_map)\n",
    "merged_gdf.plot(ax=ax, color=[sm.to_rgba(i) for i in merged_gdf['color']], edgecolor='black', linewidth=0, legend=False)\n",
    "\n",
    "# Add labels at the center of each merged shape\n",
    "for _, row in merged_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, str(row['zip_code']), fontsize=2, ha='center', va='center')\n",
    "\n",
    "# Add a colorbar to visualize the full range of colors\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Color Range (Graph Coloring)', rotation=270, labelpad=20)\n",
    "\n",
    "plt.savefig(f\"{PROJECT_DATA}/figures/map_output_zip_shuffled2.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_recode = [col for col in col_customization_dict.values() if not col.is_fk and col.is_category]\n",
    "to_recode = {k:v for k,v in lookups.items() if not v.is_fk and v.is_category}\n",
    "to_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in to_recode.keys():\n",
    "    print(f\"Processing {colname}...\")\n",
    "    lookup_table = to_recode[colname].orm\n",
    "    print(f'lookup_table: {lookup_table}')\n",
    "    # source_table_name = 'MapPLUTO_24v4_clipped'\n",
    "    # lookup_table_name = lookup_table.name\n",
    "    # text_column_name = colname\n",
    "    # s = time.process_time() # start timer\n",
    "    # populate_lookup_table(engine, lookup_table, source_table_name, lookup_table_name, text_column_name)\n",
    "    # e = time.process_time() # end time\n",
    "    # print(f\"Populating {colname} took {e-s} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more datasets to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype_mappings = {\"meta_data\" : String, \"calendar_date\" : Date, \"number\" : Float, \"text\" : String, \"point\" : String}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jsonlines\n",
    "# import orjson\n",
    "# import time\n",
    "# from shapely import from_wkt  # Vectorized conversion function in Shapely 2.0\n",
    "# from geoalchemy2.shape import from_shape\n",
    "# from sqlalchemy.engine import Engine\n",
    "\n",
    "# def convert_wkt(rows_to_insert):\n",
    "#     # Batch convert geometries.\n",
    "#     raw_wkts = [r.get('_raw_geocoded_column') for r in rows_to_insert]\n",
    "#     try:\n",
    "#         shapely_geoms = from_wkt(raw_wkts)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error converting batch geometry at row {idx}: {e}\")\n",
    "#         shapely_geoms = [None] * len(rows_to_insert)\n",
    "#     geoms = [\n",
    "#         from_shape(geom, srid=4326) if geom is not None else None \n",
    "#         for geom in shapely_geoms\n",
    "#     ]\n",
    "#     for r, geom in zip(rows_to_insert, geoms):\n",
    "#         r['geocoded_column'] = geom\n",
    "#         r.pop('_raw_geocoded_column', None)\n",
    "\n",
    "# def insert_dataset(engine,  dataset, jsonfile, columns, batch_size=10000, commit_interval=100, SessionLocal=SessionLocal):\n",
    "#     \"\"\"\n",
    "#     commit_interval: How many batches to accumulate before a commit.\n",
    "#     \"\"\"\n",
    "#     session = SessionLocal()\n",
    "#     col_names = list(columns.keys())\n",
    "#     rows_to_insert = []\n",
    "#     batch_counter = 0\n",
    "    \n",
    "#     # Define a custom loads function for orjson.\n",
    "#     def custom_loads(s):\n",
    "#         return orjson.loads(s.encode(\"utf-8\"))\n",
    "    \n",
    "#     with jsonlines.open(jsonfile, mode='r', loads=custom_loads) as reader:\n",
    "#         for idx, row in enumerate(reader):\n",
    "#             if idx == 0:\n",
    "#                 datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "#                 DynamicTable = create_table_for_dataset(\n",
    "#                     columns=dataset.col_types, \n",
    "#                     prefix=dataset.short_name, \n",
    "#                     engine=engine\n",
    "#                 )\n",
    "#                 # Prepare the insert statement (using SQLite's OR IGNORE to skip duplicates)\n",
    "#                 insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "            \n",
    "#             # Row is assumed to be a list; clean text values.\n",
    "#             row = [textClean(val) if isinstance(val, str) else val for val in row]\n",
    "#             # Map list to dict using the expected order.\n",
    "#             row_data = {col_name: (row[i] if i < len(row) else None)\n",
    "#                         for i, col_name in enumerate(col_names)}\n",
    "            \n",
    "#             # Keep the raw WKT for later geometry conversion.\n",
    "#             row_data['_raw_geocoded_column'] = row_data.get('geocoded_column')\n",
    "#             row_data['geocoded_column'] = None  # initialize to None for now.\n",
    "            \n",
    "#             # Process datetime values.\n",
    "#             temp = {}\n",
    "#             for key in row_data:\n",
    "#                 if key == '_raw_geocoded_column':\n",
    "#                     temp[key] = row_data[key]\n",
    "#                 else:\n",
    "#                     temp[key] = (parseDateString(row_data[key])\n",
    "#                                  if key in datetime_cols and row_data[key] is not None\n",
    "#                                  else row_data[key])\n",
    "#             row_data = temp\n",
    "            \n",
    "#             rows_to_insert.append(row_data)\n",
    "            \n",
    "#             if (idx + 1) % batch_size == 0:\n",
    "#                 convert_wkt(rows_to_insert)\n",
    "#                 # Batch convert geometries.\n",
    "#                 session.execute(insert_stmt, rows_to_insert)\n",
    "#                 rows_to_insert = []\n",
    "#                 batch_counter += 1\n",
    "                \n",
    "#                 # Rather than commit for each batch, commit once every commit_interval batches.\n",
    "#                 if batch_counter % commit_interval == 0:\n",
    "#                     commit_start = time.perf_counter()\n",
    "#                     session.commit()\n",
    "#                     commit_end = time.perf_counter()\n",
    "#                     print(f\"Committed {commit_interval} batches in {commit_end - commit_start:.3f} seconds.\")\n",
    "#                     # Optionally, you can also print the batch counter.\n",
    "#                     print(f\"Batch {batch_counter} processed.\")\n",
    "\n",
    "#         # Process any leftover rows.\n",
    "#         if rows_to_insert:\n",
    "#             convert_wkt(rows_to_insert)\n",
    "#             session.execute(insert_stmt, rows_to_insert)\n",
    "        \n",
    "#         # Final commit for any outstanding operations.\n",
    "#         session.commit()\n",
    "#     session.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import orjson\n",
    "import time\n",
    "from shapely import from_wkt  # Vectorized conversion function in Shapely 2.0\n",
    "from geoalchemy2.shape import from_shape\n",
    "from sqlalchemy.engine import Engine\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "def split_address(row_data, address_col):\n",
    "    if address_col in row_data.keys():\n",
    "        if row_data[address_col] is None:\n",
    "            row_data[\"building_num\"], row_data[\"street\"] = None, None\n",
    "            row_data.pop(address_col)\n",
    "            return row_data\n",
    "        else:\n",
    "            if row_data[address_col][0].isdigit():\n",
    "                try:\n",
    "                    addr = row_data[address_col].split(\" \", 1)\n",
    "                    if len(addr) == 1:\n",
    "                        addr = [None] + addr\n",
    "                    row_data[\"building_num\"], row_data[\"street\"] = addr\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(row_data[address_col])\n",
    "            else:\n",
    "                row_data[\"building_num\"], row_data[\"street_name\"] = (\n",
    "                    None,\n",
    "                    row_data[address_col],\n",
    "                )\n",
    "        row_data.pop(address_col)\n",
    "    return row_data\n",
    "\n",
    "def convert_wkt(rows_to_insert):\n",
    "    # Batch convert geometries.\n",
    "    raw_wkts = [r.get('_raw_geocoded_column') for r in rows_to_insert]\n",
    "    try:\n",
    "        shapely_geoms = from_wkt(raw_wkts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting batch geometry at row {idx}: {e}\")\n",
    "        shapely_geoms = [None] * len(rows_to_insert)\n",
    "    geoms = [\n",
    "        from_shape(geom, srid=4326) if geom is not None else None \n",
    "        for geom in shapely_geoms\n",
    "    ]\n",
    "    for r, geom in zip(rows_to_insert, geoms):\n",
    "        r['geocoded_column'] = geom\n",
    "        r.pop('_raw_geocoded_column', None)\n",
    "\n",
    "\n",
    "# def insert_dataset(session, engine, dataset, jsonfile, columns, batch_size=10000, commit_interval=100):\n",
    "#     \"\"\"\n",
    "#     Process the JSON Lines file in batches using pandas vectorized operations.\n",
    "#     \"\"\"\n",
    "#     col_names = list(columns.keys())\n",
    "#     expected_width = len(col_names)\n",
    "#     rows_buffer = []  # to store each row (as a list) from the JSON Lines file\n",
    "#     batch_counter = 0\n",
    "#     insert_stmt = None\n",
    "#     DynamicTable = None\n",
    "\n",
    "#     # Custom loads function for orjson, since jsonlines.open() must work in text mode.\n",
    "#     def custom_loads(s):\n",
    "#         return orjson.loads(s.encode(\"utf-8\"))\n",
    "\n",
    "#     with jsonlines.open(jsonfile, mode='r', loads=custom_loads) as reader:\n",
    "#         batch_start = time.perf_counter()\n",
    "#         # On the first batch, create the dynamic table and prepare the insert statement.\n",
    "#         if insert_stmt is None:\n",
    "#             print(f'Before creating dynanic table, dataset is {dataset}')\n",
    "#             DynamicTable = create_table_for_dataset(\n",
    "#                 columns=dataset.col_types,\n",
    "#                 prefix=dataset.short_name,\n",
    "#                 engine=engine\n",
    "#             )\n",
    "#             insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "#             print(f'insert_stmt is {insert_stmt}')\n",
    "#         for idx, row in enumerate(reader):\n",
    "#             # Optionally, trim the row if it has extra columns \n",
    "#             if isinstance(row, list) and len(row) > expected_width:\n",
    "#                 row = row[:expected_width]\n",
    "#             rows_buffer.append(row)\n",
    "#             if (idx + 1) % batch_size == 0:\n",
    "#                 # Convert the batch to a DataFrame.\n",
    "#                 # We'll trim each row to the expected width to safeguard against extra columns.\n",
    "#                 t0 = time.perf_counter()\n",
    "#                 df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "#                 # Vectorized text cleaning (apply only on columns with dtype object)\n",
    "#                 for col in df.columns:\n",
    "#                     if df[col].dtype == object:\n",
    "#                         df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "#                 # Convert datetime columns using vectorized pd.to_datetime.\n",
    "#                 datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "#                 for col in datetime_cols:\n",
    "#                     if col in df.columns:\n",
    "#                         df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#                 # Preserve raw geometry values and initialize geometry field.\n",
    "#                 if 'geocoded_column' in df.columns:\n",
    "#                     df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "#                     df['geocoded_column'] = None\n",
    "                \n",
    "#                 df = split_address(df, 'address')\n",
    "\n",
    "#                 # Convert the DataFrame into a list of dictionaries.\n",
    "#                 batch_rows = df.to_dict(orient='records')\n",
    "#                 # Convert geometry using batch-level vectorized conversion.\n",
    "#                 convert_wkt(batch_rows)\n",
    "#                 t1 = time.perf_counter()\n",
    "#                 print(f\"Preprocessing batch took {t1 - t0:.2f}s\")\n",
    "#                 t2 = time.perf_counter()\n",
    "#                 session.execute(insert_stmt, batch_rows)\n",
    "#                 t3 = time.perf_counter()\n",
    "#                 rows_buffer = []\n",
    "#                 batch_counter += 1\n",
    "#                 batch_end = time.perf_counter()\n",
    "#                 print(f\"Inserting batch took {t3 - t2:.2f}s\")\n",
    "#                 print(f\"Handled batch in {batch_end - batch_start:.3f} seconds\")\n",
    "#                 batch_start = time.perf_counter()\n",
    "\n",
    "#                 if batch_counter % commit_interval == 0:\n",
    "#                     commit_start = time.perf_counter()\n",
    "#                     session.commit()\n",
    "#                     commit_end = time.perf_counter()\n",
    "#                     print(f\"Committed {commit_interval} batches in {commit_end - commit_start:.3f} seconds; batch count = {batch_counter}\")\n",
    "\n",
    "#         # Process any remaining rows.\n",
    "#         if rows_buffer:\n",
    "#             df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "#             for col in df.columns:\n",
    "#                 if df[col].dtype == object:\n",
    "#                     df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "#             datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "#             for col in datetime_cols:\n",
    "#                 if col in df.columns:\n",
    "#                     df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#             if 'geocoded_column' in df.columns:\n",
    "#                 df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "#                 df['geocoded_column'] = None\n",
    "#             batch_rows = df.to_dict(orient='records')\n",
    "#             convert_wkt(batch_rows)\n",
    "#             print(f'insert_stmt is {insert_stmt}')\n",
    "#             session.execute(insert_stmt, batch_rows)\n",
    "\n",
    "#         session.commit()\n",
    "#     session.close()\n",
    "\n",
    "\n",
    "# def insert_dataset(engine, dataset, jsonfile, columns, batch_size=10000):\n",
    "#     \"\"\"\n",
    "#     Process the JSON Lines file in batches using pandas vectorized operations.\n",
    "#     Uses engine.begin() to leverage executemany().\n",
    "#     \"\"\"\n",
    "#     col_names = list(columns.keys())\n",
    "#     expected_width = len(col_names)\n",
    "#     rows_buffer = []\n",
    "#     batch_counter = 0\n",
    "#     insert_stmt = None\n",
    "#     DynamicTable = None\n",
    "\n",
    "#     def custom_loads(s):\n",
    "#         return orjson.loads(s.encode(\"utf-8\"))\n",
    "\n",
    "#     with jsonlines.open(jsonfile, mode='r', loads=custom_loads) as reader:\n",
    "#         batch_start = time.perf_counter()\n",
    "#         for idx, row in enumerate(reader):\n",
    "#             if isinstance(row, list) and len(row) > expected_width:\n",
    "#                 row = row[:expected_width]\n",
    "#             rows_buffer.append(row)\n",
    "\n",
    "#             if (idx + 1) % batch_size == 0:\n",
    "#                 t0 = time.perf_counter()\n",
    "#                 df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "#                 df = df.where(pd.notnull(df), None)\n",
    "#                 for col in df.columns:\n",
    "#                     if df[col].dtype == object:\n",
    "#                         df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "#                     if pd.api.types.is_numeric_dtype(df[col]) and df[col].isna().any():\n",
    "#                         print(f\"Warning: {col} contains NaN\")\n",
    "#                 datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "#                 for col in datetime_cols:\n",
    "#                     if col in df.columns:\n",
    "#                         df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#                 if 'geocoded_column' in df.columns:\n",
    "#                     df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "#                     df['geocoded_column'] = None\n",
    "#                 df = split_address(df, 'address')\n",
    "\n",
    "#                 # batch_rows = df.to_dict(orient='records')\n",
    "#                 batch_rows = [\n",
    "#                     {k: (None if isinstance(v, float) and math.isnan(v) else v) for k, v in row.items()}\n",
    "#                     for row in df.to_dict(orient=\"records\")\n",
    "#                 ]\n",
    "#                 convert_wkt(batch_rows)\n",
    "#                 t1 = time.perf_counter()\n",
    "#                 print(f\"Preprocessing batch took {t1 - t0:.2f}s\")\n",
    "\n",
    "#                 # First-time setup: create table and insert statement\n",
    "#                 if insert_stmt is None:\n",
    "#                     print(f\"Before creating dynamic table, dataset is {dataset}\")\n",
    "#                     DynamicTable = create_table_for_dataset(\n",
    "#                         columns=dataset.col_types,\n",
    "#                         prefix=dataset.short_name,\n",
    "#                         engine=engine\n",
    "#                     )\n",
    "#                     insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "#                     print(f\"insert_stmt is {insert_stmt}\")\n",
    "\n",
    "#                 t2 = time.perf_counter()\n",
    "#                 for i, row in enumerate(batch_rows):\n",
    "#                     for k, v in row.items():\n",
    "#                         if isinstance(v, float) and math.isnan(v):\n",
    "#                             print(f\"NaN detected at row {i}, column '{k}' — value: {repr(v)}\")\n",
    "#                             raise ValueError(\"NaN sneaked through\")\n",
    "#                 with engine.begin() as conn:\n",
    "#                     conn.execute(insert_stmt, batch_rows)\n",
    "#                 t3 = time.perf_counter()\n",
    "\n",
    "#                 rows_buffer = []\n",
    "#                 batch_counter += 1\n",
    "#                 batch_end = time.perf_counter()\n",
    "#                 print(f\"Inserting batch took {t3 - t2:.2f}s\")\n",
    "#                 print(f\"Handled batch in {batch_end - batch_start:.3f} seconds\")\n",
    "#                 batch_start = time.perf_counter()\n",
    "\n",
    "#         # Final flush\n",
    "#         if rows_buffer:\n",
    "#             df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "#             df = df.where(pd.notnull(df), None)  # <== Add this line\n",
    "#             for col in df.columns:\n",
    "#                 if df[col].dtype == object:\n",
    "#                     df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "#                 if pd.api.types.is_numeric_dtype(df[col]) and df[col].isna().any():\n",
    "#                     print(f\"Warning: {col} contains NaN\")\n",
    "#             datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "#             for col in datetime_cols:\n",
    "#                 if col in df.columns:\n",
    "#                     df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#             if 'geocoded_column' in df.columns:\n",
    "#                 df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "#                 df['geocoded_column'] = None\n",
    "#             df = split_address(df, 'address')\n",
    "#             batch_rows_raw = df.to_dict(orient=\"records\")\n",
    "#             batch_rows = [\n",
    "#                 {k: (None if isinstance(v, float) and math.isnan(v) else v) for k, v in row.items()}\n",
    "#                 for row in batch_rows_raw\n",
    "#             ]\n",
    "#             convert_wkt(batch_rows)\n",
    "#             for i, row in enumerate(batch_rows):\n",
    "#                 for k, v in row.items():\n",
    "#                     if isinstance(v, float) and math.isnan(v):\n",
    "#                         print(f\"NaN detected at row {i}, column '{k}' — value: {repr(v)}\")\n",
    "#                         raise ValueError(\"NaN sneaked through\")\n",
    "#             with engine.begin() as conn:\n",
    "#                 conn.execute(insert_stmt, batch_rows)\n",
    "\n",
    "def insert_dataset(engine, dataset, jsonfile, columns, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process the JSON Lines file in batches using pandas vectorized operations.\n",
    "    Uses engine.begin() to leverage executemany().\n",
    "    \"\"\"\n",
    "    col_names = list(columns.keys())\n",
    "    expected_width = len(col_names)\n",
    "    rows_buffer = []\n",
    "    insert_stmt = None\n",
    "    DynamicTable = None\n",
    "\n",
    "    def custom_loads(s):\n",
    "        return orjson.loads(s.encode(\"utf-8\"))\n",
    "\n",
    "    def sanitize_rows(rows):\n",
    "        cleaned = []\n",
    "        for i, row in enumerate(rows):\n",
    "            new_row = {}\n",
    "            for k, v in row.items():\n",
    "                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "                    print(f\"Offending value in row {i}, column '{k}': {repr(v)}\")\n",
    "                    v = None\n",
    "                new_row[k] = v\n",
    "            cleaned.append(new_row)\n",
    "        return cleaned\n",
    "\n",
    "    def validate_no_nans(rows):\n",
    "        for i, row in enumerate(rows):\n",
    "            for k, v in row.items():\n",
    "                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "                    print(f\"❌ Still found NaN/Inf at row {i}, column '{k}' — value: {repr(v)}\")\n",
    "                    raise ValueError(\"NaN or Inf sneaked through\")\n",
    "\n",
    "    def process_batch(df, nullable_integer_columns):\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "\n",
    "        # Sanitize strings\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                df[col] = df[col].apply(lambda x: textClean(x) if isinstance(x, str) else x)\n",
    "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].isna().any():\n",
    "                print(f\"⚠️ Warning: {col} contains NaN\")\n",
    "\n",
    "        # Fix nullable INTEGER columns: convert to nullable Int64\n",
    "        for col in nullable_integer_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # Dates\n",
    "        datetime_cols = [key for key in columns if columns[key] is Date]\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                df[col] = df[col].apply(lambda x: x.date() if pd.notnull(x) else None)\n",
    "        if 'geocoded_column' in df.columns:\n",
    "            df['_raw_geocoded_column'] = df['geocoded_column']\n",
    "            df['geocoded_column'] = None\n",
    "\n",
    "        df = split_address(df, 'address')\n",
    "\n",
    "        rows = df.to_dict(orient=\"records\")\n",
    "        rows = sanitize_rows(rows)\n",
    "        convert_wkt(rows)\n",
    "        validate_no_nans(rows)\n",
    "        return rows\n",
    "\n",
    "    with jsonlines.open(jsonfile, mode='r', loads=custom_loads) as reader:\n",
    "        batch_start = time.perf_counter()\n",
    "        for idx, row in enumerate(reader):\n",
    "            if isinstance(row, list) and len(row) > expected_width:\n",
    "                row = row[:expected_width]\n",
    "            rows_buffer.append(row)\n",
    "\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "                t0 = time.perf_counter()\n",
    "                if insert_stmt is None:\n",
    "                    print(f\"📐 Creating table for dataset {dataset.short_name}\")\n",
    "                    DynamicTable = create_table_for_dataset(\n",
    "                        columns=dataset.col_types,\n",
    "                        prefix=dataset.short_name,\n",
    "                        engine=engine\n",
    "                    )\n",
    "                    insert_stmt = DynamicTable.__table__.insert().prefix_with(\"OR IGNORE\")\n",
    "                    print(\"📋 Table schema:\")\n",
    "                    for col in DynamicTable.__table__.columns:\n",
    "                        print(f\"  {col.name}: {col.type}, nullable={col.nullable}\")\n",
    "                    nullable_integer_columns = [\n",
    "                        col for col, typ in columns.items()\n",
    "                        if typ is Integer and DynamicTable.__table__.columns[col].nullable\n",
    "                    ]\n",
    "                batch_rows = process_batch(df, nullable_integer_columns)\n",
    "                t2 = time.perf_counter()\n",
    "                with engine.begin() as conn:\n",
    "                    conn.execute(insert_stmt, batch_rows)\n",
    "                t3 = time.perf_counter()\n",
    "                print(f\"📤 Inserted batch in {t3 - t2:.2f}s, total time {t3 - batch_start:.2f}s\")\n",
    "\n",
    "                rows_buffer.clear()\n",
    "                batch_start = time.perf_counter()\n",
    "\n",
    "        # Final flush\n",
    "        if rows_buffer:\n",
    "            df = pd.DataFrame(rows_buffer, columns=col_names)\n",
    "            batch_rows = process_batch(df, nullable_integer_columns)\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(insert_stmt, batch_rows)\n",
    "            print(\"✅ Final batch inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in dataset_info_dict.items():\n",
    "#     print(f'{k} : {v}')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(dataset_info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `insert_dataset` takes a long time on the larger files. I tried a lot of diffent approaches, but none were fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = SessionLocal()\n",
    "# for name,dataset in dataset_info_dict.items():\n",
    "#     if dataset.format == 'json':\n",
    "#         print(f'Starting dataset {dataset.short_name}')\n",
    "#         print(f'The dataset to be processed is {dataset}')\n",
    "#         jsonfile = dataset.dataset_path\n",
    "#    w     columns = dataset.col_types\n",
    "#         print(f'Columns are {columns}')\n",
    "#         insert_dataset(session, engine, dataset, jsonfile, columns, batch_size = 100000, commit_interval=100)\n",
    "# session.close()\n",
    "\n",
    "for name, dataset in dataset_info_dict.items():\n",
    "    if dataset.format == 'json':\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        print(f'The dataset to be processed is {dataset}')\n",
    "        print(f'Starting dataset {dataset.short_name}')\n",
    "        insert_dataset(engine, dataset, dataset.dataset_path, dataset.col_types, batch_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_dict['assessment_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_i)  # Shows the last executed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc_property",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
